{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e373690e-3cba-4da3-9cc2-d507a86952a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pltx\n",
    "#import seaborn as sns \n",
    "#escalas\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#algoritmos \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "from keras.src.models.sequential import Sequential\n",
    "from keras.src.layers import Dense\n",
    "#métricas\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import ExponentialSmoothing #suavizado exponencial triple, que tiene en cuenta la estacionalidad\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade172a8-64a3-468a-9011-3dcfb14d19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/Users/pablomunozdelorenzo/Desktop/ventasEcommerce.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe55322-a6d7-4dfb-bc0c-4e96349be86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Order_Date\"] = pd.to_datetime(df[\"Order_Date\"])\n",
    "df2 = df[[\"Order_Date\",\"Total\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f2b92e8-a1c9-4f99-b2e6-c082561a45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultadoIng = df2.resample(\"ME\",on=\"Order_Date\").sum() #ingresos de entrenamiento\n",
    "df_resultadoVent = df2.resample(\"ME\",on=\"Order_Date\").count() #ventas de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d73fad04-7f8e-4f8d-874d-f0483c713a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.92812353],\n",
       "       [0.92812353, 1.        ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df_resultadoIng[\"Total\"],df_resultadoVent[\"Total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99d8893-b4a5-462a-a282-0b994ebd3760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>43476.4740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>19920.9974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>58863.4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>35541.9101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>43825.9822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>48190.7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>44825.1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>62837.8480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>86152.8880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>77448.1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>117938.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>83030.3888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total\n",
       "Order_Date             \n",
       "2018-01-31   43476.4740\n",
       "2018-02-28   19920.9974\n",
       "2018-03-31   58863.4128\n",
       "2018-04-30   35541.9101\n",
       "2018-05-31   43825.9822\n",
       "2018-06-30   48190.7277\n",
       "2018-07-31   44825.1040\n",
       "2018-08-31   62837.8480\n",
       "2018-09-30   86152.8880\n",
       "2018-10-31   77448.1312\n",
       "2018-11-30  117938.1550\n",
       "2018-12-31   83030.3888"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_resultadoVent[0:36] # tres primeros años de ventas\n",
    "X_test = df_resultadoVent[36:48] #último año de ventas\n",
    "Y_train = df_resultadoIng[0:36] # tres primeros años de ingresos\n",
    "Y_test = df_resultadoIng[36:48] #último año de ingresos\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36cc74e-300c-4d90-855d-839c2b7279cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.44882513],\n",
       "       [-0.90086996],\n",
       "       [ 0.64071881],\n",
       "       [ 0.21185577],\n",
       "       [ 0.67549149],\n",
       "       [ 0.61753702],\n",
       "       [ 0.4784463 ],\n",
       "       [ 0.37412827],\n",
       "       [ 3.14435169],\n",
       "       [ 1.30139971],\n",
       "       [ 3.17912437],\n",
       "       [ 3.21389705]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalerIng = StandardScaler()\n",
    "scalerVent = StandardScaler()\n",
    "X_trainEsc = scalerVent.fit_transform(X_train)\n",
    "X_testEsc = scalerVent.transform(X_test)\n",
    "Y_trainEsc = scalerIng.fit_transform(Y_train)\n",
    "Y_testEsc = scalerIng.transform(Y_test)\n",
    "print(len(X_trainEsc))\n",
    "print(len(X_testEsc))\n",
    "X_testEsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "624dabfc-eeab-40d1-9f57-beef5e359af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143.]\n",
      " [104.]\n",
      " [237.]\n",
      " [200.]\n",
      " [240.]\n",
      " [235.]\n",
      " [223.]\n",
      " [214.]\n",
      " [453.]\n",
      " [294.]\n",
      " [456.]\n",
      " [459.]]\n",
      "            Total\n",
      "Order_Date       \n",
      "2018-01-31    143\n",
      "2018-02-28    104\n",
      "2018-03-31    237\n",
      "2018-04-30    200\n",
      "2018-05-31    240\n",
      "2018-06-30    235\n",
      "2018-07-31    223\n",
      "2018-08-31    214\n",
      "2018-09-30    453\n",
      "2018-10-31    294\n",
      "2018-11-30    456\n",
      "2018-12-31    459\n"
     ]
    }
   ],
   "source": [
    "print(scalerVent.inverse_transform(X_testEsc))\n",
    "print(df_resultadoVent[36:48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8837bc88-2ed7-42b1-9b3e-31fcbf188186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.75\n",
      "36\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_trainEsc)/(len(Y_trainEsc)+len(Y_testEsc)))\n",
    "print(len(X_trainEsc)/(len(X_trainEsc)+len(X_testEsc)))\n",
    "print(len(Y_trainEsc))\n",
    "print(len(Y_testEsc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d74c2bc-1391-4553-a944-82aec6b0fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implantamos los modelos\n",
    "#comenzamos por regresión lineal\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_trainEsc,Y_trainEsc)\n",
    "y_predRLEsc = regr.predict(X_testEsc)\n",
    "resultado = np.concatenate((Y_testEsc,y_predRLEsc),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa7a10ce-f965-43a1-b5ec-fb14f63fc7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rf/zdh1bg5160s5tp2454895rfw0000gn/T/ipykernel_68604/2327039246.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['Technology'] = (df3['Category'] == 'Technology').astype(int)\n",
      "/var/folders/rf/zdh1bg5160s5tp2454895rfw0000gn/T/ipykernel_68604/2327039246.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['Office Supplies'] = (df3['Category'] == 'Office Supplies').astype(int)\n",
      "/var/folders/rf/zdh1bg5160s5tp2454895rfw0000gn/T/ipykernel_68604/2327039246.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['Furniture'] = (df3['Category'] == 'Furniture').astype(int)\n",
      "/var/folders/rf/zdh1bg5160s5tp2454895rfw0000gn/T/ipykernel_68604/2327039246.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_resultadoVent2[\"Ventas Totales\"] = df_resultadoVent2[\"Technology\"] + df_resultadoVent2[\"Office Supplies\"] + df_resultadoVent2[\"Furniture\"]\n"
     ]
    }
   ],
   "source": [
    "#continuamos por random forest\n",
    "#preparamos los datos\n",
    "df3 = df[[\"Order_Date\",\"Total\",\"Category\"]]\n",
    "df3['Technology'] = (df3['Category'] == 'Technology').astype(int)\n",
    "df3['Office Supplies'] = (df3['Category'] == 'Office Supplies').astype(int)\n",
    "df3['Furniture'] = (df3['Category'] == 'Furniture').astype(int)\n",
    "df_resultadoIng2 = df3.resample(\"ME\",on=\"Order_Date\").sum() #ingresos de entrenamiento\n",
    "df_resultadoVent2 = df3.resample(\"ME\",on=\"Order_Date\").count() #ventas de entrenamiento\n",
    "df_resultadoVent2 = df_resultadoIng2[[\"Technology\",\"Office Supplies\",\"Furniture\"]]\n",
    "df_resultadoVent2[\"Ventas Totales\"] = df_resultadoVent2[\"Technology\"] + df_resultadoVent2[\"Office Supplies\"] + df_resultadoVent2[\"Furniture\"]\n",
    "df_resultadoIng2 = df_resultadoIng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d12f477-baf4-4da5-986b-f053d458f473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>14205.7070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>4519.8920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>55205.7970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>27906.8550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>23644.3030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>34322.9356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>33781.5430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>27117.5365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>81623.5268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>31453.3930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>77907.6607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>68167.0585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>18066.9576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>11951.4110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>32339.3184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>34154.4685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>29959.5305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>23599.3740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>28608.2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>36818.3422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>63133.6060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>31011.7375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>75249.3995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>74543.6012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>18542.4910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>22978.8150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>51165.0590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>38679.7670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>56656.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>39724.4860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>38320.7830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>30542.2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>69193.3909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>59583.0330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>79066.4958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>95739.1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>43476.4740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>19920.9974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>58863.4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>35541.9101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>43825.9822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>48190.7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>44825.1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>62837.8480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>86152.8880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>77448.1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>117938.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>83030.3888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total\n",
       "Order_Date             \n",
       "2015-01-31   14205.7070\n",
       "2015-02-28    4519.8920\n",
       "2015-03-31   55205.7970\n",
       "2015-04-30   27906.8550\n",
       "2015-05-31   23644.3030\n",
       "2015-06-30   34322.9356\n",
       "2015-07-31   33781.5430\n",
       "2015-08-31   27117.5365\n",
       "2015-09-30   81623.5268\n",
       "2015-10-31   31453.3930\n",
       "2015-11-30   77907.6607\n",
       "2015-12-31   68167.0585\n",
       "2016-01-31   18066.9576\n",
       "2016-02-29   11951.4110\n",
       "2016-03-31   32339.3184\n",
       "2016-04-30   34154.4685\n",
       "2016-05-31   29959.5305\n",
       "2016-06-30   23599.3740\n",
       "2016-07-31   28608.2590\n",
       "2016-08-31   36818.3422\n",
       "2016-09-30   63133.6060\n",
       "2016-10-31   31011.7375\n",
       "2016-11-30   75249.3995\n",
       "2016-12-31   74543.6012\n",
       "2017-01-31   18542.4910\n",
       "2017-02-28   22978.8150\n",
       "2017-03-31   51165.0590\n",
       "2017-04-30   38679.7670\n",
       "2017-05-31   56656.9080\n",
       "2017-06-30   39724.4860\n",
       "2017-07-31   38320.7830\n",
       "2017-08-31   30542.2003\n",
       "2017-09-30   69193.3909\n",
       "2017-10-31   59583.0330\n",
       "2017-11-30   79066.4958\n",
       "2017-12-31   95739.1210\n",
       "2018-01-31   43476.4740\n",
       "2018-02-28   19920.9974\n",
       "2018-03-31   58863.4128\n",
       "2018-04-30   35541.9101\n",
       "2018-05-31   43825.9822\n",
       "2018-06-30   48190.7277\n",
       "2018-07-31   44825.1040\n",
       "2018-08-31   62837.8480\n",
       "2018-09-30   86152.8880\n",
       "2018-10-31   77448.1312\n",
       "2018-11-30  117938.1550\n",
       "2018-12-31   83030.3888"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultadoIng2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78d99c72-3c54-40f4-8afe-682b3a8e6f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerVentRF = StandardScaler()\n",
    "df_resultadoVentEsc2 = scalerVentRF.fit_transform(df_resultadoVent2)\n",
    "X_trainEsc2 = df_resultadoVentEsc2[0:36] # tres primeros años de ventas\n",
    "X_testEsc2 = df_resultadoVentEsc2[36:48] #último año de ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a151785a-e5d9-4592-aecb-5306f2f34fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sklearn/base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03136463, -0.42119666, -0.67936891],\n",
       "       [-1.00482077, -0.84541481, -0.87732312],\n",
       "       [ 0.70822299,  0.60127787,  0.16118106],\n",
       "       [-0.31767009,  0.19881449,  0.57193252],\n",
       "       [ 0.0467392 ,  0.63391003,  0.8596269 ],\n",
       "       [ 0.23874065,  0.57952309,  0.74228735],\n",
       "       [ 0.09068972,  0.44899443,  0.73917685],\n",
       "       [ 0.88305502,  0.35109793,  0.03264622],\n",
       "       [ 1.90866382,  2.9507938 ,  2.00420851],\n",
       "       [ 1.52574896,  1.22128902,  1.43571458],\n",
       "       [ 3.30687071,  2.98342597,  2.00420851],\n",
       "       [ 1.77130773,  3.01605813,  2.00420851]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modelo RF\n",
    "rf_model = RandomForestRegressor(n_estimators=100,random_state=4)\n",
    "rf_model.fit(X_trainEsc2, Y_trainEsc)\n",
    "y_predRF = rf_model.predict(X_testEsc2)\n",
    "resultado = np.column_stack((resultado,y_predRF))\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16f35994-eedb-4903-95db-43200ede5d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43476.474 ],\n",
       "       [ 19920.9974],\n",
       "       [ 58863.4128],\n",
       "       [ 35541.9101],\n",
       "       [ 43825.9822],\n",
       "       [ 48190.7277],\n",
       "       [ 44825.104 ],\n",
       "       [ 62837.848 ],\n",
       "       [ 86152.888 ],\n",
       "       [ 77448.1312],\n",
       "       [117938.155 ],\n",
       "       [ 83030.3888]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalerIng.inverse_transform(Y_testEsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cd9502a-38f8-459e-92a6-545ecd369877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>14205.7070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>4519.8920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>55205.7970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>27906.8550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>23644.3030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>34322.9356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>33781.5430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>27117.5365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>81623.5268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>31453.3930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>77907.6607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>68167.0585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>18066.9576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>11951.4110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>32339.3184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>34154.4685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>29959.5305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>23599.3740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>28608.2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>36818.3422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>63133.6060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>31011.7375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>75249.3995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>74543.6012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>18542.4910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>22978.8150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>51165.0590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>38679.7670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>56656.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>39724.4860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>38320.7830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>30542.2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>69193.3909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>59583.0330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>79066.4958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>95739.1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>43476.4740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>19920.9974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>58863.4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>35541.9101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>43825.9822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>48190.7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>44825.1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>62837.8480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>86152.8880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>77448.1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>117938.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>83030.3888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total\n",
       "Order_Date             \n",
       "2015-01-31   14205.7070\n",
       "2015-02-28    4519.8920\n",
       "2015-03-31   55205.7970\n",
       "2015-04-30   27906.8550\n",
       "2015-05-31   23644.3030\n",
       "2015-06-30   34322.9356\n",
       "2015-07-31   33781.5430\n",
       "2015-08-31   27117.5365\n",
       "2015-09-30   81623.5268\n",
       "2015-10-31   31453.3930\n",
       "2015-11-30   77907.6607\n",
       "2015-12-31   68167.0585\n",
       "2016-01-31   18066.9576\n",
       "2016-02-29   11951.4110\n",
       "2016-03-31   32339.3184\n",
       "2016-04-30   34154.4685\n",
       "2016-05-31   29959.5305\n",
       "2016-06-30   23599.3740\n",
       "2016-07-31   28608.2590\n",
       "2016-08-31   36818.3422\n",
       "2016-09-30   63133.6060\n",
       "2016-10-31   31011.7375\n",
       "2016-11-30   75249.3995\n",
       "2016-12-31   74543.6012\n",
       "2017-01-31   18542.4910\n",
       "2017-02-28   22978.8150\n",
       "2017-03-31   51165.0590\n",
       "2017-04-30   38679.7670\n",
       "2017-05-31   56656.9080\n",
       "2017-06-30   39724.4860\n",
       "2017-07-31   38320.7830\n",
       "2017-08-31   30542.2003\n",
       "2017-09-30   69193.3909\n",
       "2017-10-31   59583.0330\n",
       "2017-11-30   79066.4958\n",
       "2017-12-31   95739.1210\n",
       "2018-01-31   43476.4740\n",
       "2018-02-28   19920.9974\n",
       "2018-03-31   58863.4128\n",
       "2018-04-30   35541.9101\n",
       "2018-05-31   43825.9822\n",
       "2018-06-30   48190.7277\n",
       "2018-07-31   44825.1040\n",
       "2018-08-31   62837.8480\n",
       "2018-09-30   86152.8880\n",
       "2018-10-31   77448.1312\n",
       "2018-11-30  117938.1550\n",
       "2018-12-31   83030.3888"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##comenzamos las predicciones de estacionalidad. \n",
    "df_resultadoIng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "097c90dd-0bbd-441d-b04e-214d0263d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2018-01-31     30407.728742\n",
       "2018-02-28     30537.923623\n",
       "2018-03-31     56151.893477\n",
       "2018-04-30     51623.912188\n",
       "2018-05-31     59040.531183\n",
       "2018-06-30     47934.880339\n",
       "2018-07-31     52952.619865\n",
       "2018-08-31     54386.569946\n",
       "2018-09-30     95633.395068\n",
       "2018-10-31     55487.017431\n",
       "2018-11-30    101054.618238\n",
       "2018-12-31     96168.562128\n",
       "Freq: ME, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#suavizado exponencial\n",
    "modeloSE = ExponentialSmoothing(df_resultadoIng[0:36],seasonal_periods=12, trend='add', seasonal='add')\n",
    "ajusteSE = modeloSE.fit()\n",
    "y_predSE = ajusteSE.forecast(steps = len(df_resultadoIng[36:48]))\n",
    "y_predSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12ee3420-e81c-4e73-8115-553def3717ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7562577255539945\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(df_resultadoIng[36:48],y_predSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2a8b815-4479-4464-933a-dc77bd52bafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=514.796, Time=0.04 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=508.768, Time=0.06 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=507.790, Time=0.06 sec\n",
      " ARIMA(0,1,1)(0,1,0)[12]             : AIC=515.262, Time=0.01 sec\n",
      " ARIMA(0,1,1)(1,1,1)[12]             : AIC=inf, Time=0.14 sec\n",
      " ARIMA(0,1,1)(0,1,2)[12]             : AIC=508.871, Time=0.23 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12]             : AIC=509.375, Time=0.06 sec\n",
      " ARIMA(0,1,1)(1,1,2)[12]             : AIC=inf, Time=0.61 sec\n",
      " ARIMA(0,1,0)(0,1,1)[12]             : AIC=509.370, Time=0.06 sec\n",
      " ARIMA(1,1,1)(0,1,1)[12]             : AIC=inf, Time=0.28 sec\n",
      " ARIMA(0,1,2)(0,1,1)[12]             : AIC=510.395, Time=0.07 sec\n",
      " ARIMA(1,1,0)(0,1,1)[12]             : AIC=507.867, Time=0.05 sec\n",
      " ARIMA(1,1,2)(0,1,1)[12]             : AIC=inf, Time=0.29 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12] intercept   : AIC=513.716, Time=0.04 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(0,1,1)[12]          \n",
      "Total fit time: 2.010 seconds\n",
      " ARIMA(0,1,1)(0,1,1)[12]          \n"
     ]
    }
   ],
   "source": [
    "#Sarimax\n",
    "modeloSarimax = auto_arima(df_resultadoIng[0:36],start_p=0,d=1,start_q=0,\n",
    "                         max_p=4,max_d=2,max_q=4,start_P=0,\n",
    "                         D=1,start_Q=0,max_P=2,max_D=1, \n",
    "                        max_Q= 2,m=12, seasonal= True, error_action=\"warn\",\n",
    "                        trace= True, suppress_warnings=True, stepwise= True,\n",
    "                        random_state= 20, n_fits=50)\n",
    "print(modeloSarimax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bb51b03-4638-469b-879a-d1bf98e8e81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/statsmodels/tsa/statespace/sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n",
      "  warn('Too few observations to estimate starting parameters%s.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 6.772758\n",
      "         Iterations: 50\n",
      "         Function evaluations: 51\n",
      "         Gradient evaluations: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/scipy/optimize/_optimize.py:1397: OptimizeWarning: Maximum number of iterations has been exceeded.\n",
      "  res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n",
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/statsmodels/tsa/statespace/representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['typ']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2018-01-31     32720.416838\n",
       "2018-02-28     24066.923235\n",
       "2018-03-31     65745.437247\n",
       "2018-04-30     44937.389453\n",
       "2018-05-31     57004.678542\n",
       "2018-06-30     50369.864359\n",
       "2018-07-31     47127.701017\n",
       "2018-08-31     43251.520060\n",
       "2018-09-30    101577.928451\n",
       "2018-10-31     74194.716617\n",
       "2018-11-30    104467.510121\n",
       "2018-12-31    117726.508225\n",
       "Freq: ME, Name: predicted_mean, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arima_model = SARIMAX(df_resultadoIng[0:36],exog=df_resultadoVent[0:36],order=(0,1,1),seasonal_order=(0,1,1,12)).fit(method=\"bfgs\")\n",
    "y_predSarimax = arima_model.predict(start = \"2018-01-01\", end = \"2018-12-30\",exog = df_resultadoVent[36:48] ,typ=\"levels\")\n",
    "y_predSarimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df006ece-f915-4d22-b61e-3d7826950934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6910413653185703\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(df_resultadoIng[36:48],y_predSarimax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "741e009a-b25c-4b2f-a284-b7dad0d3d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03136463, -0.42119666, -0.67936891, -0.44178518, -0.54351837],\n",
       "       [-1.00482077, -0.84541481, -0.87732312, -0.82244501, -0.53779121],\n",
       "       [ 0.70822299,  0.60127787,  0.16118106,  1.0109574 ,  0.58894556],\n",
       "       [-0.31767009,  0.19881449,  0.57193252,  0.09562906,  0.38976351],\n",
       "       [ 0.0467392 ,  0.63391003,  0.8596269 ,  0.62645885,  0.71601428],\n",
       "       [ 0.23874065,  0.57952309,  0.74228735,  0.33459902,  0.22748614],\n",
       "       [ 0.09068972,  0.44899443,  0.73917685,  0.19197901,  0.44821224],\n",
       "       [ 0.88305502,  0.35109793,  0.03264622,  0.0214691 ,  0.51129048],\n",
       "       [ 1.90866382,  2.9507938 ,  2.00420851,  2.58719824,  2.32570327],\n",
       "       [ 1.52574896,  1.22128902,  1.43571458,  1.38263402,  0.55969823],\n",
       "       [ 3.30687071,  2.98342597,  2.00420851,  2.71430848,  2.56417827],\n",
       "       [ 1.77130773,  3.01605813,  2.00420851,  3.29756054,  2.34924481]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#escalamos ambos resultados y los añadimos al array\n",
    "y_predSarimaxEsc = scalerIng.transform(pd.DataFrame(y_predSarimax).values)\n",
    "y_predSEEsc = scalerIng.transform(pd.DataFrame(y_predSE).values)\n",
    "resultado = np.column_stack((resultado,y_predSarimaxEsc,y_predSEEsc))\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0096d2d-a57c-4ed2-8bf4-2ba27892f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> (772.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m193\u001b[0m (772.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> (772.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m193\u001b[0m (772.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#redes neuronales\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))  # Capa oculta con 64 neuronas y función de activación ReLU\n",
    "model.add(Dense(1))  # Capa de salida con una neurona (predicción de ingresos)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])  # Compilar el modelo con el optimizador Adam y la pérdida MSE\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcd8e9f1-09c2-4b75-9c21-8b646889d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.0000e+00 - loss: 1.0667 - val_accuracy: 0.0000e+00 - val_loss: 2.2328\n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 1.0330 - val_accuracy: 0.0000e+00 - val_loss: 2.1262\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 1.0069 - val_accuracy: 0.0000e+00 - val_loss: 2.0253\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.9357 - val_accuracy: 0.0000e+00 - val_loss: 1.9286\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.0000e+00 - loss: 0.9171 - val_accuracy: 0.0000e+00 - val_loss: 1.8340\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.8889 - val_accuracy: 0.0000e+00 - val_loss: 1.7469\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.8208 - val_accuracy: 0.0000e+00 - val_loss: 1.6644\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.7391 - val_accuracy: 0.0000e+00 - val_loss: 1.5818\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.7618 - val_accuracy: 0.0000e+00 - val_loss: 1.5038\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.7021 - val_accuracy: 0.0000e+00 - val_loss: 1.4309\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.7023 - val_accuracy: 0.0000e+00 - val_loss: 1.3618\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.6273 - val_accuracy: 0.0000e+00 - val_loss: 1.2970\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.6001 - val_accuracy: 0.0000e+00 - val_loss: 1.2317\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.5952 - val_accuracy: 0.0000e+00 - val_loss: 1.1684\n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.5668 - val_accuracy: 0.0000e+00 - val_loss: 1.1091\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.5414 - val_accuracy: 0.0000e+00 - val_loss: 1.0534\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.5025 - val_accuracy: 0.0000e+00 - val_loss: 1.0014\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.4603 - val_accuracy: 0.0000e+00 - val_loss: 0.9508\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.4559 - val_accuracy: 0.0000e+00 - val_loss: 0.9013\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.4490 - val_accuracy: 0.0000e+00 - val_loss: 0.8565\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.4194 - val_accuracy: 0.0000e+00 - val_loss: 0.8156\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.4038 - val_accuracy: 0.0000e+00 - val_loss: 0.7780\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.3824 - val_accuracy: 0.0000e+00 - val_loss: 0.7434\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.3678 - val_accuracy: 0.0000e+00 - val_loss: 0.7104\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.0000e+00 - loss: 0.3477 - val_accuracy: 0.0000e+00 - val_loss: 0.6804\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.3341 - val_accuracy: 0.0000e+00 - val_loss: 0.6524\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2888 - val_accuracy: 0.0000e+00 - val_loss: 0.6235\n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.3068 - val_accuracy: 0.0000e+00 - val_loss: 0.5942\n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.2940 - val_accuracy: 0.0000e+00 - val_loss: 0.5677\n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.2666 - val_accuracy: 0.0000e+00 - val_loss: 0.5431\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2722 - val_accuracy: 0.0000e+00 - val_loss: 0.5205\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2530 - val_accuracy: 0.0000e+00 - val_loss: 0.5006\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.2376 - val_accuracy: 0.0000e+00 - val_loss: 0.4823\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.2391 - val_accuracy: 0.0000e+00 - val_loss: 0.4652\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2311 - val_accuracy: 0.0000e+00 - val_loss: 0.4495\n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.0000e+00 - loss: 0.2187 - val_accuracy: 0.0000e+00 - val_loss: 0.4351\n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.2141 - val_accuracy: 0.0000e+00 - val_loss: 0.4225\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2062 - val_accuracy: 0.0000e+00 - val_loss: 0.4107\n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1994 - val_accuracy: 0.0000e+00 - val_loss: 0.3989\n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1914 - val_accuracy: 0.0000e+00 - val_loss: 0.3872\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1788 - val_accuracy: 0.0000e+00 - val_loss: 0.3761\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1840 - val_accuracy: 0.0000e+00 - val_loss: 0.3660\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1779 - val_accuracy: 0.0000e+00 - val_loss: 0.3575\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1719 - val_accuracy: 0.0000e+00 - val_loss: 0.3500\n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1679 - val_accuracy: 0.0000e+00 - val_loss: 0.3431\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1635 - val_accuracy: 0.0000e+00 - val_loss: 0.3368\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1589 - val_accuracy: 0.0000e+00 - val_loss: 0.3312\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0000e+00 - loss: 0.1559 - val_accuracy: 0.0000e+00 - val_loss: 0.3261\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1554 - val_accuracy: 0.0000e+00 - val_loss: 0.3216\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1516 - val_accuracy: 0.0000e+00 - val_loss: 0.3178\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1479 - val_accuracy: 0.0000e+00 - val_loss: 0.3144\n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1379 - val_accuracy: 0.0000e+00 - val_loss: 0.3117\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1377 - val_accuracy: 0.0000e+00 - val_loss: 0.3093\n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1425 - val_accuracy: 0.0000e+00 - val_loss: 0.3072\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 0.1374 - val_accuracy: 0.0000e+00 - val_loss: 0.3056\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1309 - val_accuracy: 0.0000e+00 - val_loss: 0.3044\n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1282 - val_accuracy: 0.0000e+00 - val_loss: 0.3032\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1269 - val_accuracy: 0.0000e+00 - val_loss: 0.3020\n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1328 - val_accuracy: 0.0000e+00 - val_loss: 0.3011\n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1235 - val_accuracy: 0.0000e+00 - val_loss: 0.3006\n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1313 - val_accuracy: 0.0000e+00 - val_loss: 0.3006\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1233 - val_accuracy: 0.0000e+00 - val_loss: 0.3009\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1293 - val_accuracy: 0.0000e+00 - val_loss: 0.3016\n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1156 - val_accuracy: 0.0000e+00 - val_loss: 0.3026\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1242 - val_accuracy: 0.0000e+00 - val_loss: 0.3038\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1215 - val_accuracy: 0.0000e+00 - val_loss: 0.3050\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1127 - val_accuracy: 0.0000e+00 - val_loss: 0.3063\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1218 - val_accuracy: 0.0000e+00 - val_loss: 0.3075\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1150 - val_accuracy: 0.0000e+00 - val_loss: 0.3085\n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1101 - val_accuracy: 0.0000e+00 - val_loss: 0.3098\n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1241 - val_accuracy: 0.0000e+00 - val_loss: 0.3114\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1240 - val_accuracy: 0.0000e+00 - val_loss: 0.3129\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1223 - val_accuracy: 0.0000e+00 - val_loss: 0.3142\n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1151 - val_accuracy: 0.0000e+00 - val_loss: 0.3153\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1234 - val_accuracy: 0.0000e+00 - val_loss: 0.3162\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1131 - val_accuracy: 0.0000e+00 - val_loss: 0.3173\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1237 - val_accuracy: 0.0000e+00 - val_loss: 0.3184\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1207 - val_accuracy: 0.0000e+00 - val_loss: 0.3190\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1161 - val_accuracy: 0.0000e+00 - val_loss: 0.3193\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1177 - val_accuracy: 0.0000e+00 - val_loss: 0.3199\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1211 - val_accuracy: 0.0000e+00 - val_loss: 0.3209\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1200 - val_accuracy: 0.0000e+00 - val_loss: 0.3212\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1225 - val_accuracy: 0.0000e+00 - val_loss: 0.3211\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1227 - val_accuracy: 0.0000e+00 - val_loss: 0.3210\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1114 - val_accuracy: 0.0000e+00 - val_loss: 0.3208\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1185 - val_accuracy: 0.0000e+00 - val_loss: 0.3201\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1168 - val_accuracy: 0.0000e+00 - val_loss: 0.3197\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1209 - val_accuracy: 0.0000e+00 - val_loss: 0.3197\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1154 - val_accuracy: 0.0000e+00 - val_loss: 0.3193\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1124 - val_accuracy: 0.0000e+00 - val_loss: 0.3197\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1218 - val_accuracy: 0.0000e+00 - val_loss: 0.3211\n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1192 - val_accuracy: 0.0000e+00 - val_loss: 0.3222\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1227 - val_accuracy: 0.0000e+00 - val_loss: 0.3226\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1213 - val_accuracy: 0.0000e+00 - val_loss: 0.3229\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1207 - val_accuracy: 0.0000e+00 - val_loss: 0.3229\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1227 - val_accuracy: 0.0000e+00 - val_loss: 0.3227\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1161 - val_accuracy: 0.0000e+00 - val_loss: 0.3231\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1174 - val_accuracy: 0.0000e+00 - val_loss: 0.3241\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1128 - val_accuracy: 0.0000e+00 - val_loss: 0.3259\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.1212 - val_accuracy: 0.0000e+00 - val_loss: 0.3284\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_trainEsc, Y_trainEsc, epochs=100, batch_size=32, validation_data=(X_testEsc, Y_testEsc))\n",
    "predicciones_2018 = model.predict(X_testEsc)\n",
    "resultado = np.column_stack((resultado, predicciones_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "756b4de7-10ed-41b9-9da5-57daa621349f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Valor Real</th>\n",
       "      <th>Regresión Lineal</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Sarimax</th>\n",
       "      <th>Suavización Exponencial</th>\n",
       "      <th>Red Neuronal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>0.031365</td>\n",
       "      <td>-0.421197</td>\n",
       "      <td>-0.679369</td>\n",
       "      <td>-0.441785</td>\n",
       "      <td>-0.543518</td>\n",
       "      <td>-0.506366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>-1.004821</td>\n",
       "      <td>-0.845415</td>\n",
       "      <td>-0.877323</td>\n",
       "      <td>-0.822445</td>\n",
       "      <td>-0.537791</td>\n",
       "      <td>-0.872335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>0.708223</td>\n",
       "      <td>0.601278</td>\n",
       "      <td>0.161181</td>\n",
       "      <td>1.010957</td>\n",
       "      <td>0.588946</td>\n",
       "      <td>0.844005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>-0.317670</td>\n",
       "      <td>0.198814</td>\n",
       "      <td>0.571933</td>\n",
       "      <td>0.095629</td>\n",
       "      <td>0.389764</td>\n",
       "      <td>0.425878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>0.046739</td>\n",
       "      <td>0.633910</td>\n",
       "      <td>0.859627</td>\n",
       "      <td>0.626459</td>\n",
       "      <td>0.716014</td>\n",
       "      <td>0.869831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>0.238741</td>\n",
       "      <td>0.579523</td>\n",
       "      <td>0.742287</td>\n",
       "      <td>0.334599</td>\n",
       "      <td>0.227486</td>\n",
       "      <td>0.826594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>0.090690</td>\n",
       "      <td>0.448994</td>\n",
       "      <td>0.739177</td>\n",
       "      <td>0.191979</td>\n",
       "      <td>0.448212</td>\n",
       "      <td>0.713282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.351098</td>\n",
       "      <td>0.032646</td>\n",
       "      <td>0.021469</td>\n",
       "      <td>0.511290</td>\n",
       "      <td>0.611074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>1.908664</td>\n",
       "      <td>2.950794</td>\n",
       "      <td>2.004209</td>\n",
       "      <td>2.587198</td>\n",
       "      <td>2.325703</td>\n",
       "      <td>2.461240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>1.525749</td>\n",
       "      <td>1.221289</td>\n",
       "      <td>1.435715</td>\n",
       "      <td>1.382634</td>\n",
       "      <td>0.559698</td>\n",
       "      <td>1.288585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>3.306871</td>\n",
       "      <td>2.983426</td>\n",
       "      <td>2.004209</td>\n",
       "      <td>2.714308</td>\n",
       "      <td>2.564178</td>\n",
       "      <td>2.483337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>1.771308</td>\n",
       "      <td>3.016058</td>\n",
       "      <td>2.004209</td>\n",
       "      <td>3.297561</td>\n",
       "      <td>2.349245</td>\n",
       "      <td>2.505435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Valor Real  Regresión Lineal  Random Forest   Sarimax  \\\n",
       "2018-01-31    0.031365         -0.421197      -0.679369 -0.441785   \n",
       "2018-02-28   -1.004821         -0.845415      -0.877323 -0.822445   \n",
       "2018-03-31    0.708223          0.601278       0.161181  1.010957   \n",
       "2018-04-30   -0.317670          0.198814       0.571933  0.095629   \n",
       "2018-05-31    0.046739          0.633910       0.859627  0.626459   \n",
       "2018-06-30    0.238741          0.579523       0.742287  0.334599   \n",
       "2018-07-31    0.090690          0.448994       0.739177  0.191979   \n",
       "2018-08-31    0.883055          0.351098       0.032646  0.021469   \n",
       "2018-09-30    1.908664          2.950794       2.004209  2.587198   \n",
       "2018-10-31    1.525749          1.221289       1.435715  1.382634   \n",
       "2018-11-30    3.306871          2.983426       2.004209  2.714308   \n",
       "2018-12-31    1.771308          3.016058       2.004209  3.297561   \n",
       "\n",
       "            Suavización Exponencial  Red Neuronal  \n",
       "2018-01-31                -0.543518     -0.506366  \n",
       "2018-02-28                -0.537791     -0.872335  \n",
       "2018-03-31                 0.588946      0.844005  \n",
       "2018-04-30                 0.389764      0.425878  \n",
       "2018-05-31                 0.716014      0.869831  \n",
       "2018-06-30                 0.227486      0.826594  \n",
       "2018-07-31                 0.448212      0.713282  \n",
       "2018-08-31                 0.511290      0.611074  \n",
       "2018-09-30                 2.325703      2.461240  \n",
       "2018-10-31                 0.559698      1.288585  \n",
       "2018-11-30                 2.564178      2.483337  \n",
       "2018-12-31                 2.349245      2.505435  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicciones = pd.DataFrame(resultado,index=df_resultadoVent[36:48].index.values,columns = ['Valor Real', 'Regresión Lineal', 'Random Forest', 'Sarimax', 'Suavización Exponencial',\"Red Neuronal\"])\n",
    "df_predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2dfe6909-6a7a-4d2b-a840-0e89d473a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2RL = r2_score(df_predicciones['Valor Real'], df_predicciones[\"Regresión Lineal\"])\n",
    "r2RF = r2_score(df_predicciones['Valor Real'], df_predicciones[\"Random Forest\"])\n",
    "r2Sarimax = r2_score(df_predicciones['Valor Real'], df_predicciones[\"Sarimax\"])\n",
    "r2SE = r2_score(df_predicciones['Valor Real'], df_predicciones[\"Suavización Exponencial\"])\n",
    "r2RN = r2_score(df_predicciones['Valor Real'], df_predicciones[\"Red Neuronal\"])\n",
    "mseRL = mean_squared_error(df_predicciones['Valor Real'], df_predicciones[\"Regresión Lineal\"])\n",
    "mseRF = mean_squared_error(df_predicciones['Valor Real'], df_predicciones[\"Random Forest\"])\n",
    "mseSarimax = mean_squared_error(df_predicciones['Valor Real'], df_predicciones[\"Sarimax\"])\n",
    "mseSE = mean_squared_error(df_predicciones['Valor Real'], df_predicciones[\"Suavización Exponencial\"])\n",
    "mseRN = mean_squared_error(df_predicciones['Valor Real'], df_predicciones[\"Red Neuronal\"])\n",
    "maeRL = mean_absolute_error(df_predicciones['Valor Real'], df_predicciones[\"Regresión Lineal\"])\n",
    "maeRF = mean_absolute_error(df_predicciones['Valor Real'], df_predicciones[\"Random Forest\"])\n",
    "maeSarimax = mean_absolute_error(df_predicciones['Valor Real'], df_predicciones[\"Sarimax\"])\n",
    "maeSE = mean_absolute_error(df_predicciones['Valor Real'], df_predicciones[\"Suavización Exponencial\"])\n",
    "maeRN = mean_absolute_error(df_predicciones['Valor Real'], df_predicciones[\"Red Neuronal\"])\n",
    "mapeRL = mean_absolute_percentage_error(df_predicciones['Valor Real'], df_predicciones[\"Regresión Lineal\"])\n",
    "mapeRF = mean_absolute_percentage_error(df_predicciones['Valor Real'], df_predicciones[\"Random Forest\"])\n",
    "mapeSarimax = mean_absolute_percentage_error(df_predicciones['Valor Real'], df_predicciones[\"Sarimax\"])\n",
    "mapeSE = mean_absolute_percentage_error(df_predicciones['Valor Real'], df_predicciones[\"Suavización Exponencial\"])\n",
    "mapeRN = mean_absolute_percentage_error(df_predicciones['Valor Real'], df_predicciones[\"Red Neuronal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e08cc158-69c3-40eb-94a5-6a26edeb63b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regresión Lineal</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Sarimax</th>\n",
       "      <th>Suavización Exponencial</th>\n",
       "      <th>Redes Neuronales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Coeficiente de determinación</th>\n",
       "      <td>0.727600</td>\n",
       "      <td>0.648397</td>\n",
       "      <td>0.691041</td>\n",
       "      <td>0.756258</td>\n",
       "      <td>0.745188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error cuadrático medio</th>\n",
       "      <td>0.351119</td>\n",
       "      <td>0.453211</td>\n",
       "      <td>0.398243</td>\n",
       "      <td>0.314180</td>\n",
       "      <td>0.328449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error absoluto medio</th>\n",
       "      <td>0.497366</td>\n",
       "      <td>0.567612</td>\n",
       "      <td>0.495873</td>\n",
       "      <td>0.498513</td>\n",
       "      <td>0.516872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Porcentaje de error absoluto medio</th>\n",
       "      <td>3.037836</td>\n",
       "      <td>4.550778</td>\n",
       "      <td>2.781913</td>\n",
       "      <td>3.443455</td>\n",
       "      <td>4.013569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Regresión Lineal  Random Forest   Sarimax  \\\n",
       "Coeficiente de determinación                0.727600       0.648397  0.691041   \n",
       "Error cuadrático medio                      0.351119       0.453211  0.398243   \n",
       "Error absoluto medio                        0.497366       0.567612  0.495873   \n",
       "Porcentaje de error absoluto medio          3.037836       4.550778  2.781913   \n",
       "\n",
       "                                    Suavización Exponencial  Redes Neuronales  \n",
       "Coeficiente de determinación                       0.756258          0.745188  \n",
       "Error cuadrático medio                             0.314180          0.328449  \n",
       "Error absoluto medio                               0.498513          0.516872  \n",
       "Porcentaje de error absoluto medio                 3.443455          4.013569  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calcular todos los errores (mae, mse, mape) y hacer una tablita\n",
    "index = [\"Coeficiente de determinación\", \"Error cuadrático medio\",\"Error absoluto medio\",\"Porcentaje de error absoluto medio\"]\n",
    "arrayMetrica = np.array([[r2RL, r2RF, r2Sarimax, r2SE, r2RN],\n",
    "                  [mseRL, mseRF, mseSarimax, mseSE,mseRN],\n",
    "                        [maeRL,maeRF,maeSarimax,maeSE,maeRN],\n",
    "                        [mapeRL,mapeRF,mapeSarimax,mapeSE,mapeRN]]) #hecho con los valores nuevos de RF\n",
    "df_metricas = pd.DataFrame(arrayMetrica,index=index,columns=['Regresión Lineal', 'Random Forest', 'Sarimax', 'Suavización Exponencial', \"Redes Neuronales\"])\n",
    "df_metricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11e008da-bb1c-4227-9d00-af6c597f1891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/statsmodels/tsa/holtwinters/model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2019-01-31     49682.408073\n",
       "2019-02-28     40135.135966\n",
       "2019-03-31     73568.403794\n",
       "2019-04-30     55112.106270\n",
       "2019-05-31     67276.278734\n",
       "2019-06-30     60818.224836\n",
       "2019-07-31     61453.022672\n",
       "2019-08-31     61866.994373\n",
       "2019-09-30     94522.396066\n",
       "2019-10-31     73967.755768\n",
       "2019-11-30    106402.971754\n",
       "2019-12-31    114474.811994\n",
       "Freq: ME, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#una vez tenemos predichos los ingresos de 2018, pasamos a predecir los de 2019. Para ello, entrenaremos el modelo con\n",
    "#los datos entre 2016 a 2018, y en base a ellos predeciremos los de 2019 usando suavizado exponencial \n",
    "modeloSE2019 = ExponentialSmoothing(df_resultadoIng[12:48],seasonal_periods=12, trend='add', seasonal='add')\n",
    "ajusteSE2019 = modeloSE2019.fit()\n",
    "y_predSE2019 = ajusteSE2019.forecast(steps = len(df_resultadoIng[36:48]))\n",
    "y_predSE2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46db0690-ecea-4806-a4ee-33b71dac4094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>year</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14205.7070</td>\n",
       "      <td>18066.9576</td>\n",
       "      <td>18542.4910</td>\n",
       "      <td>43476.4740</td>\n",
       "      <td>49682.408073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4519.8920</td>\n",
       "      <td>11951.4110</td>\n",
       "      <td>22978.8150</td>\n",
       "      <td>19920.9974</td>\n",
       "      <td>40135.135966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55205.7970</td>\n",
       "      <td>32339.3184</td>\n",
       "      <td>51165.0590</td>\n",
       "      <td>58863.4128</td>\n",
       "      <td>73568.403794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27906.8550</td>\n",
       "      <td>34154.4685</td>\n",
       "      <td>38679.7670</td>\n",
       "      <td>35541.9101</td>\n",
       "      <td>55112.106270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23644.3030</td>\n",
       "      <td>29959.5305</td>\n",
       "      <td>56656.9080</td>\n",
       "      <td>43825.9822</td>\n",
       "      <td>67276.278734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34322.9356</td>\n",
       "      <td>23599.3740</td>\n",
       "      <td>39724.4860</td>\n",
       "      <td>48190.7277</td>\n",
       "      <td>60818.224836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33781.5430</td>\n",
       "      <td>28608.2590</td>\n",
       "      <td>38320.7830</td>\n",
       "      <td>44825.1040</td>\n",
       "      <td>61453.022672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27117.5365</td>\n",
       "      <td>36818.3422</td>\n",
       "      <td>30542.2003</td>\n",
       "      <td>62837.8480</td>\n",
       "      <td>61866.994373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>81623.5268</td>\n",
       "      <td>63133.6060</td>\n",
       "      <td>69193.3909</td>\n",
       "      <td>86152.8880</td>\n",
       "      <td>94522.396066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31453.3930</td>\n",
       "      <td>31011.7375</td>\n",
       "      <td>59583.0330</td>\n",
       "      <td>77448.1312</td>\n",
       "      <td>73967.755768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>77907.6607</td>\n",
       "      <td>75249.3995</td>\n",
       "      <td>79066.4958</td>\n",
       "      <td>117938.1550</td>\n",
       "      <td>106402.971754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>68167.0585</td>\n",
       "      <td>74543.6012</td>\n",
       "      <td>95739.1210</td>\n",
       "      <td>83030.3888</td>\n",
       "      <td>114474.811994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "year         2015        2016        2017         2018           2019\n",
       "month                                                                \n",
       "1      14205.7070  18066.9576  18542.4910   43476.4740   49682.408073\n",
       "2       4519.8920  11951.4110  22978.8150   19920.9974   40135.135966\n",
       "3      55205.7970  32339.3184  51165.0590   58863.4128   73568.403794\n",
       "4      27906.8550  34154.4685  38679.7670   35541.9101   55112.106270\n",
       "5      23644.3030  29959.5305  56656.9080   43825.9822   67276.278734\n",
       "6      34322.9356  23599.3740  39724.4860   48190.7277   60818.224836\n",
       "7      33781.5430  28608.2590  38320.7830   44825.1040   61453.022672\n",
       "8      27117.5365  36818.3422  30542.2003   62837.8480   61866.994373\n",
       "9      81623.5268  63133.6060  69193.3909   86152.8880   94522.396066\n",
       "10     31453.3930  31011.7375  59583.0330   77448.1312   73967.755768\n",
       "11     77907.6607  75249.3995  79066.4958  117938.1550  106402.971754\n",
       "12     68167.0585  74543.6012  95739.1210   83030.3888  114474.811994"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfy_predSE2019 = pd.DataFrame(y_predSE2019,columns = [\"Total\"])\n",
    "df_resultadoIngFinal = pd.concat([df_resultadoIng, dfy_predSE2019])\n",
    "df_resultadoIngFinal[\"month\"] = df_resultadoIngFinal.index.month\n",
    "df_resultadoIngFinal[\"year\"] = df_resultadoIngFinal.index.year\n",
    "df_resultadoIngFinal = df_resultadoIngFinal.pivot(index=\"month\",columns=\"year\",values=\"Total\")\n",
    "df_resultadoIngFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "220c2439-37cb-4155-8fc2-d8caca47266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0000e+00 - loss: 1.0556 - val_accuracy: 0.0000e+00 - val_loss: 1.6383\n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.9780 - val_accuracy: 0.0000e+00 - val_loss: 1.5833\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.9924 - val_accuracy: 0.0000e+00 - val_loss: 1.5308\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.9510 - val_accuracy: 0.0000e+00 - val_loss: 1.4788\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 0.9054 - val_accuracy: 0.0000e+00 - val_loss: 1.4273\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.7824 - val_accuracy: 0.0000e+00 - val_loss: 1.3758\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.8338 - val_accuracy: 0.0000e+00 - val_loss: 1.3259\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.7867 - val_accuracy: 0.0000e+00 - val_loss: 1.2773\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.7253 - val_accuracy: 0.0000e+00 - val_loss: 1.2295\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.7360 - val_accuracy: 0.0000e+00 - val_loss: 1.1861\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.6905 - val_accuracy: 0.0000e+00 - val_loss: 1.1471\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.6505 - val_accuracy: 0.0000e+00 - val_loss: 1.1120\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.6328 - val_accuracy: 0.0000e+00 - val_loss: 1.0815\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.6046 - val_accuracy: 0.0000e+00 - val_loss: 1.0532\n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.5966 - val_accuracy: 0.0000e+00 - val_loss: 1.0262\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0000e+00 - loss: 0.5504 - val_accuracy: 0.0000e+00 - val_loss: 1.0014\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.5324 - val_accuracy: 0.0000e+00 - val_loss: 0.9779\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.5077 - val_accuracy: 0.0000e+00 - val_loss: 0.9540\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.4685 - val_accuracy: 0.0000e+00 - val_loss: 0.9296\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.4600 - val_accuracy: 0.0000e+00 - val_loss: 0.9066\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.4310 - val_accuracy: 0.0000e+00 - val_loss: 0.8855\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.4126 - val_accuracy: 0.0000e+00 - val_loss: 0.8677\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.3992 - val_accuracy: 0.0000e+00 - val_loss: 0.8518\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.3854 - val_accuracy: 0.0000e+00 - val_loss: 0.8372\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.3797 - val_accuracy: 0.0000e+00 - val_loss: 0.8232\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.3641 - val_accuracy: 0.0000e+00 - val_loss: 0.8088\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.3191 - val_accuracy: 0.0000e+00 - val_loss: 0.7946\n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.3255 - val_accuracy: 0.0000e+00 - val_loss: 0.7822\n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.3249 - val_accuracy: 0.0000e+00 - val_loss: 0.7717\n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.3134 - val_accuracy: 0.0000e+00 - val_loss: 0.7622\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2849 - val_accuracy: 0.0000e+00 - val_loss: 0.7537\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2723 - val_accuracy: 0.0000e+00 - val_loss: 0.7433\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2716 - val_accuracy: 0.0000e+00 - val_loss: 0.7286\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.2705 - val_accuracy: 0.0000e+00 - val_loss: 0.7135\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.2569 - val_accuracy: 0.0000e+00 - val_loss: 0.6982\n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 0.2461 - val_accuracy: 0.0000e+00 - val_loss: 0.6829\n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2469 - val_accuracy: 0.0000e+00 - val_loss: 0.6682\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2278 - val_accuracy: 0.0000e+00 - val_loss: 0.6535\n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2356 - val_accuracy: 0.0000e+00 - val_loss: 0.6402\n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2283 - val_accuracy: 0.0000e+00 - val_loss: 0.6289\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.2265 - val_accuracy: 0.0000e+00 - val_loss: 0.6198\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.2146 - val_accuracy: 0.0000e+00 - val_loss: 0.6110\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2141 - val_accuracy: 0.0000e+00 - val_loss: 0.6021\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.2056 - val_accuracy: 0.0000e+00 - val_loss: 0.5946\n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.2012 - val_accuracy: 0.0000e+00 - val_loss: 0.5891\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.2044 - val_accuracy: 0.0000e+00 - val_loss: 0.5854\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1965 - val_accuracy: 0.0000e+00 - val_loss: 0.5822\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1939 - val_accuracy: 0.0000e+00 - val_loss: 0.5796\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 0.1954 - val_accuracy: 0.0000e+00 - val_loss: 0.5763\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1834 - val_accuracy: 0.0000e+00 - val_loss: 0.5717\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1897 - val_accuracy: 0.0000e+00 - val_loss: 0.5674\n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 0.1857 - val_accuracy: 0.0000e+00 - val_loss: 0.5635\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1782 - val_accuracy: 0.0000e+00 - val_loss: 0.5587\n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1735 - val_accuracy: 0.0000e+00 - val_loss: 0.5553\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1779 - val_accuracy: 0.0000e+00 - val_loss: 0.5534\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1752 - val_accuracy: 0.0000e+00 - val_loss: 0.5504\n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.1762 - val_accuracy: 0.0000e+00 - val_loss: 0.5486\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1748 - val_accuracy: 0.0000e+00 - val_loss: 0.5473\n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1728 - val_accuracy: 0.0000e+00 - val_loss: 0.5443\n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1691 - val_accuracy: 0.0000e+00 - val_loss: 0.5410\n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1679 - val_accuracy: 0.0000e+00 - val_loss: 0.5397\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1722 - val_accuracy: 0.0000e+00 - val_loss: 0.5393\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1694 - val_accuracy: 0.0000e+00 - val_loss: 0.5393\n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1742 - val_accuracy: 0.0000e+00 - val_loss: 0.5391\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1583 - val_accuracy: 0.0000e+00 - val_loss: 0.5386\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1692 - val_accuracy: 0.0000e+00 - val_loss: 0.5388\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1718 - val_accuracy: 0.0000e+00 - val_loss: 0.5398\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1601 - val_accuracy: 0.0000e+00 - val_loss: 0.5412\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1702 - val_accuracy: 0.0000e+00 - val_loss: 0.5428\n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1683 - val_accuracy: 0.0000e+00 - val_loss: 0.5432\n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1707 - val_accuracy: 0.0000e+00 - val_loss: 0.5429\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1632 - val_accuracy: 0.0000e+00 - val_loss: 0.5407\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1689 - val_accuracy: 0.0000e+00 - val_loss: 0.5376\n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.1580 - val_accuracy: 0.0000e+00 - val_loss: 0.5368\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1543 - val_accuracy: 0.0000e+00 - val_loss: 0.5399\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1654 - val_accuracy: 0.0000e+00 - val_loss: 0.5447\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1632 - val_accuracy: 0.0000e+00 - val_loss: 0.5492\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1630 - val_accuracy: 0.0000e+00 - val_loss: 0.5560\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1640 - val_accuracy: 0.0000e+00 - val_loss: 0.5628\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1630 - val_accuracy: 0.0000e+00 - val_loss: 0.5680\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1581 - val_accuracy: 0.0000e+00 - val_loss: 0.5700\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1626 - val_accuracy: 0.0000e+00 - val_loss: 0.5683\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1612 - val_accuracy: 0.0000e+00 - val_loss: 0.5665\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1614 - val_accuracy: 0.0000e+00 - val_loss: 0.5657\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1524 - val_accuracy: 0.0000e+00 - val_loss: 0.5666\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1652 - val_accuracy: 0.0000e+00 - val_loss: 0.5693\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1604 - val_accuracy: 0.0000e+00 - val_loss: 0.5709\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1602 - val_accuracy: 0.0000e+00 - val_loss: 0.5706\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.0000e+00 - loss: 0.1553 - val_accuracy: 0.0000e+00 - val_loss: 0.5674\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1523 - val_accuracy: 0.0000e+00 - val_loss: 0.5647\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1562 - val_accuracy: 0.0000e+00 - val_loss: 0.5628\n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.1602 - val_accuracy: 0.0000e+00 - val_loss: 0.5593\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 0.1607 - val_accuracy: 0.0000e+00 - val_loss: 0.5569\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1589 - val_accuracy: 0.0000e+00 - val_loss: 0.5552\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1528 - val_accuracy: 0.0000e+00 - val_loss: 0.5551\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1537 - val_accuracy: 0.0000e+00 - val_loss: 0.5575\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1569 - val_accuracy: 0.0000e+00 - val_loss: 0.5610\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1567 - val_accuracy: 0.0000e+00 - val_loss: 0.5629\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.1529 - val_accuracy: 0.0000e+00 - val_loss: 0.5611\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.1587 - val_accuracy: 0.0000e+00 - val_loss: 0.5581\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "r2: 0.6913235000097534\n",
      "mse 0.5581178993412226\n"
     ]
    }
   ],
   "source": [
    "#una vez tenemos los ingresos de 2019, calculamos las ventas equivalentes. Usamos RN\n",
    "#primero comprobamos que modelo usar\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))  # Capa oculta con 64 neuronas y función de activación ReLU\n",
    "model.add(Dense(1))  # Capa de salida con una neurona (predicción de ingresos)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy']) \n",
    "model.fit(Y_trainEsc, X_trainEsc, epochs=100, batch_size=32,validation_data=(Y_testEsc,X_testEsc))\n",
    "prediccionesVentas_2018 = model.predict(Y_testEsc)\n",
    "print(\"r2:\",r2_score(X_testEsc,prediccionesVentas_2018))\n",
    "print(\"mse\",mean_squared_error(X_testEsc,prediccionesVentas_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba52f697-d487-4509-a4f6-2e189caa4ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7387825657658229\n",
      "0.4723071748924504\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(Y_trainEsc,X_trainEsc)\n",
    "X_predRLEsc = regr.predict(Y_testEsc)\n",
    "print(r2_score(X_testEsc,X_predRLEsc))\n",
    "print(mean_squared_error(X_testEsc,X_predRLEsc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cc9c288-a65c-495d-96d2-67509549a28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.44564193],\n",
       "       [-1.36450568],\n",
       "       [-0.62268853],\n",
       "       [-0.26337085],\n",
       "       [-0.44882513],\n",
       "       [-0.61109764],\n",
       "       [-0.52996138],\n",
       "       [-0.27496174],\n",
       "       [ 1.19708167],\n",
       "       [-0.20541638],\n",
       "       [ 1.61435382],\n",
       "       [ 1.49844489],\n",
       "       [-1.07473336],\n",
       "       [-1.14427871],\n",
       "       [-0.24018906],\n",
       "       [-0.15905281],\n",
       "       [ 0.45526452],\n",
       "       [ 0.16549219],\n",
       "       [ 0.16549219],\n",
       "       [-0.07791656],\n",
       "       [ 1.93889882],\n",
       "       [ 0.11912862],\n",
       "       [ 2.135944  ],\n",
       "       [ 1.811399  ],\n",
       "       [-0.44882513],\n",
       "       [-0.90086996],\n",
       "       [ 0.64071881],\n",
       "       [ 0.21185577],\n",
       "       [ 0.67549149],\n",
       "       [ 0.61753702],\n",
       "       [ 0.4784463 ],\n",
       "       [ 0.37412827],\n",
       "       [ 3.14435169],\n",
       "       [ 1.30139971],\n",
       "       [ 3.17912437],\n",
       "       [ 3.21389705]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparamos los datos. Entrenaremos al modelo con datos de 2016 a 2018 para predecir 2019\n",
    "X_trainP = np.concatenate((X_trainEsc[12:36],X_testEsc))#ventas 2016-2018\n",
    "Y_trainP = np.concatenate((Y_trainEsc[12:36],Y_testEsc))#ingresos 2016-2018\n",
    "Y_testP = scalerIng.transform(pd.DataFrame(y_predSE2019,columns = [\"Total\"]))#ingresos 2019\n",
    "X_trainP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3385aa28-c5fe-40db-8669-3abae4ddffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 2.8694  \n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 2.7375\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 2.6747\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 2.6370\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 2.5956\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 2.3752\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 2.4322\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 2.3851 \n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 2.2978\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 2.2380\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 2.0785\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.9368\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.8801\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.7556 \n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.8218\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.7513\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.5766\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.6006\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.5710\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.6012\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.5291\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.5485\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.4856\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.2482\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.4314\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 1.3910\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.2441 \n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.2968\n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.2776\n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 1.2568 \n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.2024\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.1300\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.1476\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 1.1288\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.0383\n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.0898\n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 1.0381 \n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 1.0207 \n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.9469\n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.8486\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.9383\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.8564\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.8947\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.8250 \n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.8582\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.7862\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.6999 \n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.7947\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.7585\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.7525 \n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.6501 \n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6706\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.6916 \n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6698\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.5939\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.5921 \n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.6223\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5948\n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5776\n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5777\n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.5560\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.5443\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5426\n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.5205\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.5123\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.5101\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.4544\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.4510\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.4791\n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.4723\n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.4608\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.4158\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.4114 \n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.4233\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.4251\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.4125\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.4036\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.4035\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.3978\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.3448\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.3810\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.3253\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.3294\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.3561\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.3553\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.2991\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.3389\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.3348\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.3202\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.3234\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.3142\n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.3117\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.3115\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.2978\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.3078\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.3002\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.2997\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.2924\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.2991\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.2949\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[234.],\n",
       "       [184.],\n",
       "       [302.],\n",
       "       [253.],\n",
       "       [287.],\n",
       "       [270.],\n",
       "       [272.],\n",
       "       [273.],\n",
       "       [348.],\n",
       "       [303.],\n",
       "       [374.],\n",
       "       [391.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#si no mejora, utilizar RL\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))  # Capa oculta con 64 neuronas y función de activación ReLU\n",
    "model.add(Dense(1))  # Capa de salida con una neurona (predicción de ingresos)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy']) \n",
    "model.fit(Y_trainP, X_trainP, epochs=100, batch_size=32)\n",
    "prediccionesVentas_2019Esc = model.predict(Y_testP)\n",
    "prediccionesVentas_2019 = np.round(scalerVent.inverse_transform(prediccionesVentas_2019Esc))\n",
    "prediccionesVentas_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3520d2e0-487e-4d1f-ae7d-19bc8f55e175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Total\n",
       "Order_Date       \n",
       "2018-01-31    143\n",
       "2018-02-28    104\n",
       "2018-03-31    237\n",
       "2018-04-30    200\n",
       "2018-05-31    240\n",
       "2018-06-30    235\n",
       "2018-07-31    223\n",
       "2018-08-31    214\n",
       "2018-09-30    453\n",
       "2018-10-31    294\n",
       "2018-11-30    456\n",
       "2018-12-31    459"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50660cb5-915f-4356-8163-a17079408fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>479.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Total\n",
       "2019-01-31  219.0\n",
       "2019-02-28  180.0\n",
       "2019-03-31  315.0\n",
       "2019-04-30  241.0\n",
       "2019-05-31  289.0\n",
       "2019-06-30  264.0\n",
       "2019-07-31  266.0\n",
       "2019-08-31  268.0\n",
       "2019-09-30  399.0\n",
       "2019-10-31  316.0\n",
       "2019-11-30  447.0\n",
       "2019-12-31  479.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(Y_trainP,X_trainP)\n",
    "X_predRLEsc2019 = regr.predict(Y_testP)\n",
    "X_predRL2019 = np.round(scalerVent.inverse_transform(X_predRLEsc2019))\n",
    "X_predRL2019 = pd.DataFrame(X_predRL2019,index = y_predSE2019.index, columns=[\"Total\"])\n",
    "X_predRL2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0236ad4-219c-468e-95a7-3cc8eeaa3426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.246753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.152174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.655844</td>\n",
       "      <td>0.207792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>0.223140</td>\n",
       "      <td>0.578512</td>\n",
       "      <td>0.198347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>0.160305</td>\n",
       "      <td>0.618321</td>\n",
       "      <td>0.221374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>0.190141</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>0.232394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>0.157534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>0.168539</td>\n",
       "      <td>0.621723</td>\n",
       "      <td>0.209738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.610063</td>\n",
       "      <td>0.201258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.196078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>0.178832</td>\n",
       "      <td>0.554745</td>\n",
       "      <td>0.266423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>0.232704</td>\n",
       "      <td>0.559748</td>\n",
       "      <td>0.207547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.209790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.596899</td>\n",
       "      <td>0.193798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.286765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>0.189873</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.202532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.638596</td>\n",
       "      <td>0.192982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>0.190031</td>\n",
       "      <td>0.573209</td>\n",
       "      <td>0.236760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>0.215434</td>\n",
       "      <td>0.581994</td>\n",
       "      <td>0.202572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>0.247191</td>\n",
       "      <td>0.505618</td>\n",
       "      <td>0.247191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>0.192771</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.228916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>0.229814</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.248447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>0.190045</td>\n",
       "      <td>0.624434</td>\n",
       "      <td>0.185520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.637755</td>\n",
       "      <td>0.178571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.647959</td>\n",
       "      <td>0.188776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>0.211429</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>0.128940</td>\n",
       "      <td>0.638968</td>\n",
       "      <td>0.232092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>0.151042</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.239583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>0.199454</td>\n",
       "      <td>0.579235</td>\n",
       "      <td>0.221311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>0.174556</td>\n",
       "      <td>0.582840</td>\n",
       "      <td>0.242604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>0.195804</td>\n",
       "      <td>0.650350</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.240385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>0.232068</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.160338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.220833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>0.182979</td>\n",
       "      <td>0.608511</td>\n",
       "      <td>0.208511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>0.219731</td>\n",
       "      <td>0.556054</td>\n",
       "      <td>0.224215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>0.168224</td>\n",
       "      <td>0.654206</td>\n",
       "      <td>0.177570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>0.198675</td>\n",
       "      <td>0.620309</td>\n",
       "      <td>0.181015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.564626</td>\n",
       "      <td>0.241497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.585526</td>\n",
       "      <td>0.217105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>0.163399</td>\n",
       "      <td>0.605664</td>\n",
       "      <td>0.230937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Technology  Office Supplies  Furniture\n",
       "2015-01-31    0.116883         0.636364   0.246753\n",
       "2015-02-28    0.173913         0.673913   0.152174\n",
       "2015-03-31    0.136364         0.655844   0.207792\n",
       "2015-04-30    0.184615         0.615385   0.200000\n",
       "2015-05-31    0.223140         0.578512   0.198347\n",
       "2015-06-30    0.160305         0.618321   0.221374\n",
       "2015-07-31    0.190141         0.577465   0.232394\n",
       "2015-08-31    0.178082         0.664384   0.157534\n",
       "2015-09-30    0.168539         0.621723   0.209738\n",
       "2015-10-31    0.188679         0.610063   0.201258\n",
       "2015-11-30    0.196078         0.607843   0.196078\n",
       "2015-12-31    0.178832         0.554745   0.266423\n",
       "2016-01-31    0.175439         0.561404   0.263158\n",
       "2016-02-29    0.218750         0.609375   0.171875\n",
       "2016-03-31    0.210938         0.585938   0.203125\n",
       "2016-04-30    0.232704         0.559748   0.207547\n",
       "2016-05-31    0.153846         0.636364   0.209790\n",
       "2016-06-30    0.209302         0.596899   0.193798\n",
       "2016-07-31    0.154412         0.558824   0.286765\n",
       "2016-08-31    0.189873         0.607595   0.202532\n",
       "2016-09-30    0.168421         0.638596   0.192982\n",
       "2016-10-31    0.250000         0.536585   0.213415\n",
       "2016-11-30    0.190031         0.573209   0.236760\n",
       "2016-12-31    0.215434         0.581994   0.202572\n",
       "2017-01-31    0.247191         0.505618   0.247191\n",
       "2017-02-28    0.192771         0.578313   0.228916\n",
       "2017-03-31    0.229814         0.521739   0.248447\n",
       "2017-04-30    0.130952         0.660714   0.208333\n",
       "2017-05-31    0.190045         0.624434   0.185520\n",
       "2017-06-30    0.183673         0.637755   0.178571\n",
       "2017-07-31    0.163265         0.647959   0.188776\n",
       "2017-08-31    0.211429         0.628571   0.160000\n",
       "2017-09-30    0.128940         0.638968   0.232092\n",
       "2017-10-31    0.151042         0.609375   0.239583\n",
       "2017-11-30    0.199454         0.579235   0.221311\n",
       "2017-12-31    0.174556         0.582840   0.242604\n",
       "2018-01-31    0.195804         0.650350   0.153846\n",
       "2018-02-28    0.201923         0.557692   0.240385\n",
       "2018-03-31    0.232068         0.607595   0.160338\n",
       "2018-04-30    0.125000         0.655000   0.220000\n",
       "2018-05-31    0.175000         0.604167   0.220833\n",
       "2018-06-30    0.182979         0.608511   0.208511\n",
       "2018-07-31    0.219731         0.556054   0.224215\n",
       "2018-08-31    0.168224         0.654206   0.177570\n",
       "2018-09-30    0.198675         0.620309   0.181015\n",
       "2018-10-31    0.193878         0.564626   0.241497\n",
       "2018-11-30    0.197368         0.585526   0.217105\n",
       "2018-12-31    0.163399         0.605664   0.230937"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorcentajeTecnología = df_resultadoVent2[\"Technology\"] / df_resultadoVent2[\"Ventas Totales\"]\n",
    "PorcentajeOfficeSupplies = df_resultadoVent2[\"Office Supplies\"] / df_resultadoVent2[\"Ventas Totales\"]\n",
    "PorcentajeFurniture = df_resultadoVent2[\"Furniture\"] / df_resultadoVent2[\"Ventas Totales\"]\n",
    "array_concatenado = np.column_stack((PorcentajeTecnología,PorcentajeOfficeSupplies,PorcentajeFurniture))\n",
    "pdArray_concatenado = pd.DataFrame(array_concatenado,index=df_resultadoVent2.index.values,columns=[\"Technology\",\"Office Supplies\",\"Furniture\"])\n",
    "pdArray_concatenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96b3f7ef-a7e8-4c17-89f0-495c2f3a64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#una vez tenemos las ventas de 2019, calculamos las ventas de cada categoría\n",
    "pdArray_concatenadoTrain = pdArray_concatenado[12:48]\n",
    "df_resultadoIng2 = df_resultadoIng2[12:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "777ea0f8-e5dc-43e6-b24e-22d7340e6b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.34105725e-03,  1.16206653e-03, -1.09706603e-04],\n",
       "       [ 6.55941763e-02, -5.68393309e-02,  5.36600078e-03],\n",
       "       [-1.68804044e-01,  1.46273792e-01, -1.38091929e-02],\n",
       "       [-3.94082782e-02,  3.41484609e-02, -3.22383576e-03],\n",
       "       [-1.24690411e-01,  1.08047999e-01, -1.02004306e-02],\n",
       "       [-7.94134624e-02,  6.88141589e-02, -6.49650205e-03],\n",
       "       [-8.38639840e-02,  7.26706700e-02, -6.86058167e-03],\n",
       "       [-8.67663097e-02,  7.51856227e-02, -7.09800948e-03],\n",
       "       [-3.15710967e-01,  2.73573069e-01, -2.58270686e-02],\n",
       "       [-1.71603872e-01,  1.48699927e-01, -1.40382357e-02],\n",
       "       [-3.99004823e-01,  3.45749706e-01, -3.26410104e-02],\n",
       "       [-4.55595910e-01,  3.94787589e-01, -3.72705040e-02]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predecimos las ventas por categoría, \n",
    "scalerVentasCat = StandardScaler()\n",
    "VentasCatEsc = scalerVentasCat.fit_transform(pdArray_concatenadoTrain)\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_trainP,VentasCatEsc)\n",
    "X_predRLCatEsc = regr.predict(X_predRLEsc2019)\n",
    "X_predRLCatEsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86c9a67f-4c80-497d-aa48-9736ad356251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>0.189579</td>\n",
       "      <td>0.598149</td>\n",
       "      <td>0.212272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>0.191658</td>\n",
       "      <td>0.595906</td>\n",
       "      <td>0.212436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>0.184376</td>\n",
       "      <td>0.603761</td>\n",
       "      <td>0.211863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>0.188396</td>\n",
       "      <td>0.599425</td>\n",
       "      <td>0.212179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>0.185746</td>\n",
       "      <td>0.602283</td>\n",
       "      <td>0.211971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>0.187153</td>\n",
       "      <td>0.600765</td>\n",
       "      <td>0.212082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>0.187015</td>\n",
       "      <td>0.600915</td>\n",
       "      <td>0.212071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>0.186925</td>\n",
       "      <td>0.601012</td>\n",
       "      <td>0.212064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>0.179812</td>\n",
       "      <td>0.608684</td>\n",
       "      <td>0.211504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>0.184289</td>\n",
       "      <td>0.603855</td>\n",
       "      <td>0.211856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>0.177224</td>\n",
       "      <td>0.611475</td>\n",
       "      <td>0.211301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.175466</td>\n",
       "      <td>0.613371</td>\n",
       "      <td>0.211163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Technology  Office Supplies  Furniture\n",
       "2019-01-31    0.189579         0.598149   0.212272\n",
       "2019-02-28    0.191658         0.595906   0.212436\n",
       "2019-03-31    0.184376         0.603761   0.211863\n",
       "2019-04-30    0.188396         0.599425   0.212179\n",
       "2019-05-31    0.185746         0.602283   0.211971\n",
       "2019-06-30    0.187153         0.600765   0.212082\n",
       "2019-07-31    0.187015         0.600915   0.212071\n",
       "2019-08-31    0.186925         0.601012   0.212064\n",
       "2019-09-30    0.179812         0.608684   0.211504\n",
       "2019-10-31    0.184289         0.603855   0.211856\n",
       "2019-11-30    0.177224         0.611475   0.211301\n",
       "2019-12-31    0.175466         0.613371   0.211163"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predRLCat = scalerVentasCat.inverse_transform(X_predRLCatEsc)\n",
    "X_predRLCat = pd.DataFrame(X_predRLCat,index = y_predSE2019.index,columns = pdArray_concatenado.columns)\n",
    "X_predRLCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5c529e5-ba6b-4065-a11f-0b1654a1fc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>42.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>34.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>58.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>45.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>54.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>49.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>50.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>50.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>72.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>58.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>79.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>84.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Technology  Office Supplies  Furniture\n",
       "2019-01-31        42.0            131.0       46.0\n",
       "2019-02-28        34.0            107.0       38.0\n",
       "2019-03-31        58.0            190.0       67.0\n",
       "2019-04-30        45.0            144.0       51.0\n",
       "2019-05-31        54.0            174.0       61.0\n",
       "2019-06-30        49.0            159.0       56.0\n",
       "2019-07-31        50.0            160.0       56.0\n",
       "2019-08-31        50.0            161.0       57.0\n",
       "2019-09-30        72.0            243.0       84.0\n",
       "2019-10-31        58.0            191.0       67.0\n",
       "2019-11-30        79.0            273.0       94.0\n",
       "2019-12-31        84.0            294.0      101.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiplicamos las predicciones por las ventas totales de 2019 para calcular las ventas mensuales de 2019 por categoría\n",
    "prediccionVentas2019Cat = np.round(X_predRL2019.values * X_predRLCat.values)\n",
    "prediccionVentas2019Cat = pd.DataFrame(prediccionVentas2019Cat,index = y_predSE2019.index, columns = pdArray_concatenado.columns)\n",
    "prediccionVentas2019Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b255630-9abe-45c6-96db-fcd24518f53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>3143.290</td>\n",
       "      <td>4845.140</td>\n",
       "      <td>6217.2770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>1608.510</td>\n",
       "      <td>1071.724</td>\n",
       "      <td>1839.6580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>32359.974</td>\n",
       "      <td>8602.455</td>\n",
       "      <td>14243.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>8973.144</td>\n",
       "      <td>10988.874</td>\n",
       "      <td>7944.8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>9599.876</td>\n",
       "      <td>7131.640</td>\n",
       "      <td>6912.7870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>8435.965</td>\n",
       "      <td>12742.389</td>\n",
       "      <td>13144.5816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>7839.284</td>\n",
       "      <td>15121.208</td>\n",
       "      <td>10821.0510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>8937.050</td>\n",
       "      <td>11055.276</td>\n",
       "      <td>7125.2105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>30383.748</td>\n",
       "      <td>27423.298</td>\n",
       "      <td>23816.4808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>11938.018</td>\n",
       "      <td>7211.128</td>\n",
       "      <td>12304.2470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>30073.424</td>\n",
       "      <td>26363.196</td>\n",
       "      <td>21471.0407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>20573.224</td>\n",
       "      <td>16956.492</td>\n",
       "      <td>30637.3425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>4518.236</td>\n",
       "      <td>1808.780</td>\n",
       "      <td>11739.9416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>3448.970</td>\n",
       "      <td>5368.067</td>\n",
       "      <td>3134.3740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>10143.942</td>\n",
       "      <td>13472.739</td>\n",
       "      <td>8722.6374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>11160.952</td>\n",
       "      <td>12517.818</td>\n",
       "      <td>10475.6985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>11643.000</td>\n",
       "      <td>8941.580</td>\n",
       "      <td>9374.9505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>6435.366</td>\n",
       "      <td>9623.037</td>\n",
       "      <td>7540.9710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>10234.976</td>\n",
       "      <td>4706.243</td>\n",
       "      <td>13667.0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>15444.642</td>\n",
       "      <td>11735.108</td>\n",
       "      <td>9638.5922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>19017.128</td>\n",
       "      <td>19031.055</td>\n",
       "      <td>25085.4230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>10704.890</td>\n",
       "      <td>8642.360</td>\n",
       "      <td>11664.4875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>23873.601</td>\n",
       "      <td>21178.298</td>\n",
       "      <td>30197.5005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>35632.028</td>\n",
       "      <td>16099.322</td>\n",
       "      <td>22812.2512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>5620.066</td>\n",
       "      <td>5299.682</td>\n",
       "      <td>7622.7430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>12258.914</td>\n",
       "      <td>6794.350</td>\n",
       "      <td>3925.5510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>21567.852</td>\n",
       "      <td>17324.545</td>\n",
       "      <td>12272.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>14890.502</td>\n",
       "      <td>10577.175</td>\n",
       "      <td>13212.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>28832.691</td>\n",
       "      <td>12931.642</td>\n",
       "      <td>14892.5750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>16372.152</td>\n",
       "      <td>10901.810</td>\n",
       "      <td>12450.5240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>12847.200</td>\n",
       "      <td>12903.824</td>\n",
       "      <td>12569.7590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>9672.402</td>\n",
       "      <td>8959.740</td>\n",
       "      <td>11910.0583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>19424.514</td>\n",
       "      <td>23013.402</td>\n",
       "      <td>26755.4749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>31533.374</td>\n",
       "      <td>16236.650</td>\n",
       "      <td>11813.0090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>27141.059</td>\n",
       "      <td>20141.808</td>\n",
       "      <td>31783.6288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>21801.218</td>\n",
       "      <td>37332.938</td>\n",
       "      <td>36604.9650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>16493.333</td>\n",
       "      <td>21052.979</td>\n",
       "      <td>5930.1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>5768.448</td>\n",
       "      <td>7378.172</td>\n",
       "      <td>6774.3774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>33428.622</td>\n",
       "      <td>14541.346</td>\n",
       "      <td>10893.4448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>12261.005</td>\n",
       "      <td>14214.947</td>\n",
       "      <td>9065.9581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>13374.620</td>\n",
       "      <td>13493.804</td>\n",
       "      <td>16957.5582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>16937.240</td>\n",
       "      <td>14221.101</td>\n",
       "      <td>17032.3867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>23209.926</td>\n",
       "      <td>10239.356</td>\n",
       "      <td>11375.8220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>17619.162</td>\n",
       "      <td>29799.564</td>\n",
       "      <td>15419.1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>26028.659</td>\n",
       "      <td>31607.523</td>\n",
       "      <td>28516.7060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>32855.663</td>\n",
       "      <td>22708.400</td>\n",
       "      <td>21884.0682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>49409.103</td>\n",
       "      <td>31472.337</td>\n",
       "      <td>37056.7150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>21984.910</td>\n",
       "      <td>29638.012</td>\n",
       "      <td>31407.4668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category    Technology  Office Supplies   Furniture\n",
       "Order_Date                                         \n",
       "2015-01-31    3143.290         4845.140   6217.2770\n",
       "2015-02-28    1608.510         1071.724   1839.6580\n",
       "2015-03-31   32359.974         8602.455  14243.3680\n",
       "2015-04-30    8973.144        10988.874   7944.8370\n",
       "2015-05-31    9599.876         7131.640   6912.7870\n",
       "2015-06-30    8435.965        12742.389  13144.5816\n",
       "2015-07-31    7839.284        15121.208  10821.0510\n",
       "2015-08-31    8937.050        11055.276   7125.2105\n",
       "2015-09-30   30383.748        27423.298  23816.4808\n",
       "2015-10-31   11938.018         7211.128  12304.2470\n",
       "2015-11-30   30073.424        26363.196  21471.0407\n",
       "2015-12-31   20573.224        16956.492  30637.3425\n",
       "2016-01-31    4518.236         1808.780  11739.9416\n",
       "2016-02-29    3448.970         5368.067   3134.3740\n",
       "2016-03-31   10143.942        13472.739   8722.6374\n",
       "2016-04-30   11160.952        12517.818  10475.6985\n",
       "2016-05-31   11643.000         8941.580   9374.9505\n",
       "2016-06-30    6435.366         9623.037   7540.9710\n",
       "2016-07-31   10234.976         4706.243  13667.0400\n",
       "2016-08-31   15444.642        11735.108   9638.5922\n",
       "2016-09-30   19017.128        19031.055  25085.4230\n",
       "2016-10-31   10704.890         8642.360  11664.4875\n",
       "2016-11-30   23873.601        21178.298  30197.5005\n",
       "2016-12-31   35632.028        16099.322  22812.2512\n",
       "2017-01-31    5620.066         5299.682   7622.7430\n",
       "2017-02-28   12258.914         6794.350   3925.5510\n",
       "2017-03-31   21567.852        17324.545  12272.6620\n",
       "2017-04-30   14890.502        10577.175  13212.0900\n",
       "2017-05-31   28832.691        12931.642  14892.5750\n",
       "2017-06-30   16372.152        10901.810  12450.5240\n",
       "2017-07-31   12847.200        12903.824  12569.7590\n",
       "2017-08-31    9672.402         8959.740  11910.0583\n",
       "2017-09-30   19424.514        23013.402  26755.4749\n",
       "2017-10-31   31533.374        16236.650  11813.0090\n",
       "2017-11-30   27141.059        20141.808  31783.6288\n",
       "2017-12-31   21801.218        37332.938  36604.9650\n",
       "2018-01-31   16493.333        21052.979   5930.1620\n",
       "2018-02-28    5768.448         7378.172   6774.3774\n",
       "2018-03-31   33428.622        14541.346  10893.4448\n",
       "2018-04-30   12261.005        14214.947   9065.9581\n",
       "2018-05-31   13374.620        13493.804  16957.5582\n",
       "2018-06-30   16937.240        14221.101  17032.3867\n",
       "2018-07-31   23209.926        10239.356  11375.8220\n",
       "2018-08-31   17619.162        29799.564  15419.1220\n",
       "2018-09-30   26028.659        31607.523  28516.7060\n",
       "2018-10-31   32855.663        22708.400  21884.0682\n",
       "2018-11-30   49409.103        31472.337  37056.7150\n",
       "2018-12-31   21984.910        29638.012  31407.4668"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ingresos por categoría\n",
    "#hago una predicción entre ventas por categoría e ingresos por categoría. Tengo que obtener los ingresos por categoría entre 2016 y 2018\n",
    "#preparo los datos y luego pruebo con algoritmos rf, rl y rn\n",
    "\n",
    "df_resultadoIng2 = df3.groupby('Category').resample(\"ME\",on=\"Order_Date\").sum() #ingresos de entrenamiento\n",
    "df_resultadoIng2 = df_resultadoIng2[[\"Total\"]]\n",
    "df_resultadoIng2 = df_resultadoIng2.unstack(level=0)\n",
    "df_resultadoIng2.columns = df_resultadoIng2.columns.droplevel(0)\n",
    "df_resultadoIng2 = df_resultadoIng2[[\"Technology\",\"Office Supplies\",\"Furniture\"]]\n",
    "df_resultadoIng2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a4d0b05-6566-49bf-bfa4-850d7844428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerIngCat = StandardScaler()\n",
    "IngCatTrain = df_resultadoIng2[0:36] #ingresos por categoría de 2015 a 2017\n",
    "IngCatTest = df_resultadoIng2[36:48] #ingresos por categoría 2018\n",
    "VenCatTrain = pdArray_concatenado[0:36] #ventas por categoría 2015 a 2017\n",
    "VenCatTest = pdArray_concatenado[36:48] #ventas por categoría 2018\n",
    "IngCatTrainEsc = scalerIngCat.fit_transform(IngCatTrain) #ingresos categoria de entrenamiento\n",
    "IngCatTestEsc = scalerIngCat.transform(IngCatTest) #ingresos categoria de test\n",
    "VenCatTrainEsc = scalerVentasCat.transform(VenCatTrain) #ingresos categoria de entrenamiento\n",
    "VenCatTestEsc = scalerVentasCat.transform(VenCatTest) #ingresos categoria de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9d3525b-40ef-4f92-94a4-c543046cb6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4502 - loss: 1.0394  \n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4606 - loss: 1.0265\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4606 - loss: 1.0575\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 1.0326\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4711 - loss: 0.9766\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4606 - loss: 1.0415\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4606 - loss: 1.0171\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 1.0036\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 0.9859\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.9267\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3947 - loss: 0.9049\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4236 - loss: 0.9798\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4340 - loss: 0.9609\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4132 - loss: 0.9640\n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3657 - loss: 0.9489\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3947 - loss: 0.9502\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4236 - loss: 0.9658\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.9503\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.9642\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5000 - loss: 0.8868\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4896 - loss: 0.9556\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5000 - loss: 0.9360\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4792 - loss: 0.9447 \n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4896 - loss: 0.9446\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4896 - loss: 0.9450\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5104 - loss: 0.9360\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 0.9599 \n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4919 - loss: 0.9360\n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.9155\n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.9269\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4317 - loss: 0.9231\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4317 - loss: 0.9236\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4317 - loss: 0.8860\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4132 - loss: 0.9071 \n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4132 - loss: 0.9354\n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4132 - loss: 0.9294\n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4340 - loss: 0.8772\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4236 - loss: 0.9280\n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4236 - loss: 0.9068\n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.9009\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.9244\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.9437\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4317 - loss: 0.9326\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.9106\n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.9165\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.9218\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4317 - loss: 0.9114\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.8620\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.8878\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4525 - loss: 0.9239\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4815 - loss: 0.8960\n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 0.9209\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 0.8814\n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4919 - loss: 0.9005\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.9253\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.9056\n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.8908\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.8505 \n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.9148 \n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4606 - loss: 0.8906 \n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 0.8917\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.8582\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4606 - loss: 0.9102 \n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4606 - loss: 0.8840\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.8539\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.9133\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.8905\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.8771\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.9124\n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4317 - loss: 0.8594 \n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 0.8841\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4711 - loss: 0.8796\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 0.8503\n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.8944\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4317 - loss: 0.8841\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4630 - loss: 0.9073\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4317 - loss: 0.8987\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.8989\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4421 - loss: 0.8692\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4815 - loss: 0.8886\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4711 - loss: 0.8831\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4606 - loss: 0.8958\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4606 - loss: 0.8544\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 0.8801\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4711 - loss: 0.8756\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4606 - loss: 0.8517\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4606 - loss: 0.8694\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4525 - loss: 0.8874\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4317 - loss: 0.8175\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.8809 \n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.8629 \n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4630 - loss: 0.8816\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.8941\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.8605\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4630 - loss: 0.8432\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4525 - loss: 0.8707\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4525 - loss: 0.8760\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.8771\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4317 - loss: 0.8817\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4525 - loss: 0.8803\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    }
   ],
   "source": [
    "#Una vez escalados los datos, probamos qué algoritmo da mejores resultados\n",
    "#RL\n",
    "regrIngCat = linear_model.LinearRegression()\n",
    "regrIngCat.fit(VenCatTrainEsc,IngCatTrainEsc)\n",
    "predIngCat2018RLEsc = regrIngCat.predict(VenCatTestEsc)\n",
    "\n",
    "#RF\n",
    "rf_modelIngCat = RandomForestRegressor(n_estimators=100,random_state=4)\n",
    "rf_modelIngCat.fit(VenCatTrainEsc, IngCatTrainEsc)\n",
    "predIngCat2018RFEsc = rf_modelIngCat.predict(VenCatTestEsc)\n",
    "\n",
    "#RN\n",
    "modelRNIngCat = Sequential()\n",
    "modelRNIngCat.add(Dense(64, input_dim=3, activation='relu'))  # Capa oculta con 64 neuronas y función de activación ReLU\n",
    "modelRNIngCat.add(Dense(3))  # Capa de salida con una neurona (predicción de ingresos)\n",
    "modelRNIngCat.compile(optimizer='adam', loss='mse', metrics=['accuracy']) \n",
    "modelRNIngCat.fit(VenCatTrainEsc, IngCatTrainEsc, epochs=100, batch_size=32)\n",
    "predIngCat2018RNEsc = modelRNIngCat.predict(VenCatTestEsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9062b9dc-d874-4959-b328-cb6b91088863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El coeficiente de determinación del algoritmo de regresión lineal con el valor real es de:  0.7276003992506132\n",
      "El coeficiente de determinación del algoritmo random forest con el valor real es de:  0.6483973326956496\n",
      "El coeficiente de determinación del algoritmo de red neuronal con el valor real es de:  0.7451878314825354\n"
     ]
    }
   ],
   "source": [
    "#comparamos resultados\n",
    "r2RLIC = r2_score(IngCatTestEsc, predIngCat2018RLEsc)\n",
    "r2RFIC = r2_score(IngCatTestEsc, predIngCat2018RFEsc)\n",
    "r2RNIC = r2_score(IngCatTestEsc, predIngCat2018RNEsc)\n",
    "print(\"El coeficiente de determinación del algoritmo de regresión lineal con el valor real es de: \",r2RL)\n",
    "print(\"El coeficiente de determinación del algoritmo random forest con el valor real es de: \",r2RF)\n",
    "print(\"El coeficiente de determinación del algoritmo de red neuronal con el valor real es de: \",r2RN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b4e2b42-11cb-4755-9bf7-aa3218f38d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error cuadrático medio del algoritmo de regresión lineal con el valor real es de:  1.8250182128798855\n",
      "El error cuadrático medio del algoritmo random forest con el valor real es de:  1.672770731321462\n",
      "El error cuadrático medio del algoritmo de red neuronal con el valor real es de:  2.05008738929396\n"
     ]
    }
   ],
   "source": [
    "#comparamos resultados\n",
    "mseRLIC = mean_squared_error(IngCatTestEsc, predIngCat2018RLEsc)\n",
    "mseRFIC = mean_squared_error(IngCatTestEsc, predIngCat2018RFEsc)\n",
    "mseRNIC = mean_squared_error(IngCatTestEsc, predIngCat2018RNEsc)\n",
    "print(\"El error cuadrático medio del algoritmo de regresión lineal con el valor real es de: \",mseRLIC)\n",
    "print(\"El error cuadrático medio del algoritmo random forest con el valor real es de: \",mseRFIC)\n",
    "print(\"El error cuadrático medio del algoritmo de red neuronal con el valor real es de: \",mseRNIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4723da5c-c63e-479e-83b1-6f29396c0be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablomunozdelorenzo/.pyenv/versions/3.12.2/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3079 - loss: 1.7071  \n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2975 - loss: 1.6285\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2975 - loss: 1.5854 \n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3079 - loss: 1.5103\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3079 - loss: 1.3617\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2870 - loss: 1.4188 \n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3079 - loss: 1.2845\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3079 - loss: 1.3114\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3183 - loss: 1.2427\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3368 - loss: 1.1447\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3264 - loss: 1.1397\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2975 - loss: 1.0556\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2870 - loss: 1.0640\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3183 - loss: 1.0468\n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3183 - loss: 0.9971\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3079 - loss: 0.9452\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2975 - loss: 0.9224\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3368 - loss: 0.8968\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3264 - loss: 0.8469\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3368 - loss: 0.8356\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3264 - loss: 0.8003\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3762 - loss: 0.7232\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3657 - loss: 0.7400\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3947 - loss: 0.7149\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3264 - loss: 0.7021\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3472 - loss: 0.6625\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3368 - loss: 0.6543 \n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3160 - loss: 0.6293 \n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3657 - loss: 0.5637 \n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3553 - loss: 0.5944\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3657 - loss: 0.5796 \n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3472 - loss: 0.5422\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3368 - loss: 0.5361\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3264 - loss: 0.4993\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3843 - loss: 0.4674 \n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4236 - loss: 0.4905 \n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4132 - loss: 0.4929\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4051 - loss: 0.4759 \n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4051 - loss: 0.4718 \n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3738 - loss: 0.4469\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3947 - loss: 0.4473 \n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4132 - loss: 0.4348\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.4304\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4711 - loss: 0.3994\n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5000 - loss: 0.4082\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4028 - loss: 0.3956\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3924 - loss: 0.3927\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3738 - loss: 0.3847\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3843 - loss: 0.3928\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.3839\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4213 - loss: 0.3752\n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4606 - loss: 0.3510\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4525 - loss: 0.3651\n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4630 - loss: 0.3740\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4317 - loss: 0.3777\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4317 - loss: 0.3640\n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.3606\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4525 - loss: 0.3676\n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.3672\n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4421 - loss: 0.3684\n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5000 - loss: 0.3509\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4896 - loss: 0.3496\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5000 - loss: 0.3436\n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5104 - loss: 0.3609\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4896 - loss: 0.3371\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4606 - loss: 0.3453\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.3411\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4919 - loss: 0.3399\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4606 - loss: 0.3465\n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4711 - loss: 0.3362\n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.3394\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.3306\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4815 - loss: 0.3459\n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4919 - loss: 0.3422\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.3405\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5104 - loss: 0.3424\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4792 - loss: 0.3480\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4896 - loss: 0.3443\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5208 - loss: 0.3308\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4896 - loss: 0.3349\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5185 - loss: 0.3415\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5000 - loss: 0.3361\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4896 - loss: 0.3316\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5000 - loss: 0.3418\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5000 - loss: 0.3422\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4711 - loss: 0.3443\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4815 - loss: 0.3262\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4711 - loss: 0.3363\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4815 - loss: 0.3390\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4502 - loss: 0.3373\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 0.3342\n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.3235\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.3104\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4711 - loss: 0.3192\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5394 - loss: 0.3136\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5185 - loss: 0.3342\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5185 - loss: 0.3247\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5081 - loss: 0.3136\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5185 - loss: 0.3335\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5289 - loss: 0.3132\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x2a0e8a5c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    }
   ],
   "source": [
    "#volvemos a predecir, usando RL\n",
    "scalerVentasCat2 = StandardScaler()\n",
    "VenCatTrain = df_resultadoVent2[[\"Technology\",\"Office Supplies\",\"Furniture\"]][12:48] #ventas 2016 a 2018\n",
    "VenCatTest = prediccionVentas2019Cat #ventas categoría 2019\n",
    "IngCatTrain = df_resultadoIng2[12:48]\n",
    "#escalamos los datos\n",
    "VenCatTrainEsc = scalerVentasCat2.fit_transform(VenCatTrain)\n",
    "VenCatTestEsc =  scalerVentasCat2.transform(VenCatTest)\n",
    "IngCatTrainEsc = scalerIngCat.transform(IngCatTrain)\n",
    "#predecimos los ingresos por categoría de 2019\n",
    "modelRNIngCat2019 = Sequential()\n",
    "modelRNIngCat2019.add(Dense(64, input_dim=3, activation='relu'))  # Capa oculta con 64 neuronas y función de activación ReLU\n",
    "modelRNIngCat2019.add(Dense(3))  # Capa de salida con una neurona (predicción de ingresos)\n",
    "modelRNIngCat2019.compile(optimizer='adam', loss='mse', metrics=['accuracy']) \n",
    "modelRNIngCat2019.fit(VenCatTrainEsc, IngCatTrainEsc, epochs=100, batch_size=32)\n",
    "predIngCat2019RNEsc = modelRNIngCat2019.predict(VenCatTestEsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "776a45bf-71a8-4ae5-a2e1-0abe90fac21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>18001.996094</td>\n",
       "      <td>14966.815430</td>\n",
       "      <td>16432.865234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>13572.521484</td>\n",
       "      <td>12090.882812</td>\n",
       "      <td>11684.101562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>24136.105469</td>\n",
       "      <td>21763.437500</td>\n",
       "      <td>24587.945312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>19440.990234</td>\n",
       "      <td>16538.066406</td>\n",
       "      <td>18931.869141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>22602.402344</td>\n",
       "      <td>19952.802734</td>\n",
       "      <td>22610.496094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>20912.689453</td>\n",
       "      <td>18312.994141</td>\n",
       "      <td>20782.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>21158.576172</td>\n",
       "      <td>18391.859375</td>\n",
       "      <td>20889.351562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>21252.181641</td>\n",
       "      <td>18586.609375</td>\n",
       "      <td>21139.511719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>29127.207031</td>\n",
       "      <td>27376.236328</td>\n",
       "      <td>30516.083984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>24141.535156</td>\n",
       "      <td>21815.886719</td>\n",
       "      <td>24605.310547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>31799.398438</td>\n",
       "      <td>30582.130859</td>\n",
       "      <td>33853.957031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>33698.417969</td>\n",
       "      <td>32830.125000</td>\n",
       "      <td>36187.324219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category      Technology  Office Supplies     Furniture\n",
       "2019-01-31  18001.996094     14966.815430  16432.865234\n",
       "2019-02-28  13572.521484     12090.882812  11684.101562\n",
       "2019-03-31  24136.105469     21763.437500  24587.945312\n",
       "2019-04-30  19440.990234     16538.066406  18931.869141\n",
       "2019-05-31  22602.402344     19952.802734  22610.496094\n",
       "2019-06-30  20912.689453     18312.994141  20782.781250\n",
       "2019-07-31  21158.576172     18391.859375  20889.351562\n",
       "2019-08-31  21252.181641     18586.609375  21139.511719\n",
       "2019-09-30  29127.207031     27376.236328  30516.083984\n",
       "2019-10-31  24141.535156     21815.886719  24605.310547\n",
       "2019-11-30  31799.398438     30582.130859  33853.957031\n",
       "2019-12-31  33698.417969     32830.125000  36187.324219"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predIngCat2019RN = scalerIngCat.inverse_transform(predIngCat2019RNEsc)\n",
    "predIngCat2019 = pd.DataFrame(predIngCat2019RN,columns = df_resultadoIng2.columns, index = prediccionVentas2019Cat.index.values)\n",
    "predIngCat2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d1eef0b-f874-42a2-970e-db53e6989d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>3143.290000</td>\n",
       "      <td>4845.140000</td>\n",
       "      <td>6217.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>1608.510000</td>\n",
       "      <td>1071.724000</td>\n",
       "      <td>1839.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>32359.974000</td>\n",
       "      <td>8602.455000</td>\n",
       "      <td>14243.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>8973.144000</td>\n",
       "      <td>10988.874000</td>\n",
       "      <td>7944.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>9599.876000</td>\n",
       "      <td>7131.640000</td>\n",
       "      <td>6912.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>8435.965000</td>\n",
       "      <td>12742.389000</td>\n",
       "      <td>13144.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>7839.284000</td>\n",
       "      <td>15121.208000</td>\n",
       "      <td>10821.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>8937.050000</td>\n",
       "      <td>11055.276000</td>\n",
       "      <td>7125.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>30383.748000</td>\n",
       "      <td>27423.298000</td>\n",
       "      <td>23816.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>11938.018000</td>\n",
       "      <td>7211.128000</td>\n",
       "      <td>12304.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>30073.424000</td>\n",
       "      <td>26363.196000</td>\n",
       "      <td>21471.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>20573.224000</td>\n",
       "      <td>16956.492000</td>\n",
       "      <td>30637.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>4518.236000</td>\n",
       "      <td>1808.780000</td>\n",
       "      <td>11739.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>3448.970000</td>\n",
       "      <td>5368.067000</td>\n",
       "      <td>3134.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>10143.942000</td>\n",
       "      <td>13472.739000</td>\n",
       "      <td>8722.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>11160.952000</td>\n",
       "      <td>12517.818000</td>\n",
       "      <td>10475.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>11643.000000</td>\n",
       "      <td>8941.580000</td>\n",
       "      <td>9374.950500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>6435.366000</td>\n",
       "      <td>9623.037000</td>\n",
       "      <td>7540.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>10234.976000</td>\n",
       "      <td>4706.243000</td>\n",
       "      <td>13667.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>15444.642000</td>\n",
       "      <td>11735.108000</td>\n",
       "      <td>9638.592200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>19017.128000</td>\n",
       "      <td>19031.055000</td>\n",
       "      <td>25085.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>10704.890000</td>\n",
       "      <td>8642.360000</td>\n",
       "      <td>11664.487500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>23873.601000</td>\n",
       "      <td>21178.298000</td>\n",
       "      <td>30197.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>35632.028000</td>\n",
       "      <td>16099.322000</td>\n",
       "      <td>22812.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>5620.066000</td>\n",
       "      <td>5299.682000</td>\n",
       "      <td>7622.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>12258.914000</td>\n",
       "      <td>6794.350000</td>\n",
       "      <td>3925.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>21567.852000</td>\n",
       "      <td>17324.545000</td>\n",
       "      <td>12272.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>14890.502000</td>\n",
       "      <td>10577.175000</td>\n",
       "      <td>13212.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>28832.691000</td>\n",
       "      <td>12931.642000</td>\n",
       "      <td>14892.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>16372.152000</td>\n",
       "      <td>10901.810000</td>\n",
       "      <td>12450.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>12847.200000</td>\n",
       "      <td>12903.824000</td>\n",
       "      <td>12569.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>9672.402000</td>\n",
       "      <td>8959.740000</td>\n",
       "      <td>11910.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>19424.514000</td>\n",
       "      <td>23013.402000</td>\n",
       "      <td>26755.474900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>31533.374000</td>\n",
       "      <td>16236.650000</td>\n",
       "      <td>11813.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>27141.059000</td>\n",
       "      <td>20141.808000</td>\n",
       "      <td>31783.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>21801.218000</td>\n",
       "      <td>37332.938000</td>\n",
       "      <td>36604.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>16493.333000</td>\n",
       "      <td>21052.979000</td>\n",
       "      <td>5930.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>5768.448000</td>\n",
       "      <td>7378.172000</td>\n",
       "      <td>6774.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>33428.622000</td>\n",
       "      <td>14541.346000</td>\n",
       "      <td>10893.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>12261.005000</td>\n",
       "      <td>14214.947000</td>\n",
       "      <td>9065.958100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>13374.620000</td>\n",
       "      <td>13493.804000</td>\n",
       "      <td>16957.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>16937.240000</td>\n",
       "      <td>14221.101000</td>\n",
       "      <td>17032.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>23209.926000</td>\n",
       "      <td>10239.356000</td>\n",
       "      <td>11375.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>17619.162000</td>\n",
       "      <td>29799.564000</td>\n",
       "      <td>15419.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>26028.659000</td>\n",
       "      <td>31607.523000</td>\n",
       "      <td>28516.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>32855.663000</td>\n",
       "      <td>22708.400000</td>\n",
       "      <td>21884.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>49409.103000</td>\n",
       "      <td>31472.337000</td>\n",
       "      <td>37056.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>21984.910000</td>\n",
       "      <td>29638.012000</td>\n",
       "      <td>31407.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>18001.996094</td>\n",
       "      <td>14966.815430</td>\n",
       "      <td>16432.865234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>13572.521484</td>\n",
       "      <td>12090.882812</td>\n",
       "      <td>11684.101562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>24136.105469</td>\n",
       "      <td>21763.437500</td>\n",
       "      <td>24587.945312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>19440.990234</td>\n",
       "      <td>16538.066406</td>\n",
       "      <td>18931.869141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>22602.402344</td>\n",
       "      <td>19952.802734</td>\n",
       "      <td>22610.496094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>20912.689453</td>\n",
       "      <td>18312.994141</td>\n",
       "      <td>20782.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>21158.576172</td>\n",
       "      <td>18391.859375</td>\n",
       "      <td>20889.351562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>21252.181641</td>\n",
       "      <td>18586.609375</td>\n",
       "      <td>21139.511719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>29127.207031</td>\n",
       "      <td>27376.236328</td>\n",
       "      <td>30516.083984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>24141.535156</td>\n",
       "      <td>21815.886719</td>\n",
       "      <td>24605.310547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>31799.398438</td>\n",
       "      <td>30582.130859</td>\n",
       "      <td>33853.957031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>33698.417969</td>\n",
       "      <td>32830.125000</td>\n",
       "      <td>36187.324219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category      Technology  Office Supplies     Furniture\n",
       "2015-01-31   3143.290000      4845.140000   6217.277000\n",
       "2015-02-28   1608.510000      1071.724000   1839.658000\n",
       "2015-03-31  32359.974000      8602.455000  14243.368000\n",
       "2015-04-30   8973.144000     10988.874000   7944.837000\n",
       "2015-05-31   9599.876000      7131.640000   6912.787000\n",
       "2015-06-30   8435.965000     12742.389000  13144.581600\n",
       "2015-07-31   7839.284000     15121.208000  10821.051000\n",
       "2015-08-31   8937.050000     11055.276000   7125.210500\n",
       "2015-09-30  30383.748000     27423.298000  23816.480800\n",
       "2015-10-31  11938.018000      7211.128000  12304.247000\n",
       "2015-11-30  30073.424000     26363.196000  21471.040700\n",
       "2015-12-31  20573.224000     16956.492000  30637.342500\n",
       "2016-01-31   4518.236000      1808.780000  11739.941600\n",
       "2016-02-29   3448.970000      5368.067000   3134.374000\n",
       "2016-03-31  10143.942000     13472.739000   8722.637400\n",
       "2016-04-30  11160.952000     12517.818000  10475.698500\n",
       "2016-05-31  11643.000000      8941.580000   9374.950500\n",
       "2016-06-30   6435.366000      9623.037000   7540.971000\n",
       "2016-07-31  10234.976000      4706.243000  13667.040000\n",
       "2016-08-31  15444.642000     11735.108000   9638.592200\n",
       "2016-09-30  19017.128000     19031.055000  25085.423000\n",
       "2016-10-31  10704.890000      8642.360000  11664.487500\n",
       "2016-11-30  23873.601000     21178.298000  30197.500500\n",
       "2016-12-31  35632.028000     16099.322000  22812.251200\n",
       "2017-01-31   5620.066000      5299.682000   7622.743000\n",
       "2017-02-28  12258.914000      6794.350000   3925.551000\n",
       "2017-03-31  21567.852000     17324.545000  12272.662000\n",
       "2017-04-30  14890.502000     10577.175000  13212.090000\n",
       "2017-05-31  28832.691000     12931.642000  14892.575000\n",
       "2017-06-30  16372.152000     10901.810000  12450.524000\n",
       "2017-07-31  12847.200000     12903.824000  12569.759000\n",
       "2017-08-31   9672.402000      8959.740000  11910.058300\n",
       "2017-09-30  19424.514000     23013.402000  26755.474900\n",
       "2017-10-31  31533.374000     16236.650000  11813.009000\n",
       "2017-11-30  27141.059000     20141.808000  31783.628800\n",
       "2017-12-31  21801.218000     37332.938000  36604.965000\n",
       "2018-01-31  16493.333000     21052.979000   5930.162000\n",
       "2018-02-28   5768.448000      7378.172000   6774.377400\n",
       "2018-03-31  33428.622000     14541.346000  10893.444800\n",
       "2018-04-30  12261.005000     14214.947000   9065.958100\n",
       "2018-05-31  13374.620000     13493.804000  16957.558200\n",
       "2018-06-30  16937.240000     14221.101000  17032.386700\n",
       "2018-07-31  23209.926000     10239.356000  11375.822000\n",
       "2018-08-31  17619.162000     29799.564000  15419.122000\n",
       "2018-09-30  26028.659000     31607.523000  28516.706000\n",
       "2018-10-31  32855.663000     22708.400000  21884.068200\n",
       "2018-11-30  49409.103000     31472.337000  37056.715000\n",
       "2018-12-31  21984.910000     29638.012000  31407.466800\n",
       "2019-01-31  18001.996094     14966.815430  16432.865234\n",
       "2019-02-28  13572.521484     12090.882812  11684.101562\n",
       "2019-03-31  24136.105469     21763.437500  24587.945312\n",
       "2019-04-30  19440.990234     16538.066406  18931.869141\n",
       "2019-05-31  22602.402344     19952.802734  22610.496094\n",
       "2019-06-30  20912.689453     18312.994141  20782.781250\n",
       "2019-07-31  21158.576172     18391.859375  20889.351562\n",
       "2019-08-31  21252.181641     18586.609375  21139.511719\n",
       "2019-09-30  29127.207031     27376.236328  30516.083984\n",
       "2019-10-31  24141.535156     21815.886719  24605.310547\n",
       "2019-11-30  31799.398438     30582.130859  33853.957031\n",
       "2019-12-31  33698.417969     32830.125000  36187.324219"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtener ingresos totales 5 años, ventas totales 5 años, ventas categoría 5 años, e ingresos por categoría 5 años\n",
    "#ingresos por categoría 5 años\n",
    "IngCatFinal = pd.concat([df_resultadoIng2, predIngCat2019], axis=0)\n",
    "VentCatFinal = pd.concat([df_resultadoVent2[[\"Technology\",\"Office Supplies\",\"Furniture\"]], prediccionVentas2019Cat], axis=0)\n",
    "IngFinal = pd.concat([df_resultadoIng, dfy_predSE2019])\n",
    "VentFinal = pd.concat([df_resultadoVent, X_predRL2019])\n",
    "IngCatFinal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
