{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e373690e-3cba-4da3-9cc2-d507a86952a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pltx\n",
    "#import seaborn as sns \n",
    "#escalas\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#algoritmos \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "from keras.src.models.sequential import Sequential\n",
    "from keras.src.layers import Dense\n",
    "#métricas\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import ExponentialSmoothing #suavizado exponencial triple, que tiene en cuenta la estacionalidad\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade172a8-64a3-468a-9011-3dcfb14d19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_excel(\"/Users/pablomunozdelorenzo/Desktop/ventasEcommerce.xlsx\")\n",
    "df = pd.read_excel(\"dataset/ventasEcommerce.xlsx\") #asumiendo que está en el mismo directorio donde se ejecuta el proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe55322-a6d7-4dfb-bc0c-4e96349be86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Order_Date\"] = pd.to_datetime(df[\"Order_Date\"])\n",
    "df2 = df[[\"Order_Date\",\"Total\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f2b92e8-a1c9-4f99-b2e6-c082561a45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultadoIng = df2.resample(\"ME\",on=\"Order_Date\").sum() #ingresos de entrenamiento\n",
    "df_resultadoVent = df2.resample(\"ME\",on=\"Order_Date\").count() #ventas de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d73fad04-7f8e-4f8d-874d-f0483c713a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.92812353],\n",
       "       [0.92812353, 1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df_resultadoIng[\"Total\"],df_resultadoVent[\"Total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d99d8893-b4a5-462a-a282-0b994ebd3760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>43476.4740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>19920.9974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>58863.4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>35541.9101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>43825.9822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>48190.7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>44825.1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>62837.8480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>86152.8880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>77448.1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>117938.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>83030.3888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total\n",
       "Order_Date             \n",
       "2018-01-31   43476.4740\n",
       "2018-02-28   19920.9974\n",
       "2018-03-31   58863.4128\n",
       "2018-04-30   35541.9101\n",
       "2018-05-31   43825.9822\n",
       "2018-06-30   48190.7277\n",
       "2018-07-31   44825.1040\n",
       "2018-08-31   62837.8480\n",
       "2018-09-30   86152.8880\n",
       "2018-10-31   77448.1312\n",
       "2018-11-30  117938.1550\n",
       "2018-12-31   83030.3888"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df_resultadoVent[0:36] # tres primeros años de ventas\n",
    "X_test = df_resultadoVent[36:48] #último año de ventas\n",
    "Y_train = df_resultadoIng[0:36] # tres primeros años de ingresos\n",
    "Y_test = df_resultadoIng[36:48] #último año de ingresos\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36cc74e-300c-4d90-855d-839c2b7279cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.44882513],\n",
       "       [-0.90086996],\n",
       "       [ 0.64071881],\n",
       "       [ 0.21185577],\n",
       "       [ 0.67549149],\n",
       "       [ 0.61753702],\n",
       "       [ 0.4784463 ],\n",
       "       [ 0.37412827],\n",
       "       [ 3.14435169],\n",
       "       [ 1.30139971],\n",
       "       [ 3.17912437],\n",
       "       [ 3.21389705]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalerIng = StandardScaler()\n",
    "scalerVent = StandardScaler()\n",
    "X_trainEsc = scalerVent.fit_transform(X_train)\n",
    "X_testEsc = scalerVent.transform(X_test)\n",
    "Y_trainEsc = scalerIng.fit_transform(Y_train)\n",
    "Y_testEsc = scalerIng.transform(Y_test)\n",
    "print(len(X_trainEsc))\n",
    "print(len(X_testEsc))\n",
    "X_testEsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "624dabfc-eeab-40d1-9f57-beef5e359af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143.]\n",
      " [104.]\n",
      " [237.]\n",
      " [200.]\n",
      " [240.]\n",
      " [235.]\n",
      " [223.]\n",
      " [214.]\n",
      " [453.]\n",
      " [294.]\n",
      " [456.]\n",
      " [459.]]\n",
      "            Total\n",
      "Order_Date       \n",
      "2018-01-31    143\n",
      "2018-02-28    104\n",
      "2018-03-31    237\n",
      "2018-04-30    200\n",
      "2018-05-31    240\n",
      "2018-06-30    235\n",
      "2018-07-31    223\n",
      "2018-08-31    214\n",
      "2018-09-30    453\n",
      "2018-10-31    294\n",
      "2018-11-30    456\n",
      "2018-12-31    459\n"
     ]
    }
   ],
   "source": [
    "print(scalerVent.inverse_transform(X_testEsc))\n",
    "print(df_resultadoVent[36:48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8837bc88-2ed7-42b1-9b3e-31fcbf188186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.75\n",
      "36\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_trainEsc)/(len(Y_trainEsc)+len(Y_testEsc)))\n",
    "print(len(X_trainEsc)/(len(X_trainEsc)+len(X_testEsc)))\n",
    "print(len(Y_trainEsc))\n",
    "print(len(Y_testEsc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d74c2bc-1391-4553-a944-82aec6b0fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implantamos los modelos\n",
    "#comenzamos por regresión lineal\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_trainEsc,Y_trainEsc)\n",
    "y_predRLEsc = regr.predict(X_testEsc)\n",
    "resultado = np.concatenate((Y_testEsc,y_predRLEsc),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa7a10ce-f965-43a1-b5ec-fb14f63fc7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\AppData\\Local\\Temp\\ipykernel_22288\\2327039246.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['Technology'] = (df3['Category'] == 'Technology').astype(int)\n",
      "C:\\Users\\Pablo\\AppData\\Local\\Temp\\ipykernel_22288\\2327039246.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['Office Supplies'] = (df3['Category'] == 'Office Supplies').astype(int)\n",
      "C:\\Users\\Pablo\\AppData\\Local\\Temp\\ipykernel_22288\\2327039246.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['Furniture'] = (df3['Category'] == 'Furniture').astype(int)\n",
      "C:\\Users\\Pablo\\AppData\\Local\\Temp\\ipykernel_22288\\2327039246.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_resultadoVent2[\"Ventas Totales\"] = df_resultadoVent2[\"Technology\"] + df_resultadoVent2[\"Office Supplies\"] + df_resultadoVent2[\"Furniture\"]\n"
     ]
    }
   ],
   "source": [
    "#continuamos por random forest\n",
    "#preparamos los datos\n",
    "df3 = df[[\"Order_Date\",\"Total\",\"Category\"]]\n",
    "df3['Technology'] = (df3['Category'] == 'Technology').astype(int)\n",
    "df3['Office Supplies'] = (df3['Category'] == 'Office Supplies').astype(int)\n",
    "df3['Furniture'] = (df3['Category'] == 'Furniture').astype(int)\n",
    "df_resultadoIng2 = df3.resample(\"ME\",on=\"Order_Date\").sum() #ingresos de entrenamiento\n",
    "df_resultadoVent2 = df3.resample(\"ME\",on=\"Order_Date\").count() #ventas de entrenamiento\n",
    "df_resultadoVent2 = df_resultadoIng2[[\"Technology\",\"Office Supplies\",\"Furniture\"]]\n",
    "df_resultadoVent2[\"Ventas Totales\"] = df_resultadoVent2[\"Technology\"] + df_resultadoVent2[\"Office Supplies\"] + df_resultadoVent2[\"Furniture\"]\n",
    "df_resultadoIng2 = df_resultadoIng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d12f477-baf4-4da5-986b-f053d458f473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>14205.7070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>4519.8920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>55205.7970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>27906.8550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>23644.3030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>34322.9356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>33781.5430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>27117.5365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>81623.5268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>31453.3930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>77907.6607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>68167.0585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>18066.9576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>11951.4110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>32339.3184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>34154.4685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>29959.5305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>23599.3740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>28608.2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>36818.3422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>63133.6060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>31011.7375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>75249.3995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>74543.6012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>18542.4910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>22978.8150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>51165.0590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>38679.7670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>56656.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>39724.4860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>38320.7830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>30542.2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>69193.3909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>59583.0330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>79066.4958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>95739.1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>43476.4740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>19920.9974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>58863.4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>35541.9101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>43825.9822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>48190.7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>44825.1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>62837.8480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>86152.8880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>77448.1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>117938.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>83030.3888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total\n",
       "Order_Date             \n",
       "2015-01-31   14205.7070\n",
       "2015-02-28    4519.8920\n",
       "2015-03-31   55205.7970\n",
       "2015-04-30   27906.8550\n",
       "2015-05-31   23644.3030\n",
       "2015-06-30   34322.9356\n",
       "2015-07-31   33781.5430\n",
       "2015-08-31   27117.5365\n",
       "2015-09-30   81623.5268\n",
       "2015-10-31   31453.3930\n",
       "2015-11-30   77907.6607\n",
       "2015-12-31   68167.0585\n",
       "2016-01-31   18066.9576\n",
       "2016-02-29   11951.4110\n",
       "2016-03-31   32339.3184\n",
       "2016-04-30   34154.4685\n",
       "2016-05-31   29959.5305\n",
       "2016-06-30   23599.3740\n",
       "2016-07-31   28608.2590\n",
       "2016-08-31   36818.3422\n",
       "2016-09-30   63133.6060\n",
       "2016-10-31   31011.7375\n",
       "2016-11-30   75249.3995\n",
       "2016-12-31   74543.6012\n",
       "2017-01-31   18542.4910\n",
       "2017-02-28   22978.8150\n",
       "2017-03-31   51165.0590\n",
       "2017-04-30   38679.7670\n",
       "2017-05-31   56656.9080\n",
       "2017-06-30   39724.4860\n",
       "2017-07-31   38320.7830\n",
       "2017-08-31   30542.2003\n",
       "2017-09-30   69193.3909\n",
       "2017-10-31   59583.0330\n",
       "2017-11-30   79066.4958\n",
       "2017-12-31   95739.1210\n",
       "2018-01-31   43476.4740\n",
       "2018-02-28   19920.9974\n",
       "2018-03-31   58863.4128\n",
       "2018-04-30   35541.9101\n",
       "2018-05-31   43825.9822\n",
       "2018-06-30   48190.7277\n",
       "2018-07-31   44825.1040\n",
       "2018-08-31   62837.8480\n",
       "2018-09-30   86152.8880\n",
       "2018-10-31   77448.1312\n",
       "2018-11-30  117938.1550\n",
       "2018-12-31   83030.3888"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultadoIng2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78d99c72-3c54-40f4-8afe-682b3a8e6f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerVentRF = StandardScaler()\n",
    "df_resultadoVentEsc2 = scalerVentRF.fit_transform(df_resultadoVent2)\n",
    "X_trainEsc2 = df_resultadoVentEsc2[0:36] # tres primeros años de ventas\n",
    "X_testEsc2 = df_resultadoVentEsc2[36:48] #último año de ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a151785a-e5d9-4592-aecb-5306f2f34fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03136463, -0.42119666, -0.67936891],\n",
       "       [-1.00482077, -0.84541481, -0.87732312],\n",
       "       [ 0.70822299,  0.60127787,  0.12154249],\n",
       "       [-0.31767009,  0.19881449,  0.57193252],\n",
       "       [ 0.0467392 ,  0.63391003,  0.87587436],\n",
       "       [ 0.23874065,  0.57952309,  0.75503124],\n",
       "       [ 0.09068972,  0.44899443,  0.72979094],\n",
       "       [ 0.88305502,  0.35109793,  0.01713189],\n",
       "       [ 1.90866382,  2.9507938 ,  2.00420851],\n",
       "       [ 1.52574896,  1.22128902,  1.43571458],\n",
       "       [ 3.30687071,  2.98342597,  2.00420851],\n",
       "       [ 1.77130773,  3.01605813,  2.00420851]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modelo RF\n",
    "rf_model = RandomForestRegressor(n_estimators=100,random_state=4)\n",
    "rf_model.fit(X_trainEsc2, Y_trainEsc)\n",
    "y_predRF = rf_model.predict(X_testEsc2)\n",
    "resultado = np.column_stack((resultado,y_predRF))\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16f35994-eedb-4903-95db-43200ede5d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43476.474 ],\n",
       "       [ 19920.9974],\n",
       "       [ 58863.4128],\n",
       "       [ 35541.9101],\n",
       "       [ 43825.9822],\n",
       "       [ 48190.7277],\n",
       "       [ 44825.104 ],\n",
       "       [ 62837.848 ],\n",
       "       [ 86152.888 ],\n",
       "       [ 77448.1312],\n",
       "       [117938.155 ],\n",
       "       [ 83030.3888]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalerIng.inverse_transform(Y_testEsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cd9502a-38f8-459e-92a6-545ecd369877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>14205.7070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>4519.8920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>55205.7970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>27906.8550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>23644.3030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>34322.9356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>33781.5430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>27117.5365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>81623.5268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>31453.3930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>77907.6607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>68167.0585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>18066.9576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>11951.4110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>32339.3184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>34154.4685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>29959.5305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>23599.3740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>28608.2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>36818.3422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>63133.6060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>31011.7375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>75249.3995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>74543.6012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>18542.4910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>22978.8150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>51165.0590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>38679.7670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>56656.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>39724.4860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>38320.7830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>30542.2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>69193.3909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>59583.0330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>79066.4958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>95739.1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>43476.4740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>19920.9974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>58863.4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>35541.9101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>43825.9822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>48190.7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>44825.1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>62837.8480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>86152.8880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>77448.1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>117938.1550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>83030.3888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Total\n",
       "Order_Date             \n",
       "2015-01-31   14205.7070\n",
       "2015-02-28    4519.8920\n",
       "2015-03-31   55205.7970\n",
       "2015-04-30   27906.8550\n",
       "2015-05-31   23644.3030\n",
       "2015-06-30   34322.9356\n",
       "2015-07-31   33781.5430\n",
       "2015-08-31   27117.5365\n",
       "2015-09-30   81623.5268\n",
       "2015-10-31   31453.3930\n",
       "2015-11-30   77907.6607\n",
       "2015-12-31   68167.0585\n",
       "2016-01-31   18066.9576\n",
       "2016-02-29   11951.4110\n",
       "2016-03-31   32339.3184\n",
       "2016-04-30   34154.4685\n",
       "2016-05-31   29959.5305\n",
       "2016-06-30   23599.3740\n",
       "2016-07-31   28608.2590\n",
       "2016-08-31   36818.3422\n",
       "2016-09-30   63133.6060\n",
       "2016-10-31   31011.7375\n",
       "2016-11-30   75249.3995\n",
       "2016-12-31   74543.6012\n",
       "2017-01-31   18542.4910\n",
       "2017-02-28   22978.8150\n",
       "2017-03-31   51165.0590\n",
       "2017-04-30   38679.7670\n",
       "2017-05-31   56656.9080\n",
       "2017-06-30   39724.4860\n",
       "2017-07-31   38320.7830\n",
       "2017-08-31   30542.2003\n",
       "2017-09-30   69193.3909\n",
       "2017-10-31   59583.0330\n",
       "2017-11-30   79066.4958\n",
       "2017-12-31   95739.1210\n",
       "2018-01-31   43476.4740\n",
       "2018-02-28   19920.9974\n",
       "2018-03-31   58863.4128\n",
       "2018-04-30   35541.9101\n",
       "2018-05-31   43825.9822\n",
       "2018-06-30   48190.7277\n",
       "2018-07-31   44825.1040\n",
       "2018-08-31   62837.8480\n",
       "2018-09-30   86152.8880\n",
       "2018-10-31   77448.1312\n",
       "2018-11-30  117938.1550\n",
       "2018-12-31   83030.3888"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##comenzamos las predicciones de estacionalidad. \n",
    "df_resultadoIng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "097c90dd-0bbd-441d-b04e-214d0263d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2018-01-31     30407.728742\n",
       "2018-02-28     30537.923623\n",
       "2018-03-31     56151.893477\n",
       "2018-04-30     51623.912188\n",
       "2018-05-31     59040.531183\n",
       "2018-06-30     47934.880339\n",
       "2018-07-31     52952.619865\n",
       "2018-08-31     54386.569946\n",
       "2018-09-30     95633.395068\n",
       "2018-10-31     55487.017431\n",
       "2018-11-30    101054.618238\n",
       "2018-12-31     96168.562128\n",
       "Freq: ME, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#suavizado exponencial\n",
    "modeloSE = ExponentialSmoothing(df_resultadoIng[0:36],seasonal_periods=12, trend='add', seasonal='add')\n",
    "ajusteSE = modeloSE.fit()\n",
    "y_predSE = ajusteSE.forecast(steps = len(df_resultadoIng[36:48]))\n",
    "y_predSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12ee3420-e81c-4e73-8115-553def3717ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7562577255539945\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(df_resultadoIng[36:48],y_predSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2a8b815-4479-4464-933a-dc77bd52bafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=514.796, Time=0.05 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=508.768, Time=0.21 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=507.790, Time=0.05 sec\n",
      " ARIMA(0,1,1)(0,1,0)[12]             : AIC=515.262, Time=0.03 sec\n",
      " ARIMA(0,1,1)(1,1,1)[12]             : AIC=inf, Time=0.12 sec\n",
      " ARIMA(0,1,1)(0,1,2)[12]             : AIC=508.871, Time=0.18 sec\n",
      " ARIMA(0,1,1)(1,1,0)[12]             : AIC=509.375, Time=0.04 sec\n",
      " ARIMA(0,1,1)(1,1,2)[12]             : AIC=inf, Time=0.55 sec\n",
      " ARIMA(0,1,0)(0,1,1)[12]             : AIC=509.370, Time=0.02 sec\n",
      " ARIMA(1,1,1)(0,1,1)[12]             : AIC=inf, Time=0.16 sec\n",
      " ARIMA(0,1,2)(0,1,1)[12]             : AIC=510.395, Time=0.05 sec\n",
      " ARIMA(1,1,0)(0,1,1)[12]             : AIC=507.867, Time=0.07 sec\n",
      " ARIMA(1,1,2)(0,1,1)[12]             : AIC=inf, Time=0.39 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12] intercept   : AIC=513.716, Time=0.03 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,1)(0,1,1)[12]          \n",
      "Total fit time: 1.967 seconds\n",
      " ARIMA(0,1,1)(0,1,1)[12]          \n"
     ]
    }
   ],
   "source": [
    "#Sarimax\n",
    "modeloSarimax = auto_arima(df_resultadoIng[0:36],start_p=0,d=1,start_q=0,\n",
    "                         max_p=4,max_d=2,max_q=4,start_P=0,\n",
    "                         D=1,start_Q=0,max_P=2,max_D=1, \n",
    "                        max_Q= 2,m=12, seasonal= True, error_action=\"warn\",\n",
    "                        trace= True, suppress_warnings=True, stepwise= True,\n",
    "                        random_state= 20, n_fits=50)\n",
    "print(modeloSarimax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bb51b03-4638-469b-879a-d1bf98e8e81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n",
      "  warn('Too few observations to estimate starting parameters%s.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 6.772758\n",
      "         Iterations: 50\n",
      "         Function evaluations: 51\n",
      "         Gradient evaluations: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1292: OptimizeWarning: Maximum number of iterations has been exceeded.\n",
      "  res = _minimize_bfgs(f, x0, args, fprime, callback=callback, **opts)\n",
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['typ']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2018-01-31     32720.416811\n",
       "2018-02-28     24066.923168\n",
       "2018-03-31     65745.437217\n",
       "2018-04-30     44937.389410\n",
       "2018-05-31     57004.678493\n",
       "2018-06-30     50369.864354\n",
       "2018-07-31     47127.701005\n",
       "2018-08-31     43251.520045\n",
       "2018-09-30    101577.928529\n",
       "2018-10-31     74194.716569\n",
       "2018-11-30    104467.510158\n",
       "2018-12-31    117726.508206\n",
       "Freq: ME, Name: predicted_mean, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arima_model = SARIMAX(df_resultadoIng[0:36],exog=df_resultadoVent[0:36],order=(0,1,1),seasonal_order=(0,1,1,12)).fit(method=\"bfgs\")\n",
    "y_predSarimax = arima_model.predict(start = \"2018-01-01\", end = \"2018-12-30\",exog = df_resultadoVent[36:48] ,typ=\"levels\")\n",
    "y_predSarimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df006ece-f915-4d22-b61e-3d7826950934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6910413655153816\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(df_resultadoIng[36:48],y_predSarimax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "741e009a-b25c-4b2f-a284-b7dad0d3d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03136463, -0.42119666, -0.67936891, -0.44178518, -0.54351837],\n",
       "       [-1.00482077, -0.84541481, -0.87732312, -0.82244502, -0.53779121],\n",
       "       [ 0.70822299,  0.60127787,  0.12154249,  1.0109574 ,  0.58894556],\n",
       "       [-0.31767009,  0.19881449,  0.57193252,  0.09562906,  0.38976351],\n",
       "       [ 0.0467392 ,  0.63391003,  0.87587436,  0.62645885,  0.71601428],\n",
       "       [ 0.23874065,  0.57952309,  0.75503124,  0.33459902,  0.22748614],\n",
       "       [ 0.09068972,  0.44899443,  0.72979094,  0.191979  ,  0.44821224],\n",
       "       [ 0.88305502,  0.35109793,  0.01713189,  0.0214691 ,  0.51129048],\n",
       "       [ 1.90866382,  2.9507938 ,  2.00420851,  2.58719825,  2.32570327],\n",
       "       [ 1.52574896,  1.22128902,  1.43571458,  1.38263402,  0.55969823],\n",
       "       [ 3.30687071,  2.98342597,  2.00420851,  2.71430849,  2.56417827],\n",
       "       [ 1.77130773,  3.01605813,  2.00420851,  3.29756054,  2.34924481]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#escalamos ambos resultados y los añadimos al array\n",
    "y_predSarimaxEsc = scalerIng.transform(pd.DataFrame(y_predSarimax).values)\n",
    "y_predSEEsc = scalerIng.transform(pd.DataFrame(y_predSE).values)\n",
    "resultado = np.column_stack((resultado,y_predSarimaxEsc,y_predSEEsc))\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0096d2d-a57c-4ed2-8bf4-2ba27892f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> (772.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m193\u001b[0m (772.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> (772.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m193\u001b[0m (772.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#redes neuronales\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))  # Capa oculta con 64 neuronas y función de activación ReLU\n",
    "model.add(Dense(1))  # Capa de salida con una neurona (predicción de ingresos)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])  # Compilar el modelo con el optimizador Adam y la pérdida MSE\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcd8e9f1-09c2-4b75-9c21-8b646889d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.0000e+00 - loss: 0.8111 - val_accuracy: 0.0000e+00 - val_loss: 1.1182\n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.7846 - val_accuracy: 0.0000e+00 - val_loss: 1.0456\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.7374 - val_accuracy: 0.0000e+00 - val_loss: 0.9779\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.6888 - val_accuracy: 0.0000e+00 - val_loss: 0.9140\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 0.6619 - val_accuracy: 0.0000e+00 - val_loss: 0.8548\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.6116 - val_accuracy: 0.0000e+00 - val_loss: 0.8015\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.5869 - val_accuracy: 0.0000e+00 - val_loss: 0.7545\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.5260 - val_accuracy: 0.0000e+00 - val_loss: 0.7095\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.5194 - val_accuracy: 0.0000e+00 - val_loss: 0.6659\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.4997 - val_accuracy: 0.0000e+00 - val_loss: 0.6274\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.4706 - val_accuracy: 0.0000e+00 - val_loss: 0.5919\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.4305 - val_accuracy: 0.0000e+00 - val_loss: 0.5586\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: 0.4260 - val_accuracy: 0.0000e+00 - val_loss: 0.5286\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.3945 - val_accuracy: 0.0000e+00 - val_loss: 0.5002\n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.3785 - val_accuracy: 0.0000e+00 - val_loss: 0.4730\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.3548 - val_accuracy: 0.0000e+00 - val_loss: 0.4486\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.3384 - val_accuracy: 0.0000e+00 - val_loss: 0.4259\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.3094 - val_accuracy: 0.0000e+00 - val_loss: 0.4031\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.2826 - val_accuracy: 0.0000e+00 - val_loss: 0.3802\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.2720 - val_accuracy: 0.0000e+00 - val_loss: 0.3583\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.2742 - val_accuracy: 0.0000e+00 - val_loss: 0.3394\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.2407 - val_accuracy: 0.0000e+00 - val_loss: 0.3234\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.2413 - val_accuracy: 0.0000e+00 - val_loss: 0.3096\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0000e+00 - loss: 0.2284 - val_accuracy: 0.0000e+00 - val_loss: 0.2990\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.2106 - val_accuracy: 0.0000e+00 - val_loss: 0.2905\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.2114 - val_accuracy: 0.0000e+00 - val_loss: 0.2839\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.1874 - val_accuracy: 0.0000e+00 - val_loss: 0.2788\n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 0.1691 - val_accuracy: 0.0000e+00 - val_loss: 0.2751\n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.1697 - val_accuracy: 0.0000e+00 - val_loss: 0.2734\n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: 0.1734 - val_accuracy: 0.0000e+00 - val_loss: 0.2736\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: 0.1684 - val_accuracy: 0.0000e+00 - val_loss: 0.2751\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.1508 - val_accuracy: 0.0000e+00 - val_loss: 0.2775\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.1494 - val_accuracy: 0.0000e+00 - val_loss: 0.2803\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.1465 - val_accuracy: 0.0000e+00 - val_loss: 0.2842\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.0000e+00 - loss: 0.1491 - val_accuracy: 0.0000e+00 - val_loss: 0.2889\n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.1465 - val_accuracy: 0.0000e+00 - val_loss: 0.2934\n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.1374 - val_accuracy: 0.0000e+00 - val_loss: 0.2974\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.1409 - val_accuracy: 0.0000e+00 - val_loss: 0.3013\n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.1339 - val_accuracy: 0.0000e+00 - val_loss: 0.3051\n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.1232 - val_accuracy: 0.0000e+00 - val_loss: 0.3088\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.0000e+00 - loss: 0.1341 - val_accuracy: 0.0000e+00 - val_loss: 0.3121\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.1322 - val_accuracy: 0.0000e+00 - val_loss: 0.3150\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.1287 - val_accuracy: 0.0000e+00 - val_loss: 0.3180\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.0000e+00 - loss: 0.1322 - val_accuracy: 0.0000e+00 - val_loss: 0.3205\n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.1296 - val_accuracy: 0.0000e+00 - val_loss: 0.3219\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 0.1324 - val_accuracy: 0.0000e+00 - val_loss: 0.3227\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.1300 - val_accuracy: 0.0000e+00 - val_loss: 0.3229\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 0.1283 - val_accuracy: 0.0000e+00 - val_loss: 0.3226\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.1258 - val_accuracy: 0.0000e+00 - val_loss: 0.3235\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.1217 - val_accuracy: 0.0000e+00 - val_loss: 0.3247\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.1277 - val_accuracy: 0.0000e+00 - val_loss: 0.3244\n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0000e+00 - loss: 0.1213 - val_accuracy: 0.0000e+00 - val_loss: 0.3239\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: 0.1285 - val_accuracy: 0.0000e+00 - val_loss: 0.3229\n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.1286 - val_accuracy: 0.0000e+00 - val_loss: 0.3219\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.1286 - val_accuracy: 0.0000e+00 - val_loss: 0.3210\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.1263 - val_accuracy: 0.0000e+00 - val_loss: 0.3204\n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0000e+00 - loss: 0.1267 - val_accuracy: 0.0000e+00 - val_loss: 0.3199\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.0000e+00 - loss: 0.1277 - val_accuracy: 0.0000e+00 - val_loss: 0.3193\n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0000e+00 - loss: 0.1264 - val_accuracy: 0.0000e+00 - val_loss: 0.3187\n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.1281 - val_accuracy: 0.0000e+00 - val_loss: 0.3184\n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.0000e+00 - loss: 0.1236 - val_accuracy: 0.0000e+00 - val_loss: 0.3186\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.1268 - val_accuracy: 0.0000e+00 - val_loss: 0.3181\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.1248 - val_accuracy: 0.0000e+00 - val_loss: 0.3159\n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.1238 - val_accuracy: 0.0000e+00 - val_loss: 0.3128\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0000e+00 - loss: 0.1248 - val_accuracy: 0.0000e+00 - val_loss: 0.3103\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 0.1260 - val_accuracy: 0.0000e+00 - val_loss: 0.3085\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.1243 - val_accuracy: 0.0000e+00 - val_loss: 0.3072\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.1274 - val_accuracy: 0.0000e+00 - val_loss: 0.3062\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.1230 - val_accuracy: 0.0000e+00 - val_loss: 0.3059\n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0000e+00 - loss: 0.1263 - val_accuracy: 0.0000e+00 - val_loss: 0.3068\n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.1224 - val_accuracy: 0.0000e+00 - val_loss: 0.3087\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.1268 - val_accuracy: 0.0000e+00 - val_loss: 0.3112\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.1225 - val_accuracy: 0.0000e+00 - val_loss: 0.3136\n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 0.1275 - val_accuracy: 0.0000e+00 - val_loss: 0.3157\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.1195 - val_accuracy: 0.0000e+00 - val_loss: 0.3179\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.1218 - val_accuracy: 0.0000e+00 - val_loss: 0.3216\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.1214 - val_accuracy: 0.0000e+00 - val_loss: 0.3240\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0000e+00 - loss: 0.1251 - val_accuracy: 0.0000e+00 - val_loss: 0.3233\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.1210 - val_accuracy: 0.0000e+00 - val_loss: 0.3211\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.1264 - val_accuracy: 0.0000e+00 - val_loss: 0.3177\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.1161 - val_accuracy: 0.0000e+00 - val_loss: 0.3149\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.1220 - val_accuracy: 0.0000e+00 - val_loss: 0.3114\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0000e+00 - loss: 0.1202 - val_accuracy: 0.0000e+00 - val_loss: 0.3068\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.1259 - val_accuracy: 0.0000e+00 - val_loss: 0.3025\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 0.1219 - val_accuracy: 0.0000e+00 - val_loss: 0.2992\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: 0.1201 - val_accuracy: 0.0000e+00 - val_loss: 0.2977\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.1245 - val_accuracy: 0.0000e+00 - val_loss: 0.2975\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.0000e+00 - loss: 0.1116 - val_accuracy: 0.0000e+00 - val_loss: 0.2966\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.1214 - val_accuracy: 0.0000e+00 - val_loss: 0.2959\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.1240 - val_accuracy: 0.0000e+00 - val_loss: 0.2956\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0000e+00 - loss: 0.1265 - val_accuracy: 0.0000e+00 - val_loss: 0.2947\n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.1201 - val_accuracy: 0.0000e+00 - val_loss: 0.2944\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.0000e+00 - loss: 0.1265 - val_accuracy: 0.0000e+00 - val_loss: 0.2951\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.1243 - val_accuracy: 0.0000e+00 - val_loss: 0.2954\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.1264 - val_accuracy: 0.0000e+00 - val_loss: 0.2951\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 0.1205 - val_accuracy: 0.0000e+00 - val_loss: 0.2959\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.1230 - val_accuracy: 0.0000e+00 - val_loss: 0.2977\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.0000e+00 - loss: 0.1200 - val_accuracy: 0.0000e+00 - val_loss: 0.3008\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.1238 - val_accuracy: 0.0000e+00 - val_loss: 0.3045\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.1193 - val_accuracy: 0.0000e+00 - val_loss: 0.3083\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_trainEsc, Y_trainEsc, epochs=100, batch_size=32, validation_data=(X_testEsc, Y_testEsc))\n",
    "predicciones_2018 = model.predict(X_testEsc)\n",
    "resultado = np.column_stack((resultado, predicciones_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "756b4de7-10ed-41b9-9da5-57daa621349f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Valor Real</th>\n",
       "      <th>Regresión Lineal</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Sarimax</th>\n",
       "      <th>Suavización Exponencial</th>\n",
       "      <th>Red Neuronal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>0.031365</td>\n",
       "      <td>-0.421197</td>\n",
       "      <td>-0.679369</td>\n",
       "      <td>-0.441785</td>\n",
       "      <td>-0.543518</td>\n",
       "      <td>-0.525487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>-1.004821</td>\n",
       "      <td>-0.845415</td>\n",
       "      <td>-0.877323</td>\n",
       "      <td>-0.822445</td>\n",
       "      <td>-0.537791</td>\n",
       "      <td>-0.860153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>0.708223</td>\n",
       "      <td>0.601278</td>\n",
       "      <td>0.121542</td>\n",
       "      <td>1.010957</td>\n",
       "      <td>0.588946</td>\n",
       "      <td>0.739039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>-0.317670</td>\n",
       "      <td>0.198814</td>\n",
       "      <td>0.571933</td>\n",
       "      <td>0.095629</td>\n",
       "      <td>0.389764</td>\n",
       "      <td>0.352997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>0.046739</td>\n",
       "      <td>0.633910</td>\n",
       "      <td>0.875874</td>\n",
       "      <td>0.626459</td>\n",
       "      <td>0.716014</td>\n",
       "      <td>0.765243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>0.238741</td>\n",
       "      <td>0.579523</td>\n",
       "      <td>0.755031</td>\n",
       "      <td>0.334599</td>\n",
       "      <td>0.227486</td>\n",
       "      <td>0.721569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>0.090690</td>\n",
       "      <td>0.448994</td>\n",
       "      <td>0.729791</td>\n",
       "      <td>0.191979</td>\n",
       "      <td>0.448212</td>\n",
       "      <td>0.610730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.351098</td>\n",
       "      <td>0.017132</td>\n",
       "      <td>0.021469</td>\n",
       "      <td>0.511290</td>\n",
       "      <td>0.524892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>1.908664</td>\n",
       "      <td>2.950794</td>\n",
       "      <td>2.004209</td>\n",
       "      <td>2.587198</td>\n",
       "      <td>2.325703</td>\n",
       "      <td>2.580127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>1.525749</td>\n",
       "      <td>1.221289</td>\n",
       "      <td>1.435715</td>\n",
       "      <td>1.382634</td>\n",
       "      <td>0.559698</td>\n",
       "      <td>1.228405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>3.306871</td>\n",
       "      <td>2.983426</td>\n",
       "      <td>2.004209</td>\n",
       "      <td>2.714308</td>\n",
       "      <td>2.564178</td>\n",
       "      <td>2.605631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>1.771308</td>\n",
       "      <td>3.016058</td>\n",
       "      <td>2.004209</td>\n",
       "      <td>3.297561</td>\n",
       "      <td>2.349245</td>\n",
       "      <td>2.631135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Valor Real  Regresión Lineal  Random Forest   Sarimax  \\\n",
       "2018-01-31    0.031365         -0.421197      -0.679369 -0.441785   \n",
       "2018-02-28   -1.004821         -0.845415      -0.877323 -0.822445   \n",
       "2018-03-31    0.708223          0.601278       0.121542  1.010957   \n",
       "2018-04-30   -0.317670          0.198814       0.571933  0.095629   \n",
       "2018-05-31    0.046739          0.633910       0.875874  0.626459   \n",
       "2018-06-30    0.238741          0.579523       0.755031  0.334599   \n",
       "2018-07-31    0.090690          0.448994       0.729791  0.191979   \n",
       "2018-08-31    0.883055          0.351098       0.017132  0.021469   \n",
       "2018-09-30    1.908664          2.950794       2.004209  2.587198   \n",
       "2018-10-31    1.525749          1.221289       1.435715  1.382634   \n",
       "2018-11-30    3.306871          2.983426       2.004209  2.714308   \n",
       "2018-12-31    1.771308          3.016058       2.004209  3.297561   \n",
       "\n",
       "            Suavización Exponencial  Red Neuronal  \n",
       "2018-01-31                -0.543518     -0.525487  \n",
       "2018-02-28                -0.537791     -0.860153  \n",
       "2018-03-31                 0.588946      0.739039  \n",
       "2018-04-30                 0.389764      0.352997  \n",
       "2018-05-31                 0.716014      0.765243  \n",
       "2018-06-30                 0.227486      0.721569  \n",
       "2018-07-31                 0.448212      0.610730  \n",
       "2018-08-31                 0.511290      0.524892  \n",
       "2018-09-30                 2.325703      2.580127  \n",
       "2018-10-31                 0.559698      1.228405  \n",
       "2018-11-30                 2.564178      2.605631  \n",
       "2018-12-31                 2.349245      2.631135  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicciones = pd.DataFrame(resultado,index=df_resultadoVent[36:48].index.values,columns = ['Valor Real', 'Regresión Lineal', 'Random Forest', 'Sarimax', 'Suavización Exponencial',\"Red Neuronal\"])\n",
    "df_predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2dfe6909-6a7a-4d2b-a840-0e89d473a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2RL = r2_score(df_predicciones['Valor Real'], df_predicciones[\"Regresión Lineal\"])\n",
    "r2RF = r2_score(df_predicciones['Valor Real'], df_predicciones[\"Random Forest\"])\n",
    "r2Sarimax = r2_score(df_predicciones['Valor Real'], df_predicciones[\"Sarimax\"])\n",
    "r2SE = r2_score(df_predicciones['Valor Real'], df_predicciones[\"Suavización Exponencial\"])\n",
    "r2RN = r2_score(df_predicciones['Valor Real'], df_predicciones[\"Red Neuronal\"])\n",
    "mseRL = mean_squared_error(df_predicciones['Valor Real'], df_predicciones[\"Regresión Lineal\"])\n",
    "mseRF = mean_squared_error(df_predicciones['Valor Real'], df_predicciones[\"Random Forest\"])\n",
    "mseSarimax = mean_squared_error(df_predicciones['Valor Real'], df_predicciones[\"Sarimax\"])\n",
    "mseSE = mean_squared_error(df_predicciones['Valor Real'], df_predicciones[\"Suavización Exponencial\"])\n",
    "mseRN = mean_squared_error(df_predicciones['Valor Real'], df_predicciones[\"Red Neuronal\"])\n",
    "maeRL = mean_absolute_error(df_predicciones['Valor Real'], df_predicciones[\"Regresión Lineal\"])\n",
    "maeRF = mean_absolute_error(df_predicciones['Valor Real'], df_predicciones[\"Random Forest\"])\n",
    "maeSarimax = mean_absolute_error(df_predicciones['Valor Real'], df_predicciones[\"Sarimax\"])\n",
    "maeSE = mean_absolute_error(df_predicciones['Valor Real'], df_predicciones[\"Suavización Exponencial\"])\n",
    "maeRN = mean_absolute_error(df_predicciones['Valor Real'], df_predicciones[\"Red Neuronal\"])\n",
    "mapeRL = mean_absolute_percentage_error(df_predicciones['Valor Real'], df_predicciones[\"Regresión Lineal\"])\n",
    "mapeRF = mean_absolute_percentage_error(df_predicciones['Valor Real'], df_predicciones[\"Random Forest\"])\n",
    "mapeSarimax = mean_absolute_percentage_error(df_predicciones['Valor Real'], df_predicciones[\"Sarimax\"])\n",
    "mapeSE = mean_absolute_percentage_error(df_predicciones['Valor Real'], df_predicciones[\"Suavización Exponencial\"])\n",
    "mapeRN = mean_absolute_percentage_error(df_predicciones['Valor Real'], df_predicciones[\"Red Neuronal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e08cc158-69c3-40eb-94a5-6a26edeb63b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regresión Lineal</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Sarimax</th>\n",
       "      <th>Suavización Exponencial</th>\n",
       "      <th>Redes Neuronales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Coeficiente de determinación</th>\n",
       "      <td>0.727600</td>\n",
       "      <td>0.641987</td>\n",
       "      <td>0.691041</td>\n",
       "      <td>0.756258</td>\n",
       "      <td>0.760783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error cuadrático medio</th>\n",
       "      <td>0.351119</td>\n",
       "      <td>0.461474</td>\n",
       "      <td>0.398243</td>\n",
       "      <td>0.314180</td>\n",
       "      <td>0.308347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error absoluto medio</th>\n",
       "      <td>0.497366</td>\n",
       "      <td>0.573842</td>\n",
       "      <td>0.495873</td>\n",
       "      <td>0.498513</td>\n",
       "      <td>0.501034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Porcentaje de error absoluto medio</th>\n",
       "      <td>3.037836</td>\n",
       "      <td>4.581699</td>\n",
       "      <td>2.781913</td>\n",
       "      <td>3.443455</td>\n",
       "      <td>3.735990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Regresión Lineal  Random Forest   Sarimax  \\\n",
       "Coeficiente de determinación                0.727600       0.641987  0.691041   \n",
       "Error cuadrático medio                      0.351119       0.461474  0.398243   \n",
       "Error absoluto medio                        0.497366       0.573842  0.495873   \n",
       "Porcentaje de error absoluto medio          3.037836       4.581699  2.781913   \n",
       "\n",
       "                                    Suavización Exponencial  Redes Neuronales  \n",
       "Coeficiente de determinación                       0.756258          0.760783  \n",
       "Error cuadrático medio                             0.314180          0.308347  \n",
       "Error absoluto medio                               0.498513          0.501034  \n",
       "Porcentaje de error absoluto medio                 3.443455          3.735990  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calcular todos los errores (mae, mse, mape) y hacer una tablita\n",
    "index = [\"Coeficiente de determinación\", \"Error cuadrático medio\",\"Error absoluto medio\",\"Porcentaje de error absoluto medio\"]\n",
    "arrayMetrica = np.array([[r2RL, r2RF, r2Sarimax, r2SE, r2RN],\n",
    "                  [mseRL, mseRF, mseSarimax, mseSE,mseRN],\n",
    "                        [maeRL,maeRF,maeSarimax,maeSE,maeRN],\n",
    "                        [mapeRL,mapeRF,mapeSarimax,mapeSE,mapeRN]]) #hecho con los valores nuevos de RF\n",
    "df_metricas = pd.DataFrame(arrayMetrica,index=index,columns=['Regresión Lineal', 'Random Forest', 'Sarimax', 'Suavización Exponencial', \"Redes Neuronales\"])\n",
    "df_metricas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11e008da-bb1c-4227-9d00-af6c597f1891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\statsmodels\\tsa\\holtwinters\\model.py:918: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2019-01-31     49682.408073\n",
       "2019-02-28     40135.135966\n",
       "2019-03-31     73568.403794\n",
       "2019-04-30     55112.106270\n",
       "2019-05-31     67276.278734\n",
       "2019-06-30     60818.224836\n",
       "2019-07-31     61453.022672\n",
       "2019-08-31     61866.994373\n",
       "2019-09-30     94522.396066\n",
       "2019-10-31     73967.755768\n",
       "2019-11-30    106402.971754\n",
       "2019-12-31    114474.811994\n",
       "Freq: ME, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#una vez tenemos predichos los ingresos de 2018, pasamos a predecir los de 2019. Para ello, entrenaremos el modelo con\n",
    "#los datos entre 2016 a 2018, y en base a ellos predeciremos los de 2019 usando suavizado exponencial \n",
    "modeloSE2019 = ExponentialSmoothing(df_resultadoIng[12:48],seasonal_periods=12, trend='add', seasonal='add')\n",
    "ajusteSE2019 = modeloSE2019.fit()\n",
    "y_predSE2019 = ajusteSE2019.forecast(steps = len(df_resultadoIng[36:48]))\n",
    "y_predSE2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46db0690-ecea-4806-a4ee-33b71dac4094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>year</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14205.7070</td>\n",
       "      <td>18066.9576</td>\n",
       "      <td>18542.4910</td>\n",
       "      <td>43476.4740</td>\n",
       "      <td>49682.408073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4519.8920</td>\n",
       "      <td>11951.4110</td>\n",
       "      <td>22978.8150</td>\n",
       "      <td>19920.9974</td>\n",
       "      <td>40135.135966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55205.7970</td>\n",
       "      <td>32339.3184</td>\n",
       "      <td>51165.0590</td>\n",
       "      <td>58863.4128</td>\n",
       "      <td>73568.403794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27906.8550</td>\n",
       "      <td>34154.4685</td>\n",
       "      <td>38679.7670</td>\n",
       "      <td>35541.9101</td>\n",
       "      <td>55112.106270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23644.3030</td>\n",
       "      <td>29959.5305</td>\n",
       "      <td>56656.9080</td>\n",
       "      <td>43825.9822</td>\n",
       "      <td>67276.278734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34322.9356</td>\n",
       "      <td>23599.3740</td>\n",
       "      <td>39724.4860</td>\n",
       "      <td>48190.7277</td>\n",
       "      <td>60818.224836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33781.5430</td>\n",
       "      <td>28608.2590</td>\n",
       "      <td>38320.7830</td>\n",
       "      <td>44825.1040</td>\n",
       "      <td>61453.022672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27117.5365</td>\n",
       "      <td>36818.3422</td>\n",
       "      <td>30542.2003</td>\n",
       "      <td>62837.8480</td>\n",
       "      <td>61866.994373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>81623.5268</td>\n",
       "      <td>63133.6060</td>\n",
       "      <td>69193.3909</td>\n",
       "      <td>86152.8880</td>\n",
       "      <td>94522.396066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31453.3930</td>\n",
       "      <td>31011.7375</td>\n",
       "      <td>59583.0330</td>\n",
       "      <td>77448.1312</td>\n",
       "      <td>73967.755768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>77907.6607</td>\n",
       "      <td>75249.3995</td>\n",
       "      <td>79066.4958</td>\n",
       "      <td>117938.1550</td>\n",
       "      <td>106402.971754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>68167.0585</td>\n",
       "      <td>74543.6012</td>\n",
       "      <td>95739.1210</td>\n",
       "      <td>83030.3888</td>\n",
       "      <td>114474.811994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "year         2015        2016        2017         2018           2019\n",
       "month                                                                \n",
       "1      14205.7070  18066.9576  18542.4910   43476.4740   49682.408073\n",
       "2       4519.8920  11951.4110  22978.8150   19920.9974   40135.135966\n",
       "3      55205.7970  32339.3184  51165.0590   58863.4128   73568.403794\n",
       "4      27906.8550  34154.4685  38679.7670   35541.9101   55112.106270\n",
       "5      23644.3030  29959.5305  56656.9080   43825.9822   67276.278734\n",
       "6      34322.9356  23599.3740  39724.4860   48190.7277   60818.224836\n",
       "7      33781.5430  28608.2590  38320.7830   44825.1040   61453.022672\n",
       "8      27117.5365  36818.3422  30542.2003   62837.8480   61866.994373\n",
       "9      81623.5268  63133.6060  69193.3909   86152.8880   94522.396066\n",
       "10     31453.3930  31011.7375  59583.0330   77448.1312   73967.755768\n",
       "11     77907.6607  75249.3995  79066.4958  117938.1550  106402.971754\n",
       "12     68167.0585  74543.6012  95739.1210   83030.3888  114474.811994"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfy_predSE2019 = pd.DataFrame(y_predSE2019,columns = [\"Total\"])\n",
    "df_resultadoIngFinal = pd.concat([df_resultadoIng, dfy_predSE2019])\n",
    "df_resultadoIngFinal[\"month\"] = df_resultadoIngFinal.index.month\n",
    "df_resultadoIngFinal[\"year\"] = df_resultadoIngFinal.index.year\n",
    "df_resultadoIngFinal = df_resultadoIngFinal.pivot(index=\"month\",columns=\"year\",values=\"Total\")\n",
    "df_resultadoIngFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "220c2439-37cb-4155-8fc2-d8caca47266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step - accuracy: 0.0000e+00 - loss: 0.9958 - val_accuracy: 0.0000e+00 - val_loss: 2.1762\n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.9235 - val_accuracy: 0.0000e+00 - val_loss: 2.1112\n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0000e+00 - loss: 0.8682 - val_accuracy: 0.0000e+00 - val_loss: 2.0518\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.8746 - val_accuracy: 0.0000e+00 - val_loss: 1.9963\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.8291 - val_accuracy: 0.0000e+00 - val_loss: 1.9453\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 0.7671 - val_accuracy: 0.0000e+00 - val_loss: 1.8930\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.7692 - val_accuracy: 0.0000e+00 - val_loss: 1.8416\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.6874 - val_accuracy: 0.0000e+00 - val_loss: 1.7920\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.7022 - val_accuracy: 0.0000e+00 - val_loss: 1.7430\n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.6427 - val_accuracy: 0.0000e+00 - val_loss: 1.6984\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.6357 - val_accuracy: 0.0000e+00 - val_loss: 1.6574\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.5934 - val_accuracy: 0.0000e+00 - val_loss: 1.6183\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.5914 - val_accuracy: 0.0000e+00 - val_loss: 1.5809\n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.5504 - val_accuracy: 0.0000e+00 - val_loss: 1.5451\n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.5239 - val_accuracy: 0.0000e+00 - val_loss: 1.5105\n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.4932 - val_accuracy: 0.0000e+00 - val_loss: 1.4765\n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.4950 - val_accuracy: 0.0000e+00 - val_loss: 1.4420\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.0000e+00 - loss: 0.4555 - val_accuracy: 0.0000e+00 - val_loss: 1.4057\n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.4329 - val_accuracy: 0.0000e+00 - val_loss: 1.3671\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.4324 - val_accuracy: 0.0000e+00 - val_loss: 1.3297\n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.4052 - val_accuracy: 0.0000e+00 - val_loss: 1.2952\n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.3954 - val_accuracy: 0.0000e+00 - val_loss: 1.2636\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.3776 - val_accuracy: 0.0000e+00 - val_loss: 1.2346\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.3611 - val_accuracy: 0.0000e+00 - val_loss: 1.2082\n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.3350 - val_accuracy: 0.0000e+00 - val_loss: 1.1828\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.3170 - val_accuracy: 0.0000e+00 - val_loss: 1.1572\n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.3158 - val_accuracy: 0.0000e+00 - val_loss: 1.1338\n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.2919 - val_accuracy: 0.0000e+00 - val_loss: 1.1106\n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 0.2973 - val_accuracy: 0.0000e+00 - val_loss: 1.0877\n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.2827 - val_accuracy: 0.0000e+00 - val_loss: 1.0666\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.2773 - val_accuracy: 0.0000e+00 - val_loss: 1.0463\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.2632 - val_accuracy: 0.0000e+00 - val_loss: 1.0275\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 0.2636 - val_accuracy: 0.0000e+00 - val_loss: 1.0093\n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.2571 - val_accuracy: 0.0000e+00 - val_loss: 0.9918\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0000e+00 - loss: 0.2467 - val_accuracy: 0.0000e+00 - val_loss: 0.9736\n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0000e+00 - loss: 0.2301 - val_accuracy: 0.0000e+00 - val_loss: 0.9524\n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.2359 - val_accuracy: 0.0000e+00 - val_loss: 0.9307\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.2159 - val_accuracy: 0.0000e+00 - val_loss: 0.9090\n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.2200 - val_accuracy: 0.0000e+00 - val_loss: 0.8850\n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.2085 - val_accuracy: 0.0000e+00 - val_loss: 0.8611\n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.2039 - val_accuracy: 0.0000e+00 - val_loss: 0.8372\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.2025 - val_accuracy: 0.0000e+00 - val_loss: 0.8136\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.1980 - val_accuracy: 0.0000e+00 - val_loss: 0.7910\n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.0000e+00 - loss: 0.1899 - val_accuracy: 0.0000e+00 - val_loss: 0.7699\n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.1874 - val_accuracy: 0.0000e+00 - val_loss: 0.7507\n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.0000e+00 - loss: 0.1800 - val_accuracy: 0.0000e+00 - val_loss: 0.7342\n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.0000e+00 - loss: 0.1799 - val_accuracy: 0.0000e+00 - val_loss: 0.7201\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.1656 - val_accuracy: 0.0000e+00 - val_loss: 0.7055\n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.1632 - val_accuracy: 0.0000e+00 - val_loss: 0.6924\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.1596 - val_accuracy: 0.0000e+00 - val_loss: 0.6823\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 0.1621 - val_accuracy: 0.0000e+00 - val_loss: 0.6732\n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0000e+00 - loss: 0.1539 - val_accuracy: 0.0000e+00 - val_loss: 0.6659\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.1573 - val_accuracy: 0.0000e+00 - val_loss: 0.6574\n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.0000e+00 - loss: 0.1611 - val_accuracy: 0.0000e+00 - val_loss: 0.6480\n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: 0.1535 - val_accuracy: 0.0000e+00 - val_loss: 0.6387\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.1584 - val_accuracy: 0.0000e+00 - val_loss: 0.6291\n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.1593 - val_accuracy: 0.0000e+00 - val_loss: 0.6206\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.1592 - val_accuracy: 0.0000e+00 - val_loss: 0.6130\n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0000e+00 - loss: 0.1537 - val_accuracy: 0.0000e+00 - val_loss: 0.6063\n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.1535 - val_accuracy: 0.0000e+00 - val_loss: 0.5994\n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.1532 - val_accuracy: 0.0000e+00 - val_loss: 0.5932\n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 0.1539 - val_accuracy: 0.0000e+00 - val_loss: 0.5889\n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.1553 - val_accuracy: 0.0000e+00 - val_loss: 0.5850\n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.1552 - val_accuracy: 0.0000e+00 - val_loss: 0.5809\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.1548 - val_accuracy: 0.0000e+00 - val_loss: 0.5762\n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.1476 - val_accuracy: 0.0000e+00 - val_loss: 0.5732\n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.1400 - val_accuracy: 0.0000e+00 - val_loss: 0.5695\n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.1545 - val_accuracy: 0.0000e+00 - val_loss: 0.5636\n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.1505 - val_accuracy: 0.0000e+00 - val_loss: 0.5594\n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.1468 - val_accuracy: 0.0000e+00 - val_loss: 0.5567\n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.1500 - val_accuracy: 0.0000e+00 - val_loss: 0.5551\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0000e+00 - loss: 0.1463 - val_accuracy: 0.0000e+00 - val_loss: 0.5543\n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 0.1443 - val_accuracy: 0.0000e+00 - val_loss: 0.5549\n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 0.1437 - val_accuracy: 0.0000e+00 - val_loss: 0.5571\n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 0.1471 - val_accuracy: 0.0000e+00 - val_loss: 0.5583\n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.0000e+00 - loss: 0.1527 - val_accuracy: 0.0000e+00 - val_loss: 0.5567\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.1531 - val_accuracy: 0.0000e+00 - val_loss: 0.5547\n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.1446 - val_accuracy: 0.0000e+00 - val_loss: 0.5514\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0000e+00 - loss: 0.1473 - val_accuracy: 0.0000e+00 - val_loss: 0.5475\n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.1495 - val_accuracy: 0.0000e+00 - val_loss: 0.5446\n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0000e+00 - loss: 0.1507 - val_accuracy: 0.0000e+00 - val_loss: 0.5422\n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.0000e+00 - loss: 0.1388 - val_accuracy: 0.0000e+00 - val_loss: 0.5430\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.1402 - val_accuracy: 0.0000e+00 - val_loss: 0.5468\n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0000e+00 - loss: 0.1318 - val_accuracy: 0.0000e+00 - val_loss: 0.5492\n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0000e+00 - loss: 0.1407 - val_accuracy: 0.0000e+00 - val_loss: 0.5504\n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.1456 - val_accuracy: 0.0000e+00 - val_loss: 0.5531\n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.1349 - val_accuracy: 0.0000e+00 - val_loss: 0.5560\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0000e+00 - loss: 0.1397 - val_accuracy: 0.0000e+00 - val_loss: 0.5592\n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.0000e+00 - loss: 0.1481 - val_accuracy: 0.0000e+00 - val_loss: 0.5615\n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0000e+00 - loss: 0.1454 - val_accuracy: 0.0000e+00 - val_loss: 0.5627\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.1455 - val_accuracy: 0.0000e+00 - val_loss: 0.5640\n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.0000e+00 - loss: 0.1426 - val_accuracy: 0.0000e+00 - val_loss: 0.5640\n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.0000e+00 - loss: 0.1314 - val_accuracy: 0.0000e+00 - val_loss: 0.5610\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 0.1416 - val_accuracy: 0.0000e+00 - val_loss: 0.5579\n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.1450 - val_accuracy: 0.0000e+00 - val_loss: 0.5553\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.1398 - val_accuracy: 0.0000e+00 - val_loss: 0.5537\n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.1414 - val_accuracy: 0.0000e+00 - val_loss: 0.5541\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0000e+00 - loss: 0.1367 - val_accuracy: 0.0000e+00 - val_loss: 0.5560\n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.1426 - val_accuracy: 0.0000e+00 - val_loss: 0.5589\n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0000e+00 - loss: 0.1392 - val_accuracy: 0.0000e+00 - val_loss: 0.5608\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "r2: 0.6898262522026062\n",
      "mse 0.5608250727118701\n"
     ]
    }
   ],
   "source": [
    "#una vez tenemos los ingresos de 2019, calculamos las ventas equivalentes. Usamos RN\n",
    "#primero comprobamos que modelo usar\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))  # Capa oculta con 64 neuronas y función de activación ReLU\n",
    "model.add(Dense(1))  # Capa de salida con una neurona (predicción de ingresos)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy']) \n",
    "model.fit(Y_trainEsc, X_trainEsc, epochs=100, batch_size=32,validation_data=(Y_testEsc,X_testEsc))\n",
    "prediccionesVentas_2018 = model.predict(Y_testEsc)\n",
    "print(\"r2:\",r2_score(X_testEsc,prediccionesVentas_2018))\n",
    "print(\"mse\",mean_squared_error(X_testEsc,prediccionesVentas_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba52f697-d487-4509-a4f6-2e189caa4ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7387825657658229\n",
      "0.47230717489245055\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(Y_trainEsc,X_trainEsc)\n",
    "X_predRLEsc = regr.predict(Y_testEsc)\n",
    "print(r2_score(X_testEsc,X_predRLEsc))\n",
    "print(mean_squared_error(X_testEsc,X_predRLEsc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cc9c288-a65c-495d-96d2-67509549a28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.44564193],\n",
       "       [-1.36450568],\n",
       "       [-0.62268853],\n",
       "       [-0.26337085],\n",
       "       [-0.44882513],\n",
       "       [-0.61109764],\n",
       "       [-0.52996138],\n",
       "       [-0.27496174],\n",
       "       [ 1.19708167],\n",
       "       [-0.20541638],\n",
       "       [ 1.61435382],\n",
       "       [ 1.49844489],\n",
       "       [-1.07473336],\n",
       "       [-1.14427871],\n",
       "       [-0.24018906],\n",
       "       [-0.15905281],\n",
       "       [ 0.45526452],\n",
       "       [ 0.16549219],\n",
       "       [ 0.16549219],\n",
       "       [-0.07791656],\n",
       "       [ 1.93889882],\n",
       "       [ 0.11912862],\n",
       "       [ 2.135944  ],\n",
       "       [ 1.811399  ],\n",
       "       [-0.44882513],\n",
       "       [-0.90086996],\n",
       "       [ 0.64071881],\n",
       "       [ 0.21185577],\n",
       "       [ 0.67549149],\n",
       "       [ 0.61753702],\n",
       "       [ 0.4784463 ],\n",
       "       [ 0.37412827],\n",
       "       [ 3.14435169],\n",
       "       [ 1.30139971],\n",
       "       [ 3.17912437],\n",
       "       [ 3.21389705]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparamos los datos. Entrenaremos al modelo con datos de 2016 a 2018 para predecir 2019\n",
    "X_trainP = np.concatenate((X_trainEsc[12:36],X_testEsc))#ventas 2016-2018\n",
    "Y_trainP = np.concatenate((Y_trainEsc[12:36],Y_testEsc))#ingresos 2016-2018\n",
    "Y_testP = scalerIng.transform(pd.DataFrame(y_predSE2019,columns = [\"Total\"]))#ingresos 2019\n",
    "X_trainP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3385aa28-c5fe-40db-8669-3abae4ddffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 2.3358   \n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 2.1936  \n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0000e+00 - loss: 2.0898\n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.9854  \n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 2.0679\n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.8710  \n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 1.9409\n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.8498  \n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 1.6539 \n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.7609  \n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - accuracy: 0.0000e+00 - loss: 1.6134\n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.5927  \n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 1.5446 \n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.3925  \n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.4295  \n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.4135  \n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.2759  \n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.3173  \n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.2182  \n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.2471  \n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.1780  \n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 1.1483\n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.0466  \n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.0672  \n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.0437  \n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 1.0158  \n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.9690  \n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.9338  \n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.9024 \n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.9000  \n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.8784\n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.8200  \n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.8035  \n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.7301\n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.6975  \n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.6891  \n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.7173\n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.6317  \n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.6664 \n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.6501  \n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0000e+00 - loss: 0.6216\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.5750\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.5877  \n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.5673 \n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.5530  \n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.5314 \n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.5219  \n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.4951 \n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.4957  \n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.4652  \n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.4714  \n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.4393  \n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.4480  \n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.4328 \n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.4136 \n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.4181  \n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0000e+00 - loss: 0.3929\n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.3990  \n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.3452 \n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.3802 \n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.3627 \n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.3643  \n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.3542  \n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.3336\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.3293  \n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.3248  \n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.2987  \n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.3189  \n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0000e+00 - loss: 0.2845\n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.2957 \n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0000e+00 - loss: 0.3044\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.2930  \n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.2851  \n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.2940 \n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.2832  \n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0000e+00 - loss: 0.2796\n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2844 \n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.2411\n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.2571 \n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2825 \n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2733 \n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0000e+00 - loss: 0.2527\n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2649 \n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2690 \n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.2653  \n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.2574  \n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0000e+00 - loss: 0.2561\n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2632 \n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2629 \n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.0000e+00 - loss: 0.2444\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.2639  \n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.2409  \n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0000e+00 - loss: 0.2522\n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.2593  \n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.2557\n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.2546  \n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2511 \n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.2550 \n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.2577 \n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.2455 \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[236.],\n",
       "       [181.],\n",
       "       [315.],\n",
       "       [259.],\n",
       "       [297.],\n",
       "       [277.],\n",
       "       [279.],\n",
       "       [281.],\n",
       "       [375.],\n",
       "       [316.],\n",
       "       [408.],\n",
       "       [431.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#si no mejora, utilizar RL\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1, activation='relu'))  # Capa oculta con 64 neuronas y función de activación ReLU\n",
    "model.add(Dense(1))  # Capa de salida con una neurona (predicción de ingresos)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy']) \n",
    "model.fit(Y_trainP, X_trainP, epochs=100, batch_size=32)\n",
    "prediccionesVentas_2019Esc = model.predict(Y_testP)\n",
    "prediccionesVentas_2019 = np.round(scalerVent.inverse_transform(prediccionesVentas_2019Esc))\n",
    "prediccionesVentas_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3520d2e0-487e-4d1f-ae7d-19bc8f55e175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Total\n",
       "Order_Date       \n",
       "2018-01-31    143\n",
       "2018-02-28    104\n",
       "2018-03-31    237\n",
       "2018-04-30    200\n",
       "2018-05-31    240\n",
       "2018-06-30    235\n",
       "2018-07-31    223\n",
       "2018-08-31    214\n",
       "2018-09-30    453\n",
       "2018-10-31    294\n",
       "2018-11-30    456\n",
       "2018-12-31    459"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50660cb5-915f-4356-8163-a17079408fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>479.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Total\n",
       "2019-01-31  219.0\n",
       "2019-02-28  180.0\n",
       "2019-03-31  315.0\n",
       "2019-04-30  241.0\n",
       "2019-05-31  289.0\n",
       "2019-06-30  264.0\n",
       "2019-07-31  266.0\n",
       "2019-08-31  268.0\n",
       "2019-09-30  399.0\n",
       "2019-10-31  316.0\n",
       "2019-11-30  447.0\n",
       "2019-12-31  479.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(Y_trainP,X_trainP)\n",
    "X_predRLEsc2019 = regr.predict(Y_testP)\n",
    "X_predRL2019 = np.round(scalerVent.inverse_transform(X_predRLEsc2019))\n",
    "X_predRL2019 = pd.DataFrame(X_predRL2019,index = y_predSE2019.index, columns=[\"Total\"])\n",
    "X_predRL2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0236ad4-219c-468e-95a7-3cc8eeaa3426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.246753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.152174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.655844</td>\n",
       "      <td>0.207792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>0.223140</td>\n",
       "      <td>0.578512</td>\n",
       "      <td>0.198347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>0.160305</td>\n",
       "      <td>0.618321</td>\n",
       "      <td>0.221374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>0.190141</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>0.232394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>0.157534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>0.168539</td>\n",
       "      <td>0.621723</td>\n",
       "      <td>0.209738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.610063</td>\n",
       "      <td>0.201258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.196078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>0.178832</td>\n",
       "      <td>0.554745</td>\n",
       "      <td>0.266423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.203125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>0.232704</td>\n",
       "      <td>0.559748</td>\n",
       "      <td>0.207547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.209790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.596899</td>\n",
       "      <td>0.193798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.286765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>0.189873</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.202532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.638596</td>\n",
       "      <td>0.192982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.213415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>0.190031</td>\n",
       "      <td>0.573209</td>\n",
       "      <td>0.236760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>0.215434</td>\n",
       "      <td>0.581994</td>\n",
       "      <td>0.202572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>0.247191</td>\n",
       "      <td>0.505618</td>\n",
       "      <td>0.247191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>0.192771</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.228916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>0.229814</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.248447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>0.190045</td>\n",
       "      <td>0.624434</td>\n",
       "      <td>0.185520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.637755</td>\n",
       "      <td>0.178571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.647959</td>\n",
       "      <td>0.188776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>0.211429</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>0.128940</td>\n",
       "      <td>0.638968</td>\n",
       "      <td>0.232092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>0.151042</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.239583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>0.199454</td>\n",
       "      <td>0.579235</td>\n",
       "      <td>0.221311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>0.174556</td>\n",
       "      <td>0.582840</td>\n",
       "      <td>0.242604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>0.195804</td>\n",
       "      <td>0.650350</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>0.201923</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.240385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>0.232068</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.160338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.220833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>0.182979</td>\n",
       "      <td>0.608511</td>\n",
       "      <td>0.208511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>0.219731</td>\n",
       "      <td>0.556054</td>\n",
       "      <td>0.224215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>0.168224</td>\n",
       "      <td>0.654206</td>\n",
       "      <td>0.177570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>0.198675</td>\n",
       "      <td>0.620309</td>\n",
       "      <td>0.181015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.564626</td>\n",
       "      <td>0.241497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.585526</td>\n",
       "      <td>0.217105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>0.163399</td>\n",
       "      <td>0.605664</td>\n",
       "      <td>0.230937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Technology  Office Supplies  Furniture\n",
       "2015-01-31    0.116883         0.636364   0.246753\n",
       "2015-02-28    0.173913         0.673913   0.152174\n",
       "2015-03-31    0.136364         0.655844   0.207792\n",
       "2015-04-30    0.184615         0.615385   0.200000\n",
       "2015-05-31    0.223140         0.578512   0.198347\n",
       "2015-06-30    0.160305         0.618321   0.221374\n",
       "2015-07-31    0.190141         0.577465   0.232394\n",
       "2015-08-31    0.178082         0.664384   0.157534\n",
       "2015-09-30    0.168539         0.621723   0.209738\n",
       "2015-10-31    0.188679         0.610063   0.201258\n",
       "2015-11-30    0.196078         0.607843   0.196078\n",
       "2015-12-31    0.178832         0.554745   0.266423\n",
       "2016-01-31    0.175439         0.561404   0.263158\n",
       "2016-02-29    0.218750         0.609375   0.171875\n",
       "2016-03-31    0.210938         0.585938   0.203125\n",
       "2016-04-30    0.232704         0.559748   0.207547\n",
       "2016-05-31    0.153846         0.636364   0.209790\n",
       "2016-06-30    0.209302         0.596899   0.193798\n",
       "2016-07-31    0.154412         0.558824   0.286765\n",
       "2016-08-31    0.189873         0.607595   0.202532\n",
       "2016-09-30    0.168421         0.638596   0.192982\n",
       "2016-10-31    0.250000         0.536585   0.213415\n",
       "2016-11-30    0.190031         0.573209   0.236760\n",
       "2016-12-31    0.215434         0.581994   0.202572\n",
       "2017-01-31    0.247191         0.505618   0.247191\n",
       "2017-02-28    0.192771         0.578313   0.228916\n",
       "2017-03-31    0.229814         0.521739   0.248447\n",
       "2017-04-30    0.130952         0.660714   0.208333\n",
       "2017-05-31    0.190045         0.624434   0.185520\n",
       "2017-06-30    0.183673         0.637755   0.178571\n",
       "2017-07-31    0.163265         0.647959   0.188776\n",
       "2017-08-31    0.211429         0.628571   0.160000\n",
       "2017-09-30    0.128940         0.638968   0.232092\n",
       "2017-10-31    0.151042         0.609375   0.239583\n",
       "2017-11-30    0.199454         0.579235   0.221311\n",
       "2017-12-31    0.174556         0.582840   0.242604\n",
       "2018-01-31    0.195804         0.650350   0.153846\n",
       "2018-02-28    0.201923         0.557692   0.240385\n",
       "2018-03-31    0.232068         0.607595   0.160338\n",
       "2018-04-30    0.125000         0.655000   0.220000\n",
       "2018-05-31    0.175000         0.604167   0.220833\n",
       "2018-06-30    0.182979         0.608511   0.208511\n",
       "2018-07-31    0.219731         0.556054   0.224215\n",
       "2018-08-31    0.168224         0.654206   0.177570\n",
       "2018-09-30    0.198675         0.620309   0.181015\n",
       "2018-10-31    0.193878         0.564626   0.241497\n",
       "2018-11-30    0.197368         0.585526   0.217105\n",
       "2018-12-31    0.163399         0.605664   0.230937"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorcentajeTecnología = df_resultadoVent2[\"Technology\"] / df_resultadoVent2[\"Ventas Totales\"]\n",
    "PorcentajeOfficeSupplies = df_resultadoVent2[\"Office Supplies\"] / df_resultadoVent2[\"Ventas Totales\"]\n",
    "PorcentajeFurniture = df_resultadoVent2[\"Furniture\"] / df_resultadoVent2[\"Ventas Totales\"]\n",
    "array_concatenado = np.column_stack((PorcentajeTecnología,PorcentajeOfficeSupplies,PorcentajeFurniture))\n",
    "pdArray_concatenado = pd.DataFrame(array_concatenado,index=df_resultadoVent2.index.values,columns=[\"Technology\",\"Office Supplies\",\"Furniture\"])\n",
    "pdArray_concatenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96b3f7ef-a7e8-4c17-89f0-495c2f3a64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#una vez tenemos las ventas de 2019, calculamos las ventas de cada categoría\n",
    "pdArray_concatenadoTrain = pdArray_concatenado[12:48]\n",
    "df_resultadoIng2 = df_resultadoIng2[12:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "777ea0f8-e5dc-43e6-b24e-22d7340e6b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.34105725e-03,  1.16206653e-03, -1.09706603e-04],\n",
       "       [ 6.55941763e-02, -5.68393309e-02,  5.36600078e-03],\n",
       "       [-1.68804044e-01,  1.46273792e-01, -1.38091929e-02],\n",
       "       [-3.94082782e-02,  3.41484609e-02, -3.22383576e-03],\n",
       "       [-1.24690411e-01,  1.08047999e-01, -1.02004306e-02],\n",
       "       [-7.94134624e-02,  6.88141589e-02, -6.49650205e-03],\n",
       "       [-8.38639840e-02,  7.26706700e-02, -6.86058167e-03],\n",
       "       [-8.67663097e-02,  7.51856227e-02, -7.09800948e-03],\n",
       "       [-3.15710967e-01,  2.73573069e-01, -2.58270686e-02],\n",
       "       [-1.71603872e-01,  1.48699927e-01, -1.40382357e-02],\n",
       "       [-3.99004823e-01,  3.45749706e-01, -3.26410104e-02],\n",
       "       [-4.55595910e-01,  3.94787589e-01, -3.72705040e-02]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predecimos las ventas por categoría, \n",
    "scalerVentasCat = StandardScaler()\n",
    "VentasCatEsc = scalerVentasCat.fit_transform(pdArray_concatenadoTrain)\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_trainP,VentasCatEsc)\n",
    "X_predRLCatEsc = regr.predict(X_predRLEsc2019)\n",
    "X_predRLCatEsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86c9a67f-4c80-497d-aa48-9736ad356251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>0.189579</td>\n",
       "      <td>0.598149</td>\n",
       "      <td>0.212272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>0.191658</td>\n",
       "      <td>0.595906</td>\n",
       "      <td>0.212436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>0.184376</td>\n",
       "      <td>0.603761</td>\n",
       "      <td>0.211863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>0.188396</td>\n",
       "      <td>0.599425</td>\n",
       "      <td>0.212179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>0.185746</td>\n",
       "      <td>0.602283</td>\n",
       "      <td>0.211971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>0.187153</td>\n",
       "      <td>0.600765</td>\n",
       "      <td>0.212082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>0.187015</td>\n",
       "      <td>0.600915</td>\n",
       "      <td>0.212071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>0.186925</td>\n",
       "      <td>0.601012</td>\n",
       "      <td>0.212064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>0.179812</td>\n",
       "      <td>0.608684</td>\n",
       "      <td>0.211504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>0.184289</td>\n",
       "      <td>0.603855</td>\n",
       "      <td>0.211856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>0.177224</td>\n",
       "      <td>0.611475</td>\n",
       "      <td>0.211301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.175466</td>\n",
       "      <td>0.613371</td>\n",
       "      <td>0.211163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Technology  Office Supplies  Furniture\n",
       "2019-01-31    0.189579         0.598149   0.212272\n",
       "2019-02-28    0.191658         0.595906   0.212436\n",
       "2019-03-31    0.184376         0.603761   0.211863\n",
       "2019-04-30    0.188396         0.599425   0.212179\n",
       "2019-05-31    0.185746         0.602283   0.211971\n",
       "2019-06-30    0.187153         0.600765   0.212082\n",
       "2019-07-31    0.187015         0.600915   0.212071\n",
       "2019-08-31    0.186925         0.601012   0.212064\n",
       "2019-09-30    0.179812         0.608684   0.211504\n",
       "2019-10-31    0.184289         0.603855   0.211856\n",
       "2019-11-30    0.177224         0.611475   0.211301\n",
       "2019-12-31    0.175466         0.613371   0.211163"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predRLCat = scalerVentasCat.inverse_transform(X_predRLCatEsc)\n",
    "X_predRLCat = pd.DataFrame(X_predRLCat,index = y_predSE2019.index,columns = pdArray_concatenado.columns)\n",
    "X_predRLCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5c529e5-ba6b-4065-a11f-0b1654a1fc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>42.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>34.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>58.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>45.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>54.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>49.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>50.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>50.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>72.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>58.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>79.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>84.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Technology  Office Supplies  Furniture\n",
       "2019-01-31        42.0            131.0       46.0\n",
       "2019-02-28        34.0            107.0       38.0\n",
       "2019-03-31        58.0            190.0       67.0\n",
       "2019-04-30        45.0            144.0       51.0\n",
       "2019-05-31        54.0            174.0       61.0\n",
       "2019-06-30        49.0            159.0       56.0\n",
       "2019-07-31        50.0            160.0       56.0\n",
       "2019-08-31        50.0            161.0       57.0\n",
       "2019-09-30        72.0            243.0       84.0\n",
       "2019-10-31        58.0            191.0       67.0\n",
       "2019-11-30        79.0            273.0       94.0\n",
       "2019-12-31        84.0            294.0      101.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiplicamos las predicciones por las ventas totales de 2019 para calcular las ventas mensuales de 2019 por categoría\n",
    "prediccionVentas2019Cat = np.round(X_predRL2019.values * X_predRLCat.values)\n",
    "prediccionVentas2019Cat = pd.DataFrame(prediccionVentas2019Cat,index = y_predSE2019.index, columns = pdArray_concatenado.columns)\n",
    "prediccionVentas2019Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b255630-9abe-45c6-96db-fcd24518f53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>3143.290</td>\n",
       "      <td>4845.140</td>\n",
       "      <td>6217.2770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>1608.510</td>\n",
       "      <td>1071.724</td>\n",
       "      <td>1839.6580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>32359.974</td>\n",
       "      <td>8602.455</td>\n",
       "      <td>14243.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>8973.144</td>\n",
       "      <td>10988.874</td>\n",
       "      <td>7944.8370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>9599.876</td>\n",
       "      <td>7131.640</td>\n",
       "      <td>6912.7870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>8435.965</td>\n",
       "      <td>12742.389</td>\n",
       "      <td>13144.5816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>7839.284</td>\n",
       "      <td>15121.208</td>\n",
       "      <td>10821.0510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>8937.050</td>\n",
       "      <td>11055.276</td>\n",
       "      <td>7125.2105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>30383.748</td>\n",
       "      <td>27423.298</td>\n",
       "      <td>23816.4808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>11938.018</td>\n",
       "      <td>7211.128</td>\n",
       "      <td>12304.2470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>30073.424</td>\n",
       "      <td>26363.196</td>\n",
       "      <td>21471.0407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>20573.224</td>\n",
       "      <td>16956.492</td>\n",
       "      <td>30637.3425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>4518.236</td>\n",
       "      <td>1808.780</td>\n",
       "      <td>11739.9416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>3448.970</td>\n",
       "      <td>5368.067</td>\n",
       "      <td>3134.3740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>10143.942</td>\n",
       "      <td>13472.739</td>\n",
       "      <td>8722.6374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>11160.952</td>\n",
       "      <td>12517.818</td>\n",
       "      <td>10475.6985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>11643.000</td>\n",
       "      <td>8941.580</td>\n",
       "      <td>9374.9505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>6435.366</td>\n",
       "      <td>9623.037</td>\n",
       "      <td>7540.9710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>10234.976</td>\n",
       "      <td>4706.243</td>\n",
       "      <td>13667.0400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>15444.642</td>\n",
       "      <td>11735.108</td>\n",
       "      <td>9638.5922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>19017.128</td>\n",
       "      <td>19031.055</td>\n",
       "      <td>25085.4230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>10704.890</td>\n",
       "      <td>8642.360</td>\n",
       "      <td>11664.4875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>23873.601</td>\n",
       "      <td>21178.298</td>\n",
       "      <td>30197.5005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>35632.028</td>\n",
       "      <td>16099.322</td>\n",
       "      <td>22812.2512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>5620.066</td>\n",
       "      <td>5299.682</td>\n",
       "      <td>7622.7430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>12258.914</td>\n",
       "      <td>6794.350</td>\n",
       "      <td>3925.5510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>21567.852</td>\n",
       "      <td>17324.545</td>\n",
       "      <td>12272.6620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>14890.502</td>\n",
       "      <td>10577.175</td>\n",
       "      <td>13212.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>28832.691</td>\n",
       "      <td>12931.642</td>\n",
       "      <td>14892.5750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>16372.152</td>\n",
       "      <td>10901.810</td>\n",
       "      <td>12450.5240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>12847.200</td>\n",
       "      <td>12903.824</td>\n",
       "      <td>12569.7590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>9672.402</td>\n",
       "      <td>8959.740</td>\n",
       "      <td>11910.0583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>19424.514</td>\n",
       "      <td>23013.402</td>\n",
       "      <td>26755.4749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>31533.374</td>\n",
       "      <td>16236.650</td>\n",
       "      <td>11813.0090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>27141.059</td>\n",
       "      <td>20141.808</td>\n",
       "      <td>31783.6288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>21801.218</td>\n",
       "      <td>37332.938</td>\n",
       "      <td>36604.9650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>16493.333</td>\n",
       "      <td>21052.979</td>\n",
       "      <td>5930.1620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>5768.448</td>\n",
       "      <td>7378.172</td>\n",
       "      <td>6774.3774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>33428.622</td>\n",
       "      <td>14541.346</td>\n",
       "      <td>10893.4448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>12261.005</td>\n",
       "      <td>14214.947</td>\n",
       "      <td>9065.9581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>13374.620</td>\n",
       "      <td>13493.804</td>\n",
       "      <td>16957.5582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>16937.240</td>\n",
       "      <td>14221.101</td>\n",
       "      <td>17032.3867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>23209.926</td>\n",
       "      <td>10239.356</td>\n",
       "      <td>11375.8220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>17619.162</td>\n",
       "      <td>29799.564</td>\n",
       "      <td>15419.1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>26028.659</td>\n",
       "      <td>31607.523</td>\n",
       "      <td>28516.7060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>32855.663</td>\n",
       "      <td>22708.400</td>\n",
       "      <td>21884.0682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>49409.103</td>\n",
       "      <td>31472.337</td>\n",
       "      <td>37056.7150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>21984.910</td>\n",
       "      <td>29638.012</td>\n",
       "      <td>31407.4668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category    Technology  Office Supplies   Furniture\n",
       "Order_Date                                         \n",
       "2015-01-31    3143.290         4845.140   6217.2770\n",
       "2015-02-28    1608.510         1071.724   1839.6580\n",
       "2015-03-31   32359.974         8602.455  14243.3680\n",
       "2015-04-30    8973.144        10988.874   7944.8370\n",
       "2015-05-31    9599.876         7131.640   6912.7870\n",
       "2015-06-30    8435.965        12742.389  13144.5816\n",
       "2015-07-31    7839.284        15121.208  10821.0510\n",
       "2015-08-31    8937.050        11055.276   7125.2105\n",
       "2015-09-30   30383.748        27423.298  23816.4808\n",
       "2015-10-31   11938.018         7211.128  12304.2470\n",
       "2015-11-30   30073.424        26363.196  21471.0407\n",
       "2015-12-31   20573.224        16956.492  30637.3425\n",
       "2016-01-31    4518.236         1808.780  11739.9416\n",
       "2016-02-29    3448.970         5368.067   3134.3740\n",
       "2016-03-31   10143.942        13472.739   8722.6374\n",
       "2016-04-30   11160.952        12517.818  10475.6985\n",
       "2016-05-31   11643.000         8941.580   9374.9505\n",
       "2016-06-30    6435.366         9623.037   7540.9710\n",
       "2016-07-31   10234.976         4706.243  13667.0400\n",
       "2016-08-31   15444.642        11735.108   9638.5922\n",
       "2016-09-30   19017.128        19031.055  25085.4230\n",
       "2016-10-31   10704.890         8642.360  11664.4875\n",
       "2016-11-30   23873.601        21178.298  30197.5005\n",
       "2016-12-31   35632.028        16099.322  22812.2512\n",
       "2017-01-31    5620.066         5299.682   7622.7430\n",
       "2017-02-28   12258.914         6794.350   3925.5510\n",
       "2017-03-31   21567.852        17324.545  12272.6620\n",
       "2017-04-30   14890.502        10577.175  13212.0900\n",
       "2017-05-31   28832.691        12931.642  14892.5750\n",
       "2017-06-30   16372.152        10901.810  12450.5240\n",
       "2017-07-31   12847.200        12903.824  12569.7590\n",
       "2017-08-31    9672.402         8959.740  11910.0583\n",
       "2017-09-30   19424.514        23013.402  26755.4749\n",
       "2017-10-31   31533.374        16236.650  11813.0090\n",
       "2017-11-30   27141.059        20141.808  31783.6288\n",
       "2017-12-31   21801.218        37332.938  36604.9650\n",
       "2018-01-31   16493.333        21052.979   5930.1620\n",
       "2018-02-28    5768.448         7378.172   6774.3774\n",
       "2018-03-31   33428.622        14541.346  10893.4448\n",
       "2018-04-30   12261.005        14214.947   9065.9581\n",
       "2018-05-31   13374.620        13493.804  16957.5582\n",
       "2018-06-30   16937.240        14221.101  17032.3867\n",
       "2018-07-31   23209.926        10239.356  11375.8220\n",
       "2018-08-31   17619.162        29799.564  15419.1220\n",
       "2018-09-30   26028.659        31607.523  28516.7060\n",
       "2018-10-31   32855.663        22708.400  21884.0682\n",
       "2018-11-30   49409.103        31472.337  37056.7150\n",
       "2018-12-31   21984.910        29638.012  31407.4668"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ingresos por categoría\n",
    "#hago una predicción entre ventas por categoría e ingresos por categoría. Tengo que obtener los ingresos por categoría entre 2016 y 2018\n",
    "#preparo los datos y luego pruebo con algoritmos rf, rl y rn\n",
    "\n",
    "df_resultadoIng2 = df3.groupby('Category').resample(\"ME\",on=\"Order_Date\").sum() #ingresos de entrenamiento\n",
    "df_resultadoIng2 = df_resultadoIng2[[\"Total\"]]\n",
    "df_resultadoIng2 = df_resultadoIng2.unstack(level=0)\n",
    "df_resultadoIng2.columns = df_resultadoIng2.columns.droplevel(0)\n",
    "df_resultadoIng2 = df_resultadoIng2[[\"Technology\",\"Office Supplies\",\"Furniture\"]]\n",
    "df_resultadoIng2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a4d0b05-6566-49bf-bfa4-850d7844428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerIngCat = StandardScaler()\n",
    "IngCatTrain = df_resultadoIng2[0:36] #ingresos por categoría de 2015 a 2017\n",
    "IngCatTest = df_resultadoIng2[36:48] #ingresos por categoría 2018\n",
    "VenCatTrain = pdArray_concatenado[0:36] #ventas por categoría 2015 a 2017\n",
    "VenCatTest = pdArray_concatenado[36:48] #ventas por categoría 2018\n",
    "IngCatTrainEsc = scalerIngCat.fit_transform(IngCatTrain) #ingresos categoria de entrenamiento\n",
    "IngCatTestEsc = scalerIngCat.transform(IngCatTest) #ingresos categoria de test\n",
    "VenCatTrainEsc = scalerVentasCat.transform(VenCatTrain) #ingresos categoria de entrenamiento\n",
    "VenCatTestEsc = scalerVentasCat.transform(VenCatTest) #ingresos categoria de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9d3525b-40ef-4f92-94a4-c543046cb6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3553 - loss: 1.0190  \n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3553 - loss: 1.0028  \n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3079 - loss: 0.9887  \n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3657 - loss: 0.9945\n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3553 - loss: 0.9283  \n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3947 - loss: 0.9409\n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4028 - loss: 0.9900  \n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3762 - loss: 0.9050 \n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3762 - loss: 0.9699  \n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3553 - loss: 0.9692 \n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3762 - loss: 0.9065  \n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3368 - loss: 0.9609 \n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3368 - loss: 0.9270  \n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3079 - loss: 0.8948  \n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3264 - loss: 0.9377  \n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3843 - loss: 0.9287  \n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4236 - loss: 0.9270 \n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4525 - loss: 0.8920  \n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4317 - loss: 0.9132  \n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.9355 \n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4317 - loss: 0.9131 \n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4213 - loss: 0.9244  \n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4421 - loss: 0.8956\n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4525 - loss: 0.9068  \n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4421 - loss: 0.9085\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4525 - loss: 0.8520  \n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4421 - loss: 0.9098 \n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4317 - loss: 0.9069  \n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4630 - loss: 0.9053 \n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4421 - loss: 0.8758  \n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4317 - loss: 0.9151  \n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4421 - loss: 0.9026 \n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4317 - loss: 0.8757  \n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4421 - loss: 0.9245 \n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4317 - loss: 0.8678 \n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4421 - loss: 0.8802 \n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4317 - loss: 0.9160  \n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4630 - loss: 0.9110  \n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4132 - loss: 0.8390  \n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4132 - loss: 0.8860  \n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4132 - loss: 0.9157\n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4236 - loss: 0.8859  \n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4340 - loss: 0.8986 \n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4236 - loss: 0.8537  \n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4236 - loss: 0.9101  \n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3947 - loss: 0.8967 \n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3843 - loss: 0.8777  \n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3947 - loss: 0.8857 \n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3843 - loss: 0.8401\n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3843 - loss: 0.8484\n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4051 - loss: 0.8864  \n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3947 - loss: 0.8877\n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4340 - loss: 0.8508 \n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4028 - loss: 0.8767  \n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4028 - loss: 0.8728\n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4132 - loss: 0.9013  \n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4132 - loss: 0.8835 \n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4028 - loss: 0.8754  \n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4028 - loss: 0.8705 \n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3947 - loss: 0.8779 \n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3947 - loss: 0.8979 \n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3843 - loss: 0.8717 \n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3843 - loss: 0.8723 \n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3843 - loss: 0.8594 \n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3738 - loss: 0.8531 \n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3738 - loss: 0.8761 \n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3947 - loss: 0.8502 \n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3947 - loss: 0.8744 \n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3843 - loss: 0.8907 \n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3947 - loss: 0.8688 \n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3843 - loss: 0.8371 \n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3947 - loss: 0.8406 \n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3947 - loss: 0.8813 \n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3843 - loss: 0.8739 \n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3843 - loss: 0.8625 \n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3738 - loss: 0.8624 \n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3738 - loss: 0.8822 \n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3843 - loss: 0.8135 \n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4051 - loss: 0.8565 \n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3843 - loss: 0.8669  \n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3843 - loss: 0.8869  \n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4236 - loss: 0.8252  \n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4132 - loss: 0.8684 \n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4236 - loss: 0.8669 \n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4340 - loss: 0.8530 \n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4236 - loss: 0.8703  \n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4236 - loss: 0.8291 \n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4132 - loss: 0.8540 \n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4236 - loss: 0.8581 \n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4421 - loss: 0.8681 \n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4421 - loss: 0.8386  \n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4421 - loss: 0.8240  \n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4630 - loss: 0.8620  \n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.8775 \n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4525 - loss: 0.8493  \n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4421 - loss: 0.8211 \n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4213 - loss: 0.8766 \n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4132 - loss: 0.8620 \n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4340 - loss: 0.8235 \n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4028 - loss: 0.8693  \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
     ]
    }
   ],
   "source": [
    "#Una vez escalados los datos, probamos qué algoritmo da mejores resultados\n",
    "#RL\n",
    "regrIngCat = linear_model.LinearRegression()\n",
    "regrIngCat.fit(VenCatTrainEsc,IngCatTrainEsc)\n",
    "predIngCat2018RLEsc = regrIngCat.predict(VenCatTestEsc)\n",
    "\n",
    "#RF\n",
    "rf_modelIngCat = RandomForestRegressor(n_estimators=100,random_state=4)\n",
    "rf_modelIngCat.fit(VenCatTrainEsc, IngCatTrainEsc)\n",
    "predIngCat2018RFEsc = rf_modelIngCat.predict(VenCatTestEsc)\n",
    "\n",
    "#RN\n",
    "modelRNIngCat = Sequential()\n",
    "modelRNIngCat.add(Dense(64, input_dim=3, activation='relu'))  # Capa oculta con 64 neuronas y función de activación ReLU\n",
    "modelRNIngCat.add(Dense(3))  # Capa de salida con una neurona (predicción de ingresos)\n",
    "modelRNIngCat.compile(optimizer='adam', loss='mse', metrics=['accuracy']) \n",
    "modelRNIngCat.fit(VenCatTrainEsc, IngCatTrainEsc, epochs=100, batch_size=32)\n",
    "predIngCat2018RNEsc = modelRNIngCat.predict(VenCatTestEsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9062b9dc-d874-4959-b328-cb6b91088863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El coeficiente de determinación del algoritmo de regresión lineal con el valor real es de:  0.7276003992506133\n",
      "El coeficiente de determinación del algoritmo random forest con el valor real es de:  0.6419867923353242\n",
      "El coeficiente de determinación del algoritmo de red neuronal con el valor real es de:  0.7607829412301804\n"
     ]
    }
   ],
   "source": [
    "#comparamos resultados\n",
    "r2RLIC = r2_score(IngCatTestEsc, predIngCat2018RLEsc)\n",
    "r2RFIC = r2_score(IngCatTestEsc, predIngCat2018RFEsc)\n",
    "r2RNIC = r2_score(IngCatTestEsc, predIngCat2018RNEsc)\n",
    "print(\"El coeficiente de determinación del algoritmo de regresión lineal con el valor real es de: \",r2RL)\n",
    "print(\"El coeficiente de determinación del algoritmo random forest con el valor real es de: \",r2RF)\n",
    "print(\"El coeficiente de determinación del algoritmo de red neuronal con el valor real es de: \",r2RN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b4e2b42-11cb-4755-9bf7-aa3218f38d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error cuadrático medio del algoritmo de regresión lineal con el valor real es de:  1.8309447467705982\n",
      "El error cuadrático medio del algoritmo random forest con el valor real es de:  1.6611219386241507\n",
      "El error cuadrático medio del algoritmo de red neuronal con el valor real es de:  2.047707065811102\n"
     ]
    }
   ],
   "source": [
    "#comparamos resultados\n",
    "mseRLIC = mean_squared_error(IngCatTestEsc, predIngCat2018RLEsc)\n",
    "mseRFIC = mean_squared_error(IngCatTestEsc, predIngCat2018RFEsc)\n",
    "mseRNIC = mean_squared_error(IngCatTestEsc, predIngCat2018RNEsc)\n",
    "print(\"El error cuadrático medio del algoritmo de regresión lineal con el valor real es de: \",mseRLIC)\n",
    "print(\"El error cuadrático medio del algoritmo random forest con el valor real es de: \",mseRFIC)\n",
    "print(\"El error cuadrático medio del algoritmo de red neuronal con el valor real es de: \",mseRNIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4723da5c-c63e-479e-83b1-6f29396c0be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\tutorial-env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0s/step - accuracy: 0.3368 - loss: 1.1930   \n",
      "Epoch 2/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3160 - loss: 1.1296  \n",
      "Epoch 3/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3264 - loss: 1.0156  \n",
      "Epoch 4/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3472 - loss: 1.0591  \n",
      "Epoch 5/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3657 - loss: 0.9981  \n",
      "Epoch 6/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3657 - loss: 0.9291  \n",
      "Epoch 7/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3657 - loss: 0.9239  \n",
      "Epoch 8/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3553 - loss: 0.8871\n",
      "Epoch 9/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3657 - loss: 0.7998  \n",
      "Epoch 10/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3657 - loss: 0.8166\n",
      "Epoch 11/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3947 - loss: 0.7870  \n",
      "Epoch 12/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.3947 - loss: 0.7600\n",
      "Epoch 13/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3738 - loss: 0.7374  \n",
      "Epoch 14/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3368 - loss: 0.7135  \n",
      "Epoch 15/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2870 - loss: 0.6588  \n",
      "Epoch 16/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3079 - loss: 0.6618  \n",
      "Epoch 17/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2685 - loss: 0.6430\n",
      "Epoch 18/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2789 - loss: 0.5801  \n",
      "Epoch 19/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2685 - loss: 0.5982\n",
      "Epoch 20/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2685 - loss: 0.5542  \n",
      "Epoch 21/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2685 - loss: 0.5529 \n",
      "Epoch 22/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2894 - loss: 0.5539  \n",
      "Epoch 23/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3264 - loss: 0.5358  \n",
      "Epoch 24/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3368 - loss: 0.5194 \n",
      "Epoch 25/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3079 - loss: 0.4972\n",
      "Epoch 26/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2975 - loss: 0.5125  \n",
      "Epoch 27/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2975 - loss: 0.4965  \n",
      "Epoch 28/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3079 - loss: 0.4835  \n",
      "Epoch 29/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3657 - loss: 0.4632  \n",
      "Epoch 30/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3368 - loss: 0.4398\n",
      "Epoch 31/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3472 - loss: 0.4552  \n",
      "Epoch 32/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3553 - loss: 0.4511\n",
      "Epoch 33/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3368 - loss: 0.4280  \n",
      "Epoch 34/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3472 - loss: 0.3986 \n",
      "Epoch 35/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3657 - loss: 0.4189 \n",
      "Epoch 36/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3553 - loss: 0.4188  \n",
      "Epoch 37/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2789 - loss: 0.3871 \n",
      "Epoch 38/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2685 - loss: 0.3978  \n",
      "Epoch 39/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2975 - loss: 0.3949  \n",
      "Epoch 40/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3079 - loss: 0.3877 \n",
      "Epoch 41/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2975 - loss: 0.3857  \n",
      "Epoch 42/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2870 - loss: 0.3782\n",
      "Epoch 43/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3368 - loss: 0.3629  \n",
      "Epoch 44/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3472 - loss: 0.3662  \n",
      "Epoch 45/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3079 - loss: 0.3572  \n",
      "Epoch 46/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2975 - loss: 0.3619  \n",
      "Epoch 47/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3079 - loss: 0.3673\n",
      "Epoch 48/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2975 - loss: 0.3624  \n",
      "Epoch 49/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3079 - loss: 0.3657  \n",
      "Epoch 50/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3079 - loss: 0.3747  \n",
      "Epoch 51/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3472 - loss: 0.3637  \n",
      "Epoch 52/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3264 - loss: 0.3660  \n",
      "Epoch 53/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3264 - loss: 0.3737 \n",
      "Epoch 54/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3368 - loss: 0.3527 \n",
      "Epoch 55/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3160 - loss: 0.3660  \n",
      "Epoch 56/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3264 - loss: 0.3552 \n",
      "Epoch 57/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3160 - loss: 0.3660  \n",
      "Epoch 58/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3762 - loss: 0.3467 \n",
      "Epoch 59/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3449 - loss: 0.3504  \n",
      "Epoch 60/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3553 - loss: 0.3564 \n",
      "Epoch 61/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3657 - loss: 0.3506 \n",
      "Epoch 62/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3843 - loss: 0.3576  \n",
      "Epoch 63/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3843 - loss: 0.3501  \n",
      "Epoch 64/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.3345 - loss: 0.3403\n",
      "Epoch 65/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3345 - loss: 0.3571  \n",
      "Epoch 66/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3843 - loss: 0.3467  \n",
      "Epoch 67/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3738 - loss: 0.3514 \n",
      "Epoch 68/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3947 - loss: 0.3531  \n",
      "Epoch 69/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3947 - loss: 0.3524  \n",
      "Epoch 70/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3738 - loss: 0.3373  \n",
      "Epoch 71/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3947 - loss: 0.3451\n",
      "Epoch 72/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4236 - loss: 0.3425  \n",
      "Epoch 73/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4132 - loss: 0.3453 \n",
      "Epoch 74/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4132 - loss: 0.3496  \n",
      "Epoch 75/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4340 - loss: 0.3460 \n",
      "Epoch 76/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4051 - loss: 0.3366 \n",
      "Epoch 77/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4132 - loss: 0.3299 \n",
      "Epoch 78/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4421 - loss: 0.3386  \n",
      "Epoch 79/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4525 - loss: 0.3360  \n",
      "Epoch 80/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4317 - loss: 0.3431 \n",
      "Epoch 81/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3738 - loss: 0.3442 \n",
      "Epoch 82/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3843 - loss: 0.3225 \n",
      "Epoch 83/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3843 - loss: 0.3046 \n",
      "Epoch 84/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4340 - loss: 0.3241 \n",
      "Epoch 85/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4317 - loss: 0.3337  \n",
      "Epoch 86/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4421 - loss: 0.3160 \n",
      "Epoch 87/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4317 - loss: 0.3388  \n",
      "Epoch 88/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4340 - loss: 0.3321 \n",
      "Epoch 89/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4236 - loss: 0.3326  \n",
      "Epoch 90/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - accuracy: 0.4236 - loss: 0.3348\n",
      "Epoch 91/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3947 - loss: 0.3261  \n",
      "Epoch 92/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3843 - loss: 0.3370 \n",
      "Epoch 93/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.3843 - loss: 0.3199  \n",
      "Epoch 94/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3947 - loss: 0.3237 \n",
      "Epoch 95/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4236 - loss: 0.3244 \n",
      "Epoch 96/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4236 - loss: 0.3262 \n",
      "Epoch 97/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4317 - loss: 0.3198\n",
      "Epoch 98/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.4606 - loss: 0.3292  \n",
      "Epoch 99/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4815 - loss: 0.3234 \n",
      "Epoch 100/100\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4711 - loss: 0.3188 \n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001CDF80940E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n"
     ]
    }
   ],
   "source": [
    "#volvemos a predecir, usando RL\n",
    "scalerVentasCat2 = StandardScaler()\n",
    "VenCatTrain = df_resultadoVent2[[\"Technology\",\"Office Supplies\",\"Furniture\"]][12:48] #ventas 2016 a 2018\n",
    "VenCatTest = prediccionVentas2019Cat #ventas categoría 2019\n",
    "IngCatTrain = df_resultadoIng2[12:48]\n",
    "#escalamos los datos\n",
    "VenCatTrainEsc = scalerVentasCat2.fit_transform(VenCatTrain)\n",
    "VenCatTestEsc =  scalerVentasCat2.transform(VenCatTest)\n",
    "IngCatTrainEsc = scalerIngCat.transform(IngCatTrain)\n",
    "#predecimos los ingresos por categoría de 2019\n",
    "modelRNIngCat2019 = Sequential()\n",
    "modelRNIngCat2019.add(Dense(64, input_dim=3, activation='relu'))  # Capa oculta con 64 neuronas y función de activación ReLU\n",
    "modelRNIngCat2019.add(Dense(3))  # Capa de salida con una neurona (predicción de ingresos)\n",
    "modelRNIngCat2019.compile(optimizer='adam', loss='mse', metrics=['accuracy']) \n",
    "modelRNIngCat2019.fit(VenCatTrainEsc, IngCatTrainEsc, epochs=100, batch_size=32)\n",
    "predIngCat2019RNEsc = modelRNIngCat2019.predict(VenCatTestEsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "776a45bf-71a8-4ae5-a2e1-0abe90fac21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>18752.746094</td>\n",
       "      <td>14766.466797</td>\n",
       "      <td>15311.119141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>14737.069336</td>\n",
       "      <td>12161.745117</td>\n",
       "      <td>11564.304688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>25111.851562</td>\n",
       "      <td>21308.195312</td>\n",
       "      <td>24246.886719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>20025.031250</td>\n",
       "      <td>16339.403320</td>\n",
       "      <td>17920.017578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>23412.404297</td>\n",
       "      <td>19552.759766</td>\n",
       "      <td>22078.068359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>21589.613281</td>\n",
       "      <td>17929.630859</td>\n",
       "      <td>20068.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>21837.064453</td>\n",
       "      <td>18021.697266</td>\n",
       "      <td>20192.105469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>21950.597656</td>\n",
       "      <td>18180.832031</td>\n",
       "      <td>20444.789062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>30616.365234</td>\n",
       "      <td>26880.906250</td>\n",
       "      <td>30857.195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>25122.427734</td>\n",
       "      <td>21373.429688</td>\n",
       "      <td>24277.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>33553.144531</td>\n",
       "      <td>30056.023438</td>\n",
       "      <td>34591.367188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>35629.929688</td>\n",
       "      <td>32287.302734</td>\n",
       "      <td>37211.417969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category      Technology  Office Supplies     Furniture\n",
       "2019-01-31  18752.746094     14766.466797  15311.119141\n",
       "2019-02-28  14737.069336     12161.745117  11564.304688\n",
       "2019-03-31  25111.851562     21308.195312  24246.886719\n",
       "2019-04-30  20025.031250     16339.403320  17920.017578\n",
       "2019-05-31  23412.404297     19552.759766  22078.068359\n",
       "2019-06-30  21589.613281     17929.630859  20068.990234\n",
       "2019-07-31  21837.064453     18021.697266  20192.105469\n",
       "2019-08-31  21950.597656     18180.832031  20444.789062\n",
       "2019-09-30  30616.365234     26880.906250  30857.195312\n",
       "2019-10-31  25122.427734     21373.429688  24277.062500\n",
       "2019-11-30  33553.144531     30056.023438  34591.367188\n",
       "2019-12-31  35629.929688     32287.302734  37211.417969"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predIngCat2019RN = scalerIngCat.inverse_transform(predIngCat2019RNEsc)\n",
    "predIngCat2019 = pd.DataFrame(predIngCat2019RN,columns = df_resultadoIng2.columns, index = prediccionVentas2019Cat.index.values)\n",
    "predIngCat2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d1eef0b-f874-42a2-970e-db53e6989d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>3143.290000</td>\n",
       "      <td>4845.140000</td>\n",
       "      <td>6217.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>1608.510000</td>\n",
       "      <td>1071.724000</td>\n",
       "      <td>1839.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>32359.974000</td>\n",
       "      <td>8602.455000</td>\n",
       "      <td>14243.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>8973.144000</td>\n",
       "      <td>10988.874000</td>\n",
       "      <td>7944.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>9599.876000</td>\n",
       "      <td>7131.640000</td>\n",
       "      <td>6912.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>8435.965000</td>\n",
       "      <td>12742.389000</td>\n",
       "      <td>13144.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>7839.284000</td>\n",
       "      <td>15121.208000</td>\n",
       "      <td>10821.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>8937.050000</td>\n",
       "      <td>11055.276000</td>\n",
       "      <td>7125.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>30383.748000</td>\n",
       "      <td>27423.298000</td>\n",
       "      <td>23816.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>11938.018000</td>\n",
       "      <td>7211.128000</td>\n",
       "      <td>12304.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>30073.424000</td>\n",
       "      <td>26363.196000</td>\n",
       "      <td>21471.040700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>20573.224000</td>\n",
       "      <td>16956.492000</td>\n",
       "      <td>30637.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>4518.236000</td>\n",
       "      <td>1808.780000</td>\n",
       "      <td>11739.941600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>3448.970000</td>\n",
       "      <td>5368.067000</td>\n",
       "      <td>3134.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>10143.942000</td>\n",
       "      <td>13472.739000</td>\n",
       "      <td>8722.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>11160.952000</td>\n",
       "      <td>12517.818000</td>\n",
       "      <td>10475.698500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>11643.000000</td>\n",
       "      <td>8941.580000</td>\n",
       "      <td>9374.950500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>6435.366000</td>\n",
       "      <td>9623.037000</td>\n",
       "      <td>7540.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>10234.976000</td>\n",
       "      <td>4706.243000</td>\n",
       "      <td>13667.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>15444.642000</td>\n",
       "      <td>11735.108000</td>\n",
       "      <td>9638.592200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>19017.128000</td>\n",
       "      <td>19031.055000</td>\n",
       "      <td>25085.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>10704.890000</td>\n",
       "      <td>8642.360000</td>\n",
       "      <td>11664.487500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>23873.601000</td>\n",
       "      <td>21178.298000</td>\n",
       "      <td>30197.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>35632.028000</td>\n",
       "      <td>16099.322000</td>\n",
       "      <td>22812.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>5620.066000</td>\n",
       "      <td>5299.682000</td>\n",
       "      <td>7622.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>12258.914000</td>\n",
       "      <td>6794.350000</td>\n",
       "      <td>3925.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>21567.852000</td>\n",
       "      <td>17324.545000</td>\n",
       "      <td>12272.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>14890.502000</td>\n",
       "      <td>10577.175000</td>\n",
       "      <td>13212.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>28832.691000</td>\n",
       "      <td>12931.642000</td>\n",
       "      <td>14892.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>16372.152000</td>\n",
       "      <td>10901.810000</td>\n",
       "      <td>12450.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>12847.200000</td>\n",
       "      <td>12903.824000</td>\n",
       "      <td>12569.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>9672.402000</td>\n",
       "      <td>8959.740000</td>\n",
       "      <td>11910.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>19424.514000</td>\n",
       "      <td>23013.402000</td>\n",
       "      <td>26755.474900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>31533.374000</td>\n",
       "      <td>16236.650000</td>\n",
       "      <td>11813.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>27141.059000</td>\n",
       "      <td>20141.808000</td>\n",
       "      <td>31783.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>21801.218000</td>\n",
       "      <td>37332.938000</td>\n",
       "      <td>36604.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>16493.333000</td>\n",
       "      <td>21052.979000</td>\n",
       "      <td>5930.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>5768.448000</td>\n",
       "      <td>7378.172000</td>\n",
       "      <td>6774.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>33428.622000</td>\n",
       "      <td>14541.346000</td>\n",
       "      <td>10893.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>12261.005000</td>\n",
       "      <td>14214.947000</td>\n",
       "      <td>9065.958100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>13374.620000</td>\n",
       "      <td>13493.804000</td>\n",
       "      <td>16957.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>16937.240000</td>\n",
       "      <td>14221.101000</td>\n",
       "      <td>17032.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>23209.926000</td>\n",
       "      <td>10239.356000</td>\n",
       "      <td>11375.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>17619.162000</td>\n",
       "      <td>29799.564000</td>\n",
       "      <td>15419.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>26028.659000</td>\n",
       "      <td>31607.523000</td>\n",
       "      <td>28516.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>32855.663000</td>\n",
       "      <td>22708.400000</td>\n",
       "      <td>21884.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>49409.103000</td>\n",
       "      <td>31472.337000</td>\n",
       "      <td>37056.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>21984.910000</td>\n",
       "      <td>29638.012000</td>\n",
       "      <td>31407.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>18752.746094</td>\n",
       "      <td>14766.466797</td>\n",
       "      <td>15311.119141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>14737.069336</td>\n",
       "      <td>12161.745117</td>\n",
       "      <td>11564.304688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>25111.851562</td>\n",
       "      <td>21308.195312</td>\n",
       "      <td>24246.886719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>20025.031250</td>\n",
       "      <td>16339.403320</td>\n",
       "      <td>17920.017578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>23412.404297</td>\n",
       "      <td>19552.759766</td>\n",
       "      <td>22078.068359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>21589.613281</td>\n",
       "      <td>17929.630859</td>\n",
       "      <td>20068.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>21837.064453</td>\n",
       "      <td>18021.697266</td>\n",
       "      <td>20192.105469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>21950.597656</td>\n",
       "      <td>18180.832031</td>\n",
       "      <td>20444.789062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>30616.365234</td>\n",
       "      <td>26880.906250</td>\n",
       "      <td>30857.195312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>25122.427734</td>\n",
       "      <td>21373.429688</td>\n",
       "      <td>24277.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>33553.144531</td>\n",
       "      <td>30056.023438</td>\n",
       "      <td>34591.367188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>35629.929688</td>\n",
       "      <td>32287.302734</td>\n",
       "      <td>37211.417969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category      Technology  Office Supplies     Furniture\n",
       "2015-01-31   3143.290000      4845.140000   6217.277000\n",
       "2015-02-28   1608.510000      1071.724000   1839.658000\n",
       "2015-03-31  32359.974000      8602.455000  14243.368000\n",
       "2015-04-30   8973.144000     10988.874000   7944.837000\n",
       "2015-05-31   9599.876000      7131.640000   6912.787000\n",
       "2015-06-30   8435.965000     12742.389000  13144.581600\n",
       "2015-07-31   7839.284000     15121.208000  10821.051000\n",
       "2015-08-31   8937.050000     11055.276000   7125.210500\n",
       "2015-09-30  30383.748000     27423.298000  23816.480800\n",
       "2015-10-31  11938.018000      7211.128000  12304.247000\n",
       "2015-11-30  30073.424000     26363.196000  21471.040700\n",
       "2015-12-31  20573.224000     16956.492000  30637.342500\n",
       "2016-01-31   4518.236000      1808.780000  11739.941600\n",
       "2016-02-29   3448.970000      5368.067000   3134.374000\n",
       "2016-03-31  10143.942000     13472.739000   8722.637400\n",
       "2016-04-30  11160.952000     12517.818000  10475.698500\n",
       "2016-05-31  11643.000000      8941.580000   9374.950500\n",
       "2016-06-30   6435.366000      9623.037000   7540.971000\n",
       "2016-07-31  10234.976000      4706.243000  13667.040000\n",
       "2016-08-31  15444.642000     11735.108000   9638.592200\n",
       "2016-09-30  19017.128000     19031.055000  25085.423000\n",
       "2016-10-31  10704.890000      8642.360000  11664.487500\n",
       "2016-11-30  23873.601000     21178.298000  30197.500500\n",
       "2016-12-31  35632.028000     16099.322000  22812.251200\n",
       "2017-01-31   5620.066000      5299.682000   7622.743000\n",
       "2017-02-28  12258.914000      6794.350000   3925.551000\n",
       "2017-03-31  21567.852000     17324.545000  12272.662000\n",
       "2017-04-30  14890.502000     10577.175000  13212.090000\n",
       "2017-05-31  28832.691000     12931.642000  14892.575000\n",
       "2017-06-30  16372.152000     10901.810000  12450.524000\n",
       "2017-07-31  12847.200000     12903.824000  12569.759000\n",
       "2017-08-31   9672.402000      8959.740000  11910.058300\n",
       "2017-09-30  19424.514000     23013.402000  26755.474900\n",
       "2017-10-31  31533.374000     16236.650000  11813.009000\n",
       "2017-11-30  27141.059000     20141.808000  31783.628800\n",
       "2017-12-31  21801.218000     37332.938000  36604.965000\n",
       "2018-01-31  16493.333000     21052.979000   5930.162000\n",
       "2018-02-28   5768.448000      7378.172000   6774.377400\n",
       "2018-03-31  33428.622000     14541.346000  10893.444800\n",
       "2018-04-30  12261.005000     14214.947000   9065.958100\n",
       "2018-05-31  13374.620000     13493.804000  16957.558200\n",
       "2018-06-30  16937.240000     14221.101000  17032.386700\n",
       "2018-07-31  23209.926000     10239.356000  11375.822000\n",
       "2018-08-31  17619.162000     29799.564000  15419.122000\n",
       "2018-09-30  26028.659000     31607.523000  28516.706000\n",
       "2018-10-31  32855.663000     22708.400000  21884.068200\n",
       "2018-11-30  49409.103000     31472.337000  37056.715000\n",
       "2018-12-31  21984.910000     29638.012000  31407.466800\n",
       "2019-01-31  18752.746094     14766.466797  15311.119141\n",
       "2019-02-28  14737.069336     12161.745117  11564.304688\n",
       "2019-03-31  25111.851562     21308.195312  24246.886719\n",
       "2019-04-30  20025.031250     16339.403320  17920.017578\n",
       "2019-05-31  23412.404297     19552.759766  22078.068359\n",
       "2019-06-30  21589.613281     17929.630859  20068.990234\n",
       "2019-07-31  21837.064453     18021.697266  20192.105469\n",
       "2019-08-31  21950.597656     18180.832031  20444.789062\n",
       "2019-09-30  30616.365234     26880.906250  30857.195312\n",
       "2019-10-31  25122.427734     21373.429688  24277.062500\n",
       "2019-11-30  33553.144531     30056.023438  34591.367188\n",
       "2019-12-31  35629.929688     32287.302734  37211.417969"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtener ingresos totales 5 años, ventas totales 5 años, ventas categoría 5 años, e ingresos por categoría 5 años\n",
    "#ingresos por categoría 5 años\n",
    "IngCatFinal = pd.concat([df_resultadoIng2, predIngCat2019], axis=0)\n",
    "VentCatFinal = pd.concat([df_resultadoVent2[[\"Technology\",\"Office Supplies\",\"Furniture\"]], prediccionVentas2019Cat], axis=0)\n",
    "IngFinal = pd.concat([df_resultadoIng, dfy_predSE2019])\n",
    "VentFinal = pd.concat([df_resultadoVent, X_predRL2019])\n",
    "IngCatFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9bf174ec-0a39-450c-be18-d72fc280a46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Technology</th>\n",
       "      <th>Office Supplies</th>\n",
       "      <th>Furniture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>9.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>8.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>21.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>24.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>27.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>21.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>27.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>26.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>45.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>60.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>49.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>14.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>27.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>37.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>22.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>27.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>21.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>30.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>48.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>41.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>61.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>67.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>22.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>16.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>37.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>22.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>42.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>36.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>32.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>37.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>45.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>29.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>73.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>59.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>28.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>21.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>55.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>25.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>42.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>43.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>49.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>36.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>90.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>57.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>90.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>75.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>42.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>34.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>58.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>45.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>54.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>49.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>50.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>50.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>72.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>58.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>79.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>84.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Technology  Office Supplies  Furniture\n",
       "2015-01-31         9.0             49.0       19.0\n",
       "2015-02-28         8.0             31.0        7.0\n",
       "2015-03-31        21.0            101.0       32.0\n",
       "2015-04-30        24.0             80.0       26.0\n",
       "2015-05-31        27.0             70.0       24.0\n",
       "2015-06-30        21.0             81.0       29.0\n",
       "2015-07-31        27.0             82.0       33.0\n",
       "2015-08-31        26.0             97.0       23.0\n",
       "2015-09-30        45.0            166.0       56.0\n",
       "2015-10-31        30.0             97.0       32.0\n",
       "2015-11-30        60.0            186.0       60.0\n",
       "2015-12-31        49.0            152.0       73.0\n",
       "2016-01-31        10.0             32.0       15.0\n",
       "2016-02-29        14.0             39.0       11.0\n",
       "2016-03-31        27.0             75.0       26.0\n",
       "2016-04-30        37.0             89.0       33.0\n",
       "2016-05-31        22.0             91.0       30.0\n",
       "2016-06-30        27.0             77.0       25.0\n",
       "2016-07-31        21.0             76.0       39.0\n",
       "2016-08-31        30.0             96.0       32.0\n",
       "2016-09-30        48.0            182.0       55.0\n",
       "2016-10-31        41.0             88.0       35.0\n",
       "2016-11-30        61.0            184.0       76.0\n",
       "2016-12-31        67.0            181.0       63.0\n",
       "2017-01-31        22.0             45.0       22.0\n",
       "2017-02-28        16.0             48.0       19.0\n",
       "2017-03-31        37.0             84.0       40.0\n",
       "2017-04-30        22.0            111.0       35.0\n",
       "2017-05-31        42.0            138.0       41.0\n",
       "2017-06-30        36.0            125.0       35.0\n",
       "2017-07-31        32.0            127.0       37.0\n",
       "2017-08-31        37.0            110.0       28.0\n",
       "2017-09-30        45.0            223.0       81.0\n",
       "2017-10-31        29.0            117.0       46.0\n",
       "2017-11-30        73.0            212.0       81.0\n",
       "2017-12-31        59.0            197.0       82.0\n",
       "2018-01-31        28.0             93.0       22.0\n",
       "2018-02-28        21.0             58.0       25.0\n",
       "2018-03-31        55.0            144.0       38.0\n",
       "2018-04-30        25.0            131.0       44.0\n",
       "2018-05-31        42.0            145.0       53.0\n",
       "2018-06-30        43.0            143.0       49.0\n",
       "2018-07-31        49.0            124.0       50.0\n",
       "2018-08-31        36.0            140.0       38.0\n",
       "2018-09-30        90.0            281.0       82.0\n",
       "2018-10-31        57.0            166.0       71.0\n",
       "2018-11-30        90.0            267.0       99.0\n",
       "2018-12-31        75.0            278.0      106.0\n",
       "2019-01-31        42.0            131.0       46.0\n",
       "2019-02-28        34.0            107.0       38.0\n",
       "2019-03-31        58.0            190.0       67.0\n",
       "2019-04-30        45.0            144.0       51.0\n",
       "2019-05-31        54.0            174.0       61.0\n",
       "2019-06-30        49.0            159.0       56.0\n",
       "2019-07-31        50.0            160.0       56.0\n",
       "2019-08-31        50.0            161.0       57.0\n",
       "2019-09-30        72.0            243.0       84.0\n",
       "2019-10-31        58.0            191.0       67.0\n",
       "2019-11-30        79.0            273.0       94.0\n",
       "2019-12-31        84.0            294.0      101.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VentCatFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "574fd78f-85f6-48a3-b24e-9757b4a55cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>338.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>453.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>459.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>479.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Total\n",
       "2015-01-31   77.0\n",
       "2015-02-28   46.0\n",
       "2015-03-31  154.0\n",
       "2015-04-30  130.0\n",
       "2015-05-31  121.0\n",
       "2015-06-30  131.0\n",
       "2015-07-31  142.0\n",
       "2015-08-31  146.0\n",
       "2015-09-30  267.0\n",
       "2015-10-31  159.0\n",
       "2015-11-30  306.0\n",
       "2015-12-31  274.0\n",
       "2016-01-31   57.0\n",
       "2016-02-29   64.0\n",
       "2016-03-31  128.0\n",
       "2016-04-30  159.0\n",
       "2016-05-31  143.0\n",
       "2016-06-30  129.0\n",
       "2016-07-31  136.0\n",
       "2016-08-31  158.0\n",
       "2016-09-30  285.0\n",
       "2016-10-31  164.0\n",
       "2016-11-30  321.0\n",
       "2016-12-31  311.0\n",
       "2017-01-31   89.0\n",
       "2017-02-28   83.0\n",
       "2017-03-31  161.0\n",
       "2017-04-30  168.0\n",
       "2017-05-31  221.0\n",
       "2017-06-30  196.0\n",
       "2017-07-31  196.0\n",
       "2017-08-31  175.0\n",
       "2017-09-30  349.0\n",
       "2017-10-31  192.0\n",
       "2017-11-30  366.0\n",
       "2017-12-31  338.0\n",
       "2018-01-31  143.0\n",
       "2018-02-28  104.0\n",
       "2018-03-31  237.0\n",
       "2018-04-30  200.0\n",
       "2018-05-31  240.0\n",
       "2018-06-30  235.0\n",
       "2018-07-31  223.0\n",
       "2018-08-31  214.0\n",
       "2018-09-30  453.0\n",
       "2018-10-31  294.0\n",
       "2018-11-30  456.0\n",
       "2018-12-31  459.0\n",
       "2019-01-31  219.0\n",
       "2019-02-28  180.0\n",
       "2019-03-31  315.0\n",
       "2019-04-30  241.0\n",
       "2019-05-31  289.0\n",
       "2019-06-30  264.0\n",
       "2019-07-31  266.0\n",
       "2019-08-31  268.0\n",
       "2019-09-30  399.0\n",
       "2019-10-31  316.0\n",
       "2019-11-30  447.0\n",
       "2019-12-31  479.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VentFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1dc0f74e-9b95-40dc-bb8b-ed1431cce089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-31</th>\n",
       "      <td>14205.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-28</th>\n",
       "      <td>4519.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-31</th>\n",
       "      <td>55205.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30</th>\n",
       "      <td>27906.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-05-31</th>\n",
       "      <td>23644.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-06-30</th>\n",
       "      <td>34322.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-31</th>\n",
       "      <td>33781.543000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-08-31</th>\n",
       "      <td>27117.536500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-09-30</th>\n",
       "      <td>81623.526800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-10-31</th>\n",
       "      <td>31453.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-11-30</th>\n",
       "      <td>77907.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>68167.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-31</th>\n",
       "      <td>18066.957600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-29</th>\n",
       "      <td>11951.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31</th>\n",
       "      <td>32339.318400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-30</th>\n",
       "      <td>34154.468500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>29959.530500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-30</th>\n",
       "      <td>23599.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-31</th>\n",
       "      <td>28608.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>36818.342200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>63133.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>31011.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>75249.399500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>74543.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-31</th>\n",
       "      <td>18542.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28</th>\n",
       "      <td>22978.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>51165.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-30</th>\n",
       "      <td>38679.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>56656.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>39724.486000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-31</th>\n",
       "      <td>38320.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>30542.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>69193.390900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31</th>\n",
       "      <td>59583.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-30</th>\n",
       "      <td>79066.495800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>95739.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-31</th>\n",
       "      <td>43476.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-28</th>\n",
       "      <td>19920.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>58863.412800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-30</th>\n",
       "      <td>35541.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-31</th>\n",
       "      <td>43825.982200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>48190.727700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-31</th>\n",
       "      <td>44825.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-31</th>\n",
       "      <td>62837.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>86152.888000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-31</th>\n",
       "      <td>77448.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>117938.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>83030.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>49682.408073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-28</th>\n",
       "      <td>40135.135966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>73568.403794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-30</th>\n",
       "      <td>55112.106270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-31</th>\n",
       "      <td>67276.278734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>60818.224836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-31</th>\n",
       "      <td>61453.022672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-31</th>\n",
       "      <td>61866.994373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-30</th>\n",
       "      <td>94522.396066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-31</th>\n",
       "      <td>73967.755768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-30</th>\n",
       "      <td>106402.971754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>114474.811994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Total\n",
       "2015-01-31   14205.707000\n",
       "2015-02-28    4519.892000\n",
       "2015-03-31   55205.797000\n",
       "2015-04-30   27906.855000\n",
       "2015-05-31   23644.303000\n",
       "2015-06-30   34322.935600\n",
       "2015-07-31   33781.543000\n",
       "2015-08-31   27117.536500\n",
       "2015-09-30   81623.526800\n",
       "2015-10-31   31453.393000\n",
       "2015-11-30   77907.660700\n",
       "2015-12-31   68167.058500\n",
       "2016-01-31   18066.957600\n",
       "2016-02-29   11951.411000\n",
       "2016-03-31   32339.318400\n",
       "2016-04-30   34154.468500\n",
       "2016-05-31   29959.530500\n",
       "2016-06-30   23599.374000\n",
       "2016-07-31   28608.259000\n",
       "2016-08-31   36818.342200\n",
       "2016-09-30   63133.606000\n",
       "2016-10-31   31011.737500\n",
       "2016-11-30   75249.399500\n",
       "2016-12-31   74543.601200\n",
       "2017-01-31   18542.491000\n",
       "2017-02-28   22978.815000\n",
       "2017-03-31   51165.059000\n",
       "2017-04-30   38679.767000\n",
       "2017-05-31   56656.908000\n",
       "2017-06-30   39724.486000\n",
       "2017-07-31   38320.783000\n",
       "2017-08-31   30542.200300\n",
       "2017-09-30   69193.390900\n",
       "2017-10-31   59583.033000\n",
       "2017-11-30   79066.495800\n",
       "2017-12-31   95739.121000\n",
       "2018-01-31   43476.474000\n",
       "2018-02-28   19920.997400\n",
       "2018-03-31   58863.412800\n",
       "2018-04-30   35541.910100\n",
       "2018-05-31   43825.982200\n",
       "2018-06-30   48190.727700\n",
       "2018-07-31   44825.104000\n",
       "2018-08-31   62837.848000\n",
       "2018-09-30   86152.888000\n",
       "2018-10-31   77448.131200\n",
       "2018-11-30  117938.155000\n",
       "2018-12-31   83030.388800\n",
       "2019-01-31   49682.408073\n",
       "2019-02-28   40135.135966\n",
       "2019-03-31   73568.403794\n",
       "2019-04-30   55112.106270\n",
       "2019-05-31   67276.278734\n",
       "2019-06-30   60818.224836\n",
       "2019-07-31   61453.022672\n",
       "2019-08-31   61866.994373\n",
       "2019-09-30   94522.396066\n",
       "2019-10-31   73967.755768\n",
       "2019-11-30  106402.971754\n",
       "2019-12-31  114474.811994"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IngFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac4c45-bd20-49af-a6f5-87e8cd79458f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
